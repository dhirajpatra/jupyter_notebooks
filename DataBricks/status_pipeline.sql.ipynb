{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e54f1d7e-8f8d-4ce5-a694-569c2e38883b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f9353a-c1dc-40d0-8030-f92a9f258da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Status Pipeline\n",
    "\n",
    "In this pipeline, the **status** pipeline is implemented within a notebook. Lakeflow Declarative Pipelines support using `.py`, `.sql`, or notebook files as pipeline sources.\n",
    "\n",
    "**NOTE:** In a notebook you must use either Python or SQL. \n",
    "\n",
    "This notebook pipeline performs the following tasks:\n",
    "\n",
    "1. Creates the **status_bronze_demo5** streaming table by ingesting raw JSON files from `/Volumes/dbacademy/ops/your-lab-user/status/`.\n",
    "\n",
    "2. Creates the **status_silver_demo5** streaming table from the **status_bronze_demo5** table.\n",
    "\n",
    "3. Creates the materialized view **full_order_status_gold_demo5** to capture each order's status by joining the following tables:\n",
    "   - **status_silver_demo5**\n",
    "   - **orders_silver_demo5**\n",
    "\n",
    "4. Creates the following materialized views:\n",
    "   - **cancelled_orders_gold_demo5** – Displays all cancelled orders and how many days passed before cancellation.\n",
    "   - **delivered_orders_gold_demo5** – Displays all delivered orders and how many days it took to deliver each order.\n",
    "![Pipeline](../../Includes/images/demo6_pipeline_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72781af1-9c00-41ac-8145-0c4c2002bd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. JSON -> Bronze\n",
    "The code below ingests JSON files located in your `/Volumes/dbacademy/ops/your-lab-user/status/` volume, using the `source` configuration parameter to point to the base path `/Volumes/dbacademy/ops/your-lab-user/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63625775-81c6-4841-bd4f-d083bf546ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 1_bronze_db.status_bronze_demo5\n",
    "  COMMENT \"Ingest raw JSON order status files from cloud storage\"\n",
    "  TBLPROPERTIES (\n",
    "    \"quality\" = \"bronze\",\n",
    "    \"pipelines.reset.allowed\" = false    -- prevent full table refreshes on the bronze table\n",
    "  )\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() processing_time, \n",
    "  _metadata.file_name AS source_file\n",
    "FROM STREAM read_files(\n",
    "  \"${source}/status\", \n",
    "  format => \"json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5958ac07-7b0f-4bbd-822d-381b6e3e19b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Bronze -> Silver\n",
    "The code below performs a simple transformation on the date field and selects only the necessary columns for the silver streaming table **status_silver_demo5**.  \n",
    "\n",
    "We're also adding a comment and table properties to document the table for production use, along with pipeline expectations to enforce data quality on the streaming table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ba1b7e-cfd3-4395-a54e-73287c813a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 2_silver_db.status_silver_demo5\n",
    "  (\n",
    "    -- Drop rows if order_status_timestamp is not valid\n",
    "    CONSTRAINT valid_timestamp EXPECT (order_status_timestamp > \"2021-12-25\") ON VIOLATION DROP ROW,\n",
    "    -- Warn if order_status is not in the following\n",
    "    CONSTRAINT valid_order_status EXPECT (order_status IN ('on the way','canceled','return canceled','delivered','return processed','placed','preparing'))\n",
    "  )\n",
    "  COMMENT \"Order with each status and timestamp\"\n",
    "  TBLPROPERTIES (\"quality\" = \"silver\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_status,\n",
    "  timestamp(status_timestamp) AS order_status_timestamp\n",
    "FROM STREAM 1_bronze_db.status_bronze_demo5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171d02e3-a0e3-4479-bd51-7fed82790b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Use a Materialized View to Join Two Streaming Tables\n",
    "One way to join two streaming tables in Lakeflow Declarative Pipelines is by creating a materialized view that performs the join.  This approach takes all rows from each streaming table and executes a full inner join operation and incorporates optimizations where applicable.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "- **Materialized views include built-in optimizations where applicable:**\n",
    "  - [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh)\n",
    "  - [Delta Live Tables Announces New Capabilities and Performance Optimizations](https://www.databricks.com/blog/2022/06/29/delta-live-tables-announces-new-capabilities-and-performance-optimizations.html)\n",
    "  - [Cost-effective, incremental ETL with serverless compute for Delta Live Tables pipelines](https://www.databricks.com/blog/cost-effective-incremental-etl-serverless-compute-delta-live-tables-pipelines)\n",
    "\n",
    "- **Stateful joins (Stream to Stream):** For stateful joins in pipelines (i.e., joining incrementally as data is ingested), refer to the [Optimize stateful processing in Lakeflow Declarative Pipelines with watermarks](https://docs.databricks.com/aws/en/dlt/stateful-processing) documentation. **Stateful joins are an advanced topic and outside the scope of this course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62755fa8-5748-4da4-a5a4-255418c13141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.full_order_info_gold_demo5\n",
    "  COMMENT \"Joining the orders and order status silver tables to view all orders with each individual status per order\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  orders.order_id,\n",
    "  orders.order_timestamp,\n",
    "  status.order_status,\n",
    "  status.order_status_timestamp\n",
    "-- Notice that the STREAM keyword was not used when referencing the streaming tables to create the MV\n",
    "FROM 2_silver_db.status_silver_demo5 status    \n",
    "  INNER JOIN 2_silver_db.orders_silver_demo5 orders \n",
    "  ON orders.order_id = status.order_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e505e6-776f-4305-abbb-c1351a4ddf68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Create Materialized Views for Cancelled and Delivered Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0f6716-061b-4ac6-91b5-84f20e1ce2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The code below will create two tables using the joined data from above:\n",
    "\n",
    "- **3_gold_db.cancelled_orders_gold_demo5**\n",
    "    - A materialized view containing all **cancelled** orders\n",
    "    - number of days it took to cancel each order.\n",
    "\n",
    "- **3_gold_db.delivered_orders_gold_demo5**\n",
    "    - A materialized view containing all **delivered** orders\n",
    "    - number of days it took to deliver each order.\n",
    "\n",
    "    [datediff function](https://docs.databricks.com/aws/en/sql/language-manual/functions/datediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a29bfd1c-c9aa-4774-b912-c2e20cdb0c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- CANCELLED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.cancelled_orders_gold_demo5\n",
    "  COMMENT \"All cancelled orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_cancel -- calculate days to cancel\n",
    "FROM 3_gold_db.full_order_info_gold_demo5\n",
    "WHERE order_status = 'canceled';\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- DELIVERED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.delivered_orders_gold_demo5\n",
    "  COMMENT \"All delivered orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_delivery -- calculate days to deliver\n",
    "FROM 3_gold_db.full_order_info_gold_demo5\n",
    "WHERE order_status = 'delivered';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0abf5434-0953-4ed5-827a-4169b2b7f137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Create the Production Pipeline\n",
    "Follow the steps below to modify the pipeline settings and run the production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b092780b-048c-44a3-b682-481399649586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to modify your pipeline configuration for production:\n",
    "\n",
    "   a. Select the **Settings** icon ![Pipeline Settings](../../Includes/images/pipeline_settings_icon.png) in the left navigation pane.\n",
    "\n",
    "   b. In the **Pipeline settings** section, you can modify the **Pipeline name** and **Run as** settings (this lab does not give you permission to modify **Run as**).\n",
    "\n",
    "      - If you had permission, you could select the pencil icon ![pencil_settings_icon.png](../../Includes/images/pencil_settings_icon.png) next to **Run as** to modify the option.\n",
    "\n",
    "      - You can optionally change the executor of the pipeline to a service principal. A service principal is an identity you create in Databricks for use with automated tools, jobs, and applications.  \n",
    "\n",
    "        - For more information, see the [What is a service principal?](https://docs.databricks.com/aws/en/admin/users-groups/service-principals#what-is-a-service-principal) documentation.\n",
    "\n",
    "   c. In the **Code assets** section, confirm that:\n",
    "\n",
    "      - **Root folder** points to this pipeline project (**5 - Deploying a Pipeline to Production**).\n",
    "\n",
    "      - **Source code** references the **orders** and **status** folders within this project.\n",
    "\n",
    "   d. In the **Default location for data assets** section, confirm the following:\n",
    "\n",
    "      - **Default catalog** is your **labuser** catalog.\n",
    "\n",
    "      - **Default schema** is the **default** schema.\n",
    "\n",
    "   e. In the **Compute** section, confirm that **Serverless** compute is selected.\n",
    "\n",
    "   f. In the **Configuration** section, ensure that the `source` key is set to your data source volume path: `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "   g. In the **Advanced settings** section:\n",
    "   \n",
    "      - Expand **Advanced settings**.\n",
    "\n",
    "      - Click **Edit advanced settings**.\n",
    "\n",
    "      - In **Pipeline mode**, ensure **Triggered** is selected so the pipeline runs on a schedule.  \n",
    "        - Alternatively, you can choose **Continuous** mode to keep the pipeline running at all times.  \n",
    "        - For more details, see [Triggered vs. continuous pipeline mode](https://docs.databricks.com/aws/en/dlt/pipeline-mode).\n",
    "\n",
    "      - In **Pipeline user mode** select **Production**.\n",
    "\n",
    "      - For **Channel**, you can leave it as **Preview** for training purposes:\n",
    "        - **Current** – Uses the latest stable Databricks Runtime version, recommended for production.\n",
    "        - **Preview** – Uses a more recent, potentially less stable Runtime version, ideal for testing upcoming features.\n",
    "        - View the [Lakeflow Declarative Pipelines release notes and the release upgrade process](https://docs.databricks.com/aws/en/release-notes/dlt/) documentation for more information.\n",
    "\n",
    "      - In the **Event logs** section:\n",
    "        - Select **Publish event log to metastore**.\n",
    "        - Set **Event log name** to `event_log_demo_5`.\n",
    "        - Set **Event log catalog** to your **labuser** catalog.\n",
    "        - Set **Event log schema** to the **default** schema.\n",
    "        - Select **Save**.\n",
    "\n",
    "        **NOTE:** If the event log is not saved to the correct location, the event log exploration steps will not work properly in the main notebook.\n",
    "\n",
    "   h. Click **Save** to save your pipeline settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faf7a67e-e44f-49e2-87da-86c2e27809ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Once your pipeline is production-ready, you'll want to schedule it to run either on a time interval or continuously.\n",
    "\n",
    "   For this demonstration, we’ll:\n",
    "   - Schedule the pipeline to run every day at 8:00 PM.\n",
    "   - Optionally configure notifications to alert you upon job **Start**, **Success**, and **Failure**.  \n",
    "     *(If you don’t want email notifications, you can skip this step.)*\n",
    "\n",
    "   Complete the following steps to schedule the pipeline:\n",
    "\n",
    "   a. Select the **Schedule** button (might be a small calendar icon if your screen is minimized).\n",
    "\n",
    "   b. For the job name, leave it as **5 - Deploying a Pipeline to Production Project - labuser-name**.\n",
    "\n",
    "   c. Below **Job name**, select **Advanced**.\n",
    "\n",
    "   d. In the **Schedule** section, configure the following:\n",
    "   - Set the **Day**.\n",
    "   - Set the time to **20:00** (8:00 PM).\n",
    "   - Leave the **Timezone** as default.\n",
    "   - Select **More options**, and under **Notifications**, add your email to receive alerts for:\n",
    "     - **Start**\n",
    "     - **Success**\n",
    "     - **Failure**\n",
    "\n",
    "   e. Click **Create** to save and schedule the job.\n",
    "\n",
    "  **NOTE:** You could also set the pipeline to run a few minutes after your current time to see it start through the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c302c99e-d6dd-4f5e-a223-aa987302d0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. After completing the pipeline settings and scheduling the pipeline, let's manually trigger the pipeline by selecting the **Run pipeline** button.\n",
    "\n",
    "    While the pipeline is running, you can explore what the final pipeline will look like using the image below:\n",
    "\n",
    "    ![DLT Pipeline Demo 6](../../Includes/images/demo6_pipeline_image.png) \n",
    "\n",
    "**NOTE:** Currently we have one JSON file in both **status** and **orders**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80b4d812-8834-4079-a3c3-306f4fae8690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. After the pipeline has completed it's first run, complete the following:\n",
    "\n",
    "   a. Examine the **Pipeline graph** and confirm:\n",
    "      - 174 rows were read into the **orders_bronze** and **orders_silver** streaming tables\n",
    "      - 536 rows were read into the **status_bronze** and **status_silver** streaming tables\n",
    "      - 536 rows are in the **full_order_info_gold** materialized view (JOIN)\n",
    "      - 7 rows are in the **orders_by_date_gold** materialized view\n",
    "      - 8 rows are in the **cancelled_orders_gold** materialized view\n",
    "      - 94 rows are in the **delivered_orders_gold** materialized view\n",
    "\n",
    "   b. Go back to the main notebook **5 - Deploying a Pipeline to Production**\n",
    "   \n",
    "   c. Complete the steps in step **D. Land More Data to Your Data Source Volume**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7f677a-dc75-4a8e-939c-98072df0835b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. After you have landed **4** new files into the data source volume, run the pipeline to process the newly landed JSON files.\n",
    "\n",
    "   Notice the following:\n",
    "\n",
    "   a. The **status** bronze to silver flow ingests 410 new rows.\n",
    "\n",
    "   b. The **orders** bronze to silver flow ingests 98 new rows.\n",
    "\n",
    "   c. The **full_order_info_gold** materialized view join contains a total of 946 rows (the previous 536 rows + the new 410 rows).\n",
    "\n",
    "   d. The **cancelled_orders_gold** materialized view contains 21 rows.\n",
    "\n",
    "   e. The **delivered_orders_gold** materialized view contains 176 rows.\n",
    "\n",
    "   f. The **orders_by_date_gold** materialized view contains 11 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9d714a-079e-4400-ab30-7cc5551dac78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. In the window at the bottom, select the **Expectations** link for the **status_silver_demo5** table. It should contain the value **2**. Notice that in this run, 7.6% (31 rows) for the **valid_order_status** expectation returned a warning.\n",
    "\n",
    "    This is something we would want to investigate and address in future stages of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b7a0a9-c9ec-41eb-8296-1348810ba26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Go back to the main notebook **5 - Deploying a Pipeline to Production** and complete the steps in step **E. Monitor Your Pipeline with the Event Log Introduction (Advanced Topic)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc8b2f3-cb71-4cb1-ad98-13ed43833f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "status_pipeline.sql",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}