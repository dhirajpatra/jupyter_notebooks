{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a9636d-c006-416f-a92e-7d0b8fbc0a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f2bd13-f820-4a77-892f-ce5124ff250b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customers Pipeline with Change Data Capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa02cc3c-dcc5-43d8-876c-9b91dbaeb842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the Pipeline\n",
    "To save some time, let's run the entire pipeline for **status**, **orders** and **customers**. While the pipeline is running explore the code cells for the new **customers** flow.\n",
    "\n",
    "**NOTE:** The **status** and **orders** pipelines are the same as the previous demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f2232b9-5986-417b-afdd-e4318c705f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. JSON -> Bronze\n",
    "\n",
    " As in the previous notebook, we define a bronze streaming table named **customers_bronze_raw_demo6** using a data source configured with Auto Loader (`FROM STREAM`).\n",
    "\n",
    "The code below includes:\n",
    "\n",
    "   - Adds comment for clarity.\n",
    "\n",
    "   - The table property `pipelines.reset.allowed = false` to prevent deletion of all ingested bronze data if a full refresh is triggered.\n",
    "\n",
    "   - Creates columns to capture the time of data ingestion and the source file name for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5f39fc3-2ab2-4b11-a989-c18d3726210d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 1_bronze_db.customers_bronze_raw_demo6\n",
    "  COMMENT \"Raw data from customers CDC feed\"\n",
    "  TBLPROPERTIES (\n",
    "    \"quality\" = \"bronze\",\n",
    "    \"pipelines.reset.allowed\" = false     -- prevent full table refreshes on the bronze table\n",
    "  )\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() processing_time,\n",
    "  _metadata.file_name as source_file\n",
    "FROM STREAM read_files(\n",
    "  \"${source}/customers\", \n",
    "  format => \"json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbefed3b-7ed9-4220-84d4-0985f9491e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Data Quality Enforcement\n",
    "\n",
    "The query below demonstrates:\n",
    "\n",
    "- The three violation constraint actions: **WARN**, **DROP**, and **FAIL**. Each defines how to handle constraint violations.\n",
    "- Applying multiple conditions to a single constraint.\n",
    "- Using a built-in SQL function within a constraint.\n",
    "\n",
    "### About the data source:\n",
    "\n",
    "- The data is a CDC feed that contains **`INSERT`**, **`UPDATE`**, and **`DELETE`** operations.  \n",
    "- REQUIREMENT: **UPDATE** and **INSERT** operations should contain valid entries for all fields.  \n",
    "- REQUIREMENT: **DELETE** operations should contain **`NULL`** values for all fields except the **timestamp**, **customer_id**, and **operation** fields.\n",
    "\n",
    "**NOTE:** To ensure only valid data reaches our silver table, weâ€™ll write a series of quality enforcement rules that allow expected null values in **DELETE** operations while rejecting bad data elsewhere.\n",
    "\n",
    "\n",
    "### We'll break down each of these constraints below:\n",
    "\n",
    "##### 1. **`valid_id`**\n",
    "This constraint will cause our transaction to fail if a record contains a null value in the **`customer_id`** field.\n",
    "\n",
    "##### 2. **`valid_operation`**\n",
    "This constraint will drop any records that contain a null value in the **`operation`** field.\n",
    "\n",
    "##### 3. **`valid_name`**\n",
    "This constraint will track any records that contain a null value in the **`name`** field. Because there is no additional instruction for what to do with invalid records, violating rows will be recorded in metrics but not dropped.\n",
    "\n",
    "##### 4. **`valid_address`**\n",
    "This constraint checks if the **`operation`** field is **`DELETE`**; if not, it checks for null values in any of the 4 fields comprising an address. Because there is no additional instruction for what to do with invalid records, violating rows will be recorded in metrics but not dropped.\n",
    "\n",
    "##### 5. **`valid_email`**\n",
    "This constraint uses regex pattern matching to check that the value in the **`email`** field is a valid email address. It contains logic to not apply this to records if the **`operation`** field is **`DELETE`** (because these will have a null value for the **`email`** field). Violating records are dropped.\n",
    "\n",
    "**NOTE:** When a record is going to be dropped, all values except the **customer_id** will be `null`.\n",
    "| address                               | city         | customer_id | email                    | name           | operation | state |\n",
    "|---------------------------------------|--------------|-------------|--------------------------|----------------|-----------|-------|\n",
    "| null                                  | null         | 23617       | null                     | null           | DELETE    | null  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb66c8e-a33a-43de-b15b-ffad265b54d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING TABLE 1_bronze_db.customers_bronze_clean_demo6\n",
    "  (\n",
    "    CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE,\n",
    "    CONSTRAINT valid_operation EXPECT (operation IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT valid_name EXPECT (name IS NOT NULL OR operation = \"DELETE\"),\n",
    "    CONSTRAINT valid_address EXPECT (\n",
    "      (address IS NOT NULL and \n",
    "        city IS NOT NULL and \n",
    "        state IS NOT NULL and \n",
    "        zip_code IS NOT NULL) OR\n",
    "       operation = \"DELETE\"),\n",
    "    CONSTRAINT valid_email EXPECT (\n",
    "      rlike(email, '^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$') OR \n",
    "            operation = \"DELETE\") ON VIOLATION DROP ROW\n",
    "  )\n",
    "  COMMENT \"Clean raw bronze timestamp column and add data quality constraints\"\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  CAST(from_unixtime(timestamp) AS timestamp) AS timestamp_datetime\n",
    "FROM STREAM 1_bronze_db.customers_bronze_raw_demo6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e91bd65-c503-48fa-94a4-154cdd392332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Processing CDC Data with **`APPLY CHANGES INTO`**\n",
    "DLT introduces a new syntactic structure for simplifying CDC feed processing.\n",
    "\n",
    "**`APPLY CHANGES INTO`** has the following guarantees and requirements:\n",
    "- Performs incremental/streaming ingestion of CDC data\n",
    "- Provides simple syntax to specify one or many fields as the primary key for a table\n",
    "- Default assumption is that rows will contain inserts and updates\n",
    "- Can optionally apply deletes\n",
    "- Automatically orders late-arriving records using user-provided sequencing key (order to process rows)\n",
    "- Uses a simple syntax for specifying columns to ignore with the **`EXCEPT`** keyword\n",
    "- The default to applying changes is SCD Type 1. We will use SCD Type 2 here.\n",
    "\n",
    "The code below uses `APPLY CHANGES INTO` to create and update the **2_silver_db.customers_silver_demo6** streaming table using records from the **1_bronze_db.customers_bronze_clean_demo6** streaming table.\n",
    "\n",
    "[APPLY CHANGES INTO](https://docs.databricks.com/gcp/en/dlt-ref/dlt-sql-ref-apply-changes-into) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb2236e-b169-4453-8e37-9d8ecba53b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Create the streaming target table if it's not already created\n",
    "CREATE OR REFRESH STREAMING TABLE 2_silver_db.customers_silver_demo6\n",
    "  COMMENT 'SCD Type 2 Historical Customer Data';\n",
    "\n",
    "\n",
    "-- Apply CDC changes from the cleaned bronze stream to the silver table using SCD Type 2\n",
    "APPLY CHANGES INTO 2_silver_db.customers_silver_demo6   -- Target table to update\n",
    "  FROM STREAM 1_bronze_db.customers_bronze_clean_demo6  -- Source records to determine updates, deletes and inserts\n",
    "  KEYS (customer_id)                              -- Primary key for identifying records\n",
    "  APPLY AS DELETE WHEN operation = \"DELETE\"       -- Handle deletes from source to the target\n",
    "  SEQUENCE BY timestamp_datetime                  -- Defines order of operations for applying changes\n",
    "  COLUMNS * EXCEPT (timestamp, _rescued_data, operation)     -- Select columns and exclude metadata fields\n",
    "  STORED AS SCD TYPE 2;                           -- Use Slowly Changing Dimension Type 2 to keep historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2830999d-fc87-48cd-803d-0872beb26880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Querying Tables with Applied Changes\n",
    "\n",
    "The `APPLY CHANGES INTO` statement defaults to a Type 1 SCD table, where each key has a single current record and updates overwrite existing data. In this case, we're using Type 2 SCD to preserve historical changes.\n",
    "\n",
    "\n",
    "#### Important\n",
    "Although **customers_silver_demo6** is defined as a streaming table, applying updates and deletes makes it unsuitable as a streaming source for downstream operations due to the violation of append-only constraints.\n",
    "\n",
    "This approach ensures that out-of-order updates can be properly reconciled and that deleted records are excluded from downstream results.\n",
    "\n",
    "The cell below defines a materialized view from the **customers_silver_demo6** table to store only the **current customer data** for use in downstream analysis, joins or other processing by querying for all `null` values in the **__END_AT** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44937958-61af-4089-baba-fef2eeb01c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.current_customers_gold_demo6\n",
    "COMMENT \"Current updated list of active customers\"\n",
    "AS \n",
    "SELECT \n",
    "  * EXCEPT (processing_time),\n",
    "  current_timestamp() updated_at\n",
    "FROM 2_silver_db.customers_silver_demo6\n",
    "WHERE `__END_AT` IS NULL;      -- Filter for only rows that contain a null value for __END_AT, which indicates the current version of the record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e48deea-5822-43f9-b077-6984f53b56db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Explore the Pipeline Results\n",
    "\n",
    "After running the pipeline and reviewing the code cells, take time to explore the pipeline results for the **customers** flow following the steps below.\n",
    "\n",
    "**Run with 1 JSON File**\n",
    "\n",
    "![demo6_cdc_run01.png](../../Includes/images/demo6_cdc_run01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17bc300c-9b60-47eb-ba50-e08678adf7f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. In the **customers** flow in the pipeline graph, notice that **939** rows were streamed into the three streaming tables and the materialized view. This is because all records are new and valid entries, they were ingested throughout the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76a9311-06ff-4c58-9bf5-89b8ab6f5e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. In the table below, find the **customers_silver_demo6** table and note the following:\n",
    "\n",
    "  - The **Upserted** column indicates that all **939** rows were upserted into the table, as all rows are new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad2bb39-30e8-4023-8175-766f502de0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Leave this pipeline open and navigate back to the **6 - Change Data Capture with APPLY INTO** notebook and follow the steps in **D. Land New Data to Your Data Source Volume** and then run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d5c149-6e1c-47e7-84bd-d8583d0a91cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. After you have explored and landed  1 new JSON file to each of your data sources, complete the following to explore the **customers** flow in the **Pipeline graph**:\n",
    "\n",
    "   a. 23 rows were read into the **customers_bronze_raw_demo6** and **customers_bronze_clean_demo6** streaming tables (from the new **01.json** file) since all data quality checks passed.\n",
    "\n",
    "   b. In the **customers_silver_demo6** streaming table (CDC SCD Type 2), 35 rows were processed and **upserted** into the table:\n",
    "   \n",
    "      - **12 updates** (each consisting of updating the original row to inactive, and adding a new row, for a total of **24** upserts)  \n",
    "      - **1** row was marked as deleted (value added for the **__END_AT** column)  \n",
    "      - **10** new customers were added  \n",
    "      - Total changes: 24 + 1 + 10 = **35**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd27eed-874f-4964-881c-78514639ac50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Navigate back to the **6 - Change Data Capture with APPLY INTO** notebook and follow the steps in **E. View the CDC SCD Type 2 on the Customers Table**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477a2917-9c42-4623-a90d-016cf3ebcb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "customers_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}