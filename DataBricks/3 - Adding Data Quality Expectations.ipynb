{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396bf819-bb67-47c6-92ea-19bed6b4a40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ba31e4-75cc-4e59-ab54-6fd634e6d124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Adding Data Quality Expectations\n",
    "\n",
    "In this demonstration we will add data quality expectations to apply quality constraints that validates data as it flows through Lakeflow Declarative Pipelines. Expectations provide greater insight into data quality metrics and allow you to fail updates or drop records when detecting invalid records.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Add quality constraints within a Lakeflow Declarative Pipeline to trigger appropriate actions (warn, drop, or fail) based on data expectations.\n",
    "- Analyze pipeline metrics to identify and interpret data quality issues across different data flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "818bd53a-c8c4-43a9-9050-3d41855cae7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26437f79-6c04-44ce-b556-39b7c0db1b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "This cell will also reset your `/Volumes/dbacademy/ops/labuser/` volume with the JSON files to the starting point, with one JSON file in each volume.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af2a5ab-b13d-48dc-8d7b-be43d7282932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema labuser11058730_1754017152.1_bronze_db already exists. No action taken.\nSchema labuser11058730_1754017152.2_silver_db already exists. No action taken.\nSchema labuser11058730_1754017152.3_gold_db already exists. No action taken.\n----------------------------------------------------------------------------------------\nDirectory /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/customers already exists. No action taken.\nDirectory /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders already exists. No action taken.\nDirectory /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/status already exists. No action taken.\n----------------------------------------------------------------------------------------\n\n\nSearching for files in /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/customers/ volume to delete prior to creating files...\nDeleting file: /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/customers/00.json\n\nSearching for files in /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/ volume to delete prior to creating files...\nDeleting file: /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/00.json\nDeleting file: /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/01.json\n\nSearching for files in /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/status/ volume to delete prior to creating files...\nDeleting file: /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/status/00.json\n\n----------------Loading files to user's volume: '/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/customers'----------------\nFile number 1 - Copying file /Volumes/dbacademy_retail/v01/retail-pipeline/customers/stream_json/00.json --> /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/customers/00.json.\n\n----------------Loading files to user's volume: '/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders'----------------\nFile number 1 - Copying file /Volumes/dbacademy_retail/v01/retail-pipeline/orders/stream_json/00.json --> /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/00.json.\n\n----------------Loading files to user's volume: '/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/status'----------------\nFile number 1 - Copying file /Volumes/dbacademy_retail/v01/retail-pipeline/status/stream_json/00.json --> /Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/status/00.json.\n\n\n\n------------------------------------------------------------------------------\nSETUP COMPLETE!\n------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas are available, lab check passed: ['1_bronze_db', '2_silver_db', '3_gold_db'].\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "        <tr>\n",
       "            <td style=\"white-space:nowrap; width:1em\">Your catalog name variable reference: DA.catalog_name:</td>\n",
       "            <td><input type=\"text\" value=\"labuser11058730_1754017152\" style=\"width: 100%\"></td></tr>\n",
       "        <tr>\n",
       "            <td style=\"white-space:nowrap; width:1em\">Variable reference to your source files (Python - DA.paths.working_dir, SQL - DA.paths_working_dir):</td>\n",
       "            <td><input type=\"text\" value=\"/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a33f918-2570-4ed8-9c16-3f5d142a39f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to programmatically view the files in your `/Volumes/dbacademy/ops/labuser/orders` volume. Confirm you only see the original **00.json** file in the **orders** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c694bc30-1f09-4b16-86b2-457d0d39a0a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View files in the orders volume"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modification_time</th></tr></thead><tbody><tr><td>/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/00.json</td><td>00.json</td><td>15313</td><td>1754027178000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "/Volumes/dbacademy/ops/labuser11058730_1754017152@vocareum_com/orders/00.json",
         "00.json",
         15313,
         1754027178000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modification_time",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "spark.sql(f'LIST \"{DA.paths.working_dir}/orders\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837398f2-ae12-4e78-bfab-f610d9df133f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Adding Data Quality Expectations\n",
    "\n",
    "This demonstration includes a simple starter Lakeflow Declarative Pipeline that has already been created. We will continue to build on it to explore it's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0896defa-4605-4562-b5bc-9769940a5a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "1. Run the cell below to create your starter pipeline for this demonstration. The pipeline will set the following for you:\n",
    "\n",
    "- Your default catalog: `labuser`\n",
    "\n",
    "- Your configuration parameter: `source` = `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "  **NOTE:** If the pipeline already exists, an error will be returned. In that case, you'll need to delete the existing pipeline and rerun this cell.\n",
    "\n",
    "  To delete the pipeline:\n",
    "\n",
    "  - Select **Jobs and Pipelines** from the far-left navigation bar.  \n",
    "\n",
    "  - Find the pipeline you want to delete.  \n",
    "\n",
    "  - Click the three-dot menu ![ellipsis icon](./Includes/images/ellipsis_icon.png).  \n",
    "\n",
    "  - Select **Delete**.\n",
    "\n",
    "**NOTE:**  The `create_declarative_pipeline` function is a custom function built for this course to create the sample pipeline using the Databricks REST API. This avoids manually creating the pipeline and referencing the pipeline assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef00fe58-dd3e-4466-9e3c-4e0f0a000ae9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create pipeline 3"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Lakeflow Declarative Pipeline '3 - Adding Data Quality Expectations Project - labuser11058730_1754017152'...\nRoot folder path: /Workspace/Users/labuser11058730_1754017152@vocareum.com/build-data-pipelines-with-lakeflow-declarative-pipelines-3.0.2/Build Data Pipelines with Lakeflow Declarative Pipelines/3 - Adding Data Quality Expectations Project\nSource folder path(s): [{'glob': {'include': '/Workspace/Users/labuser11058730_1754017152@vocareum.com/build-data-pipelines-with-lakeflow-declarative-pipelines-3.0.2/Build Data Pipelines with Lakeflow Declarative Pipelines/3 - Adding Data Quality Expectations Project/orders/**'}}]\n\nLakeflow Declarative Pipeline Creation '3 - Adding Data Quality Expectations Project - labuser11058730_1754017152' Complete!\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "create_declarative_pipeline(pipeline_name=f'3 - Adding Data Quality Expectations Project - {DA.catalog_name}', \n",
    "                            root_path_folder_name='3 - Adding Data Quality Expectations Project',\n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            source_folder_names=['orders'],\n",
    "                            configuration = {'source':DA.paths.working_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e9d5b7-4201-47c5-95ba-a03423cbd8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to open the starter pipeline for this demonstration:\n",
    "\n",
    "   a. Click the folder icon ![Folder](./Includes/images/folder_icon.png) in the left navigation panel.\n",
    "   \n",
    "   b. In the **Build Data Pipelines with Lakeflow Declarative Pipelines** folder, find the **3 - Adding Data Quality Expectations Project** folder.\n",
    "   \n",
    "   c. Right-click and select **Open in a new tab**.\n",
    "\n",
    "   d. In the new tab:\n",
    "      - Select the **orders** folder (The main folder also contains the extra **python_excluded** folder that contains the Python version)\n",
    "\n",
    "      - Click on **orders_pipeline.sql**.\n",
    "      \n",
    "\n",
    "   e. In the navigation pane of the new tab, you should see **Pipeline** and **All Files**. Ensure you are in the **Pipeline** tab. This will list all files in your pipeline.\n",
    "   <br></br>\n",
    "   **Example**\n",
    "   \n",
    "   ![Pipeline and All Files Tab](./Includes/images/pipeline_projecttabs.png)\n",
    "\n",
    "#### IMPORTANT\n",
    "   **NOTE:** If you open the **orders_pipeline.sql** file and it does not open up the pipeline editor, that is because that folder is not associated with a pipeline. Please make sure to run the previous cell to associate the folder with the pipeline and try again.\n",
    "\n",
    "   **WARNING:** If you get the following warning when opening the **orders_pipeline.sql** file: \n",
    "\n",
    "   ```pipeline you are trying to access does not exist or is inaccessible. Please verify the pipeline ID, request access or detach this file from the pipeline.``` \n",
    "\n",
    "   Simply refresh the page and/or reselect the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a24f3f45-b5d8-4249-a8d0-ef52fa522caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. In the new tab, follow the instructions provided in the comments within the **orders_pipeline.sql** file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4768e28-57e6-44ec-8c74-a20d3590bff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/dlt/expectations)\n",
    "\n",
    "- [Expectation recommendations and advanced patterns](https://docs.databricks.com/aws/en/dlt/expectation-patterns)\n",
    "\n",
    "- [Data Quality Management With Databricks](https://www.databricks.com/discover/pages/data-quality-management#expectations-with-delta-live-tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cbd6e2d-9b9a-4ba7-8ce9-f6998365b24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 841731085777252,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3 - Adding Data Quality Expectations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}