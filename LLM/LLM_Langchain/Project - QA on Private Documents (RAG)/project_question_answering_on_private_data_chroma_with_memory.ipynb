{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640e4bbc",
   "metadata": {},
   "source": [
    "# Project: Question-Answering on Private Documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b748fd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13727cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2732726-ea0e-4926-a8ae-cb086d856aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade -q openai langchain tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07900179",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1ae19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bb8ad",
   "metadata": {},
   "source": [
    "## Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62c3cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b298177",
   "metadata": {},
   "source": [
    "## Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "add191c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f92afa",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c72c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    \n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name in pc.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(\n",
    "                environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52ef1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a94cb",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08b0a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efe8fc",
   "metadata": {},
   "source": [
    "## Using Chroma as a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a67c9969-9a26-4763-a303-45717144eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01aaa44b-fc99-4021-8307-6091e071a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "432555ac-a13f-4e6f-8e5f-e8b350970a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4938e99-ad31-4093-a5e3-e774fcbd6385",
   "metadata": {},
   "source": [
    "#### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "947584f7-3d74-4562-a4b0-a4588fdc57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "# Loading the pdf document into LangChain \n",
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2625b802-00f7-47ae-b093-5e55cffdbea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is Vertex AI Search?', 'result': \"Vertex AI Search is a component of Google's Vertex AI platform that offers customizable answers, search tuning, vector search, grounding, and compliance updates tailored for enterprises. It provides new generative AI capabilities and enterprise-ready features to enhance search functionalities and information retrieval for businesses.\"}\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "q = 'What is Vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b76d77f8-aaa7-43b4-83cd-c4087afe718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Search is a component of Google's Vertex AI platform that offers customizable answers, search tuning, vector search, grounding, and compliance updates tailored for enterprises. It provides new generative AI capabilities and enterprise-ready features to enhance search functionalities and information retrieval for businesses.\n"
     ]
    }
   ],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "700b0c4d-a247-4aa4-b7c4-93c3e656c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How many pairs of questions and answers had the StackOverflow dataset?', 'result': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "# Load a Chroma vector store from the specified directory (default ./chroma_db) \n",
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ba55a05-d1c4-4034-b883-967f06a70aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have a specific number from the provided context to multiply by 2.\n"
     ]
    }
   ],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "q = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259f6a4-a2ad-48f2-b414-225b35a49cc5",
   "metadata": {},
   "source": [
    "## Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93410211-0cb2-4524-9e90-7d7f67173704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
    "\n",
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)  \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38309528-51c9-4b4a-b8d4-4b26d3163173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ask questions\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c0632a8-b463-43e9-91fe-d7d7d8135260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e143255-faac-4806-abdb-c2b55b82bf95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='The StackOverflow dataset had 8 million pairs of questions and answers.')], 'answer': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bfda3a7-409c-4511-a698-0165b6acad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cb937c3-8769-44af-9afe-fc49c3d5a1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = 'Multiply that number by 10.'\n",
    "result = ask_question(q, crc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1848f53-68b5-4265-bf0c-f02b50028fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 8 million pairs of questions and answers by 10 would be 80 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0120ceea-f701-44de-838c-a4edec180c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of dividing 80 million pairs of questions and answers by 80 is 1 million.\n"
     ]
    }
   ],
   "source": [
    "q = 'Devide the result by 80.'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39f373d4-13fb-4ee9-9c22-c0072a24be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?'\n",
      "content='The StackOverflow dataset had 8 million pairs of questions and answers.'\n",
      "content='Multiply that number by 10.'\n",
      "content='The result of multiplying 8 million pairs of questions and answers by 10 would be 80 million pairs of questions and answers.'\n",
      "content='Devide the result by 80.'\n",
      "content='The result of dividing 80 million pairs of questions and answers by 80 is 1 million.'\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c84af-793b-4f40-ba8d-ed57170788e2",
   "metadata": {},
   "source": [
    "### Loop for asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efcd92ad-cc2e-47b0-b8da-112aa8a4c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Tell me about Google Search technologies as described in the document.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document mentions that Google Search technologies have enabled search experiences for public or internal websites, mobile applications, and various enterprise search services. It also highlights that the product is the result of deep collaboration between Google Search and other entities.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Is Vertex AI Search a fully-managed platform?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Vertex AI Search is a fully-managed platform that offers customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  What advancements have been made in semantic search?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancements in semantic search include the ability to understand the context and intent behind a user's query, providing more relevant search results. This is achieved through natural language processing, entity recognition, and machine learning algorithms that help search engines understand the meaning of words and phrases in a query. Additionally, semantic search allows for better understanding of user intent, enabling search engines to deliver more accurate and personalized results.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  bye\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, crc)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808125d7-5026-4771-b7ec-72b8b35c42dc",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "825f97e1-eb73-4fb6-b253-13e564dd6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "Before answering translate your response to Spanish.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5667186-1562-4c68-b479-725824f9974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\nUse the following pieces of context to answer the user\\'s question.\\nBefore answering translate your response to Spanish.\\nIf you don\\'t find the answer in the provided context, just respond \"I don\\'t know.\"\\n---------------\\nContext: ```{context}```\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='\\nQuestion: ```{question}```\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0df828fa-8d84-4753-8a0a-dc11d4f1f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Spanish.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-```\n",
      "\n",
      "Human: \n",
      "Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nContexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```')], 'answer': 'El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nContexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffdef3a8-07ff-4671-9502-096d8e8655ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\n",
      "\n",
      "Contexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```\n",
      "Follow Up Input: When was Elon Musk born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Spanish.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```Follow usTelecommunications\n",
      "By Ankur Jain • 7-minute readGaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow usTelecommunications\n",
      "By Ankur Jain • 7-minute readGaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow usTelecommunications\n",
      "By Ankur Jain • 7-minute readGaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow usTelecommunications\n",
      "By Ankur Jain • 7-minute readGaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow usTelecommunications\n",
      "By Ankur Jain • 7-minute readGaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute readMWC’24: Unlocking the AI-enabled Telco with```\n",
      "\n",
      "Human: \n",
      "Question: ```¿Cuándo nació Elon Musk?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'When was Elon Musk born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nContexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```'), HumanMessage(content='When was Elon Musk born?'), AIMessage(content=\"I don't know.\")], 'answer': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'When was Elon Musk born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e556f294-1ff1-47cb-b83b-cc2cee79c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\n",
      "\n",
      "Contexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```\n",
      "Human: When was Elon Musk born?\n",
      "Assistant: I don't know.\n",
      "Follow Up Input: When was Bill Gates born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Spanish.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-```\n",
      "\n",
      "Human: \n",
      "Question: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'When was Bill Gates born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='El conjunto de datos de StackOverflow tenía 8 millones de pares de preguntas y respuestas.\\n\\nContexto: ```la búsqueda de similitud simple fue altamente efectiva porque el conjunto de datos tenía 8 millones de pares de preguntas y respuestas. Sin embargo, los conjuntos de datos generalmente no contienen preguntas y respuestas preexistentes o consultas y respuestas preexistentes.```'), HumanMessage(content='When was Elon Musk born?'), AIMessage(content=\"I don't know.\"), HumanMessage(content='When was Bill Gates born?'), AIMessage(content='No sé.')], 'answer': 'No sé.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'When was Bill Gates born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c08ea5c3-8a5f-4447-8d40-dd13150fb6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sé.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4822533-ed90-4adb-8c53-13d605b6bb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
