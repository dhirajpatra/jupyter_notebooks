{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc",
   "metadata": {},
   "source": [
    "# Deep Dive into LangChain\n",
    "## LLMs, Prompt Templates, Caching, Streaming, Chains\n",
    "\n",
    "This notebook uses the latest versions of the OpenAI and LangChain libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72695b0b-0069-484d-9edd-9b75faaa8307",
   "metadata": {},
   "source": [
    "Download [requirements.txt](https://drive.google.com/file/d/1UpURYL9kqjXfe9J8o-_Dq5KJTbQpzMef/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade  -q langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the API Keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654afaf9-00bc-4d4d-99ab-907055d4139c",
   "metadata": {},
   "source": [
    "## Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2eb526e-4105-4301-b931-f285f258035b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of particles at the smallest scales, where traditional laws of physics no longer apply and instead, phenomena such as superposition and entanglement occur.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()  \n",
    "\n",
    "# invoking the llm (running the prompt)\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', temperature=0.1)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b5b306-6905-4c58-82ef-17cc9be9777d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(ChatOpenAI)  # see the llm constructor arguments with its defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf atomarer und subatomarer Ebene durch Wellenfunktionen und Wahrscheinlichkeitsverteilungen.\n"
     ]
    }
   ],
   "source": [
    "# using Chat Completions API Messages: System, Assistant and Human\n",
    "from langchain.schema import(\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e837c-d307-4165-ad99-551a2bee6cf5",
   "metadata": {},
   "source": [
    "## Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379171e-2081-4cc1-a834-89ebff34e84c",
   "metadata": {},
   "source": [
    "### 1. In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d356c163-afd3-4534-9947-69e088f16913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20232db3-97a5-498e-805f-fbcb24d14584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 947 μs, total: 21.8 ms\n",
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the cookie go to the doctor? Because it was feeling crumbly!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 μs, sys: 0 ns, total: 308 μs\n",
      "Wall time: 312 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the cookie go to the doctor? Because it was feeling crumbly!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2",
   "metadata": {},
   "source": [
    "### 2. SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9271fe21-1ca3-4cb2-8599-9a368071482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 ms, sys: 33.4 ms, total: 83.8 ms\n",
      "Wall time: 83 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 ms, sys: 193 μs, total: 2.31 ms\n",
      "Wall time: 1.75 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aa044-5842-4d50-b3e9-0207421df1cf",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49",
   "metadata": {},
   "source": [
    "## PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced virologist.\\nWrite a few sentences about the following virus \"hiv\" in german.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for the prompt\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Fill in the variable: virus and language\n",
    "prompt = prompt_template.format(virus='hiv', language='german')\n",
    "prompt  # Returns the generated prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV, das humane Immundefizienzvirus, ist ein Retrovirus, das das menschliche Immunsystem schwächt und zu AIDS führen kann. Es wird hauptsächlich durch ungeschützten Geschlechtsverkehr, den Austausch von infizierten Nadeln oder von der Mutter auf das Kind während der Schwangerschaft übertragen. Es gibt keine Heilung für HIV, aber mit antiretroviralen Medikamenten kann die Krankheit gut kontrolliert werden. Prävention und frühzeitige Diagnose sind entscheidend im Kampf gegen HIV.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"United States\",\n",
      "    \"4\": \"Indonesia\",\n",
      "    \"5\": \"Pakistan\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eb160-df97-4229-8115-2e8864b400d3",
   "metadata": {},
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experience virologist.\n",
      "Write a few sentences about the following virus \"HSV\" in Spanish.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'HSV', 'language': 'Spanish', 'text': 'El virus del herpes simple, o HSV por sus siglas en inglés, es un virus de doble cadena de ADN que pertenece a la familia de los herpesvirus. Este virus se clasifica en dos tipos: HSV-1, que suele causar herpes labial, y HSV-2, que es la principal causa de herpes genital. Ambos tipos de HSV pueden causar infecciones recurrentes y se transmiten principalmente a través del contacto directo con una persona infectada.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22466/1243800677.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experience virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce8fb5a-542b-4ad6-a53e-e2f6f085ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El virus del herpes simple, o HSV por sus siglas en inglés, es un virus de doble cadena de ADN que pertenece a la familia de los herpesvirus. Este virus se clasifica en dos tipos: HSV-1, que suele causar herpes labial, y HSV-2, que es la principal causa de herpes genital. Ambos tipos de HSV pueden causar infecciones recurrentes y se transmiten principalmente a través del contacto directo con una persona infectada.\n"
     ]
    }
   ],
   "source": [
    "# if the code in the cell above gives an LLMChain deprecation error replace:\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=prompt_template,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# with: \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Country:  Italy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of Italy?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Top 3 places to visit in Rome:\n",
      "- The Colosseum\n",
      "- The Vatican City (including St. Peter's Basilica and the Sistine Chapel)\n",
      "- The Roman Forum\n"
     ]
    }
   ],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "\n",
    "# Invoke the chain with specific virus and language values\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963536b8-76e7-4681-bb0d-8309795e2fa3",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is an example of a function that implements linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    X = np.array(X)\n",
      "    y = np.array(y)\n",
      "    \n",
      "    # Calculate the slope (m) and intercept (b) using the least squares method\n",
      "    X_mean = np.mean(X)\n",
      "    y_mean = np.mean(y)\n",
      "    \n",
      "    m = np.sum((X - X_mean) * (y - y_mean)) / np.sum((X - X_mean)**2)\n",
      "    b = y_mean - m * X_mean\n",
      "    \n",
      "    return m, b\n",
      "\n",
      "# Example usage\n",
      "X = [1, 2, 3, 4, 5]\n",
      "y = [2, 4, 5, 4, 5]\n",
      "\n",
      "m, b = linear_regression(X, y)\n",
      "print(f\"Slope: {m}, Intercept: {b}\")\n",
      "```\n",
      "\n",
      "This function takes two arrays `X` and `y` as input, where `X` represents the independent variable and `y` represents the dependent variable. It calculates the slope `m` and intercept `b` of the linear regression line using the least squares method and returns them.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Python function `linear_regression` is designed to perform a linear regression analysis, which is a basic statistical approach for modeling the relationship between a dependent variable (`y`) and one independent variable (`X`). The goal is to find the best-fitting straight line through the points represented by the `X` and `y` data sets.\n",
      "\n",
      "Let's deconstruct this function step by step:\n",
      "\n",
      "### Importing Necessary Library\n",
      "\n",
      "- **`import numpy as np`** : This line imports the NumPy library with the alias `np`. NumPy is a powerful library for numerical computations in Python, offering support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions to perform operations on these arrays.\n",
      "\n",
      "### Function Definition\n",
      "\n",
      "- **`def linear_regression(X, y):`** : This line defines the function `linear_regression` with two parameters, `X` and `y`. `X` represents the array of independent variable(s), and `y` is the array of corresponding dependent variable values.\n",
      "\n",
      "### Converting Inputs\n",
      "\n",
      "- **`X = np.array(X)`**\n",
      "- **`y = np.array(y)`**\n",
      "    \n",
      "These lines convert the input lists `X` and `y` into NumPy arrays if they aren't already. This conversion ensures that mathematical operations can be applied to these inputs efficiently.\n",
      "\n",
      "### Mathematical Calculations\n",
      "\n",
      "1. **Calculate Means**:\n",
      "    - `X_mean = np.mean(X)`\n",
      "    - `y_mean = np.mean(y)`\n",
      "    \n",
      "   These lines calculate the means of arrays `X` and `y` respectively, using the `np.mean()` method. The mean values `X_mean` and `y_mean` are used in the calculation of the slope and intercept of the best fitting line.\n",
      "   \n",
      "2. **Calculate Slope (m)**:\n",
      "   \n",
      "   - `m = np.sum((X - X_mean) * (y - y_mean)) / np.sum((X - X_mean)**2)`\n",
      "   \n",
      "   This line calculates the slope (`m`) of the line of best fit using the least squares method. The numerator consists of the sum of products of differences (`X - X_mean`) and (`y - y_mean`), essentially calculating the covariance of `X` and `y`. The denominator is the sum of squares of `X - X_mean`, which measures the variance of `X`. Dividing these gives the slope of the regression line.\n",
      "   \n",
      "3. **Calculate Intercept (b)**:\n",
      "   \n",
      "   - `b = y_mean - m * X_mean`\n",
      "   \n",
      "   Once the slope (`m`) is known, this line calculates the y-intercept (`b`) of the line. Here, the mean of `y` (i.e., `y_mean`) is adjusted by the product of the slope and the mean of `X` (i.e., `X_mean`). This calculation ensures that the regression line will pass through the means of `X` and `y`, which is a characteristic of linear regression.\n",
      "\n",
      "### Returning Results\n",
      "\n",
      "- **`return m, b`**\n",
      "   \n",
      "   Finally, the function returns the slope (`m`) and y-intercept (`b`) of the line of best fit as calculated by the linear regression model.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "- After defining the linear regression function, it is then applied to an example set of data points `X` and `y`, highlighting how the function can be employed in practice to obtain the slope and intercept for linear regression analysis. \n",
      "\n",
      "### Mathematical Essence \n",
      "\n",
      "At its core, this linear regression implementation utilizes a formula derived from the least squares method, aimed at minimizing the sum of the squares of the differences between observed and predicted values. By finding values of `m` and `b` that minimize these differences, a line of best fit is identified that best represents the relationship between the independent (`X`) and dependent (`y`) variables within the data.\n",
      "\n",
      "This approach in statistics is widely employed for predictive modeling, understanding relationships between variables, and data analysis in various fields ranging from finance to biomedical sciences.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python function `linear_regression` is designed to perform a linear regression analysis, which is a basic statistical approach for modeling the relationship between a dependent variable (`y`) and one independent variable (`X`). The goal is to find the best-fitting straight line through the points represented by the `X` and `y` data sets.\n",
      "\n",
      "Let's deconstruct this function step by step:\n",
      "\n",
      "### Importing Necessary Library\n",
      "\n",
      "- **`import numpy as np`** : This line imports the NumPy library with the alias `np`. NumPy is a powerful library for numerical computations in Python, offering support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions to perform operations on these arrays.\n",
      "\n",
      "### Function Definition\n",
      "\n",
      "- **`def linear_regression(X, y):`** : This line defines the function `linear_regression` with two parameters, `X` and `y`. `X` represents the array of independent variable(s), and `y` is the array of corresponding dependent variable values.\n",
      "\n",
      "### Converting Inputs\n",
      "\n",
      "- **`X = np.array(X)`**\n",
      "- **`y = np.array(y)`**\n",
      "    \n",
      "These lines convert the input lists `X` and `y` into NumPy arrays if they aren't already. This conversion ensures that mathematical operations can be applied to these inputs efficiently.\n",
      "\n",
      "### Mathematical Calculations\n",
      "\n",
      "1. **Calculate Means**:\n",
      "    - `X_mean = np.mean(X)`\n",
      "    - `y_mean = np.mean(y)`\n",
      "    \n",
      "   These lines calculate the means of arrays `X` and `y` respectively, using the `np.mean()` method. The mean values `X_mean` and `y_mean` are used in the calculation of the slope and intercept of the best fitting line.\n",
      "   \n",
      "2. **Calculate Slope (m)**:\n",
      "   \n",
      "   - `m = np.sum((X - X_mean) * (y - y_mean)) / np.sum((X - X_mean)**2)`\n",
      "   \n",
      "   This line calculates the slope (`m`) of the line of best fit using the least squares method. The numerator consists of the sum of products of differences (`X - X_mean`) and (`y - y_mean`), essentially calculating the covariance of `X` and `y`. The denominator is the sum of squares of `X - X_mean`, which measures the variance of `X`. Dividing these gives the slope of the regression line.\n",
      "   \n",
      "3. **Calculate Intercept (b)**:\n",
      "   \n",
      "   - `b = y_mean - m * X_mean`\n",
      "   \n",
      "   Once the slope (`m`) is known, this line calculates the y-intercept (`b`) of the line. Here, the mean of `y` (i.e., `y_mean`) is adjusted by the product of the slope and the mean of `X` (i.e., `X_mean`). This calculation ensures that the regression line will pass through the means of `X` and `y`, which is a characteristic of linear regression.\n",
      "\n",
      "### Returning Results\n",
      "\n",
      "- **`return m, b`**\n",
      "   \n",
      "   Finally, the function returns the slope (`m`) and y-intercept (`b`) of the line of best fit as calculated by the linear regression model.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "- After defining the linear regression function, it is then applied to an example set of data points `X` and `y`, highlighting how the function can be employed in practice to obtain the slope and intercept for linear regression analysis. \n",
      "\n",
      "### Mathematical Essence \n",
      "\n",
      "At its core, this linear regression implementation utilizes a formula derived from the least squares method, aimed at minimizing the sum of the squares of the differences between observed and predicted values. By finding values of `m` and `b` that minimize these differences, a line of best fit is identified that best represents the relationship between the independent (`X`) and dependent (`y`) variables within the data.\n",
      "\n",
      "This approach in statistics is widely employed for predictive modeling, understanding relationships between variables, and data analysis in various fields ranging from finance to biomedical sciences.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a8cee1-18fa-4068-a234-defdb7ed2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 12 using the math module's factorial function. Then, I will calculate the square root of that result using the math module's sqrt function. Finally, I will format the result to display it with 4 decimal points using the format function.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: print(\"{:.4f}\".format(sqrt_factorial_12))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the answer to 5.1 ** 7.3?', 'output': '146306.05007233328'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e936c363-5720-443b-b4f9-29d0b5263536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the answer to 5.1 ** 7.3?\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146306.05007233328\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9fda3-fcfd-4818-85b5-131304070ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
