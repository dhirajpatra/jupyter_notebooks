<<<<<<< HEAD
{"cells":[{"cell_type":"code","execution_count":1,"id":"fe52be3d-c6bf-4422-bb71-1c90dc8225e0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":682},"id":"fe52be3d-c6bf-4422-bb71-1c90dc8225e0","executionInfo":{"status":"error","timestamp":1750240645080,"user_tz":-330,"elapsed":19972,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}},"outputId":"c9883369-b030-4a9f-deeb-4335884330cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n","Looking in indexes: https://download.pytorch.org/whl/cu121\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n","\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m493.2/780.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4145874900>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For CUDA 12.1+ (more likely on your server)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch --index-url https://download.pytorch.org/whl/cu121'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Core libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n","\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Check CUDA version first\n","!nvcc --version\n","\n","# For CUDA 12.1+ (more likely on your server)\n","!pip install torch --index-url https://download.pytorch.org/whl/cu121\n","\n","# Core libraries\n","!pip install transformers==4.40.2\n","!pip install datasets\n","!pip install peft\n","!pip install trl\n","!pip install bitsandbytes\n","!pip install accelerate\n","\n","# Flash attention (improved installation)\n","!pip install packaging wheel\n","# !pip install flash-attn --no-build-isolation\n","\n","# Tokenization and data handling\n","!pip install sentencepiece\n","!pip install protobuf\n","\n","# Evaluation metrics\n","!pip install evaluate\n","!pip install rouge-score\n","!pip install sacrebleu\n","\n","# Monitoring options\n","!pip install wandb\n","!pip install tensorboard\n","!pip install pynvml\n","!pip install psutil\n","\n","# TensorRT optimization\n","!pip install tensorrt\n","\n","# uninstall not required but already installed\n","!pip uninstall -y torchvision torchaudio"]},{"cell_type":"code","source":["!pip show transformers\n","\n","from transformers import PreTrainedModel, Trainer\n","print(\"PreTrainedModel and Trainer imported successfully!\")"],"metadata":{"id":"7wSDjygMX6i8","executionInfo":{"status":"aborted","timestamp":1750240645113,"user_tz":-330,"elapsed":30,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"id":"7wSDjygMX6i8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6e718c64-a0cb-47c8-8c41-676bbf532471","metadata":{"id":"6e718c64-a0cb-47c8-8c41-676bbf532471","executionInfo":{"status":"aborted","timestamp":1750240645114,"user_tz":-330,"elapsed":20080,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import torch\n","import os\n","import gc\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"codellama_cobol_finetune.ipynb\"\n","\n","# ‚úÖ Ensure CUDA is available\n","assert torch.cuda.is_available(), \"CUDA GPU not available.\"\n","device = torch.device(\"cuda\")\n","\n","# ‚úÖ Display GPU information\n","print(f\"üñ•Ô∏è  GPU : {torch.cuda.get_device_name(0)}\")\n","print(f\"üöÄ CUDA Version : {torch.version.cuda}\")\n","print(f\"üß† Total Memory : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n","\n","# ‚úÖ Environment configs\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid tokenizer deadlocks in multi-threading\n","\n","# ‚úÖ GPU optimization for RTX 4090 (TensorFloat-32 support)\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = False\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# ‚úÖ Clear GPU memory cache\n","torch.cuda.empty_cache()\n","\n","# (Optional) limit memory usage, uncomment if needed\n","# torch.cuda.set_per_process_memory_fraction(0.95)\n"]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"id":"eqrTUNRgWYwz","executionInfo":{"status":"aborted","timestamp":1750240645115,"user_tz":-330,"elapsed":20080,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"id":"eqrTUNRgWYwz","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aa8e1620-afef-4ded-8dc9-be22e2c979c2","metadata":{"id":"aa8e1620-afef-4ded-8dc9-be22e2c979c2","executionInfo":{"status":"aborted","timestamp":1750240645116,"user_tz":-330,"elapsed":20080,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["#!/usr/bin/env python3\n","\"\"\"\n","Optimized Fine-tuning Pipeline for CodeLlama COBOL to Python Conversion\n","Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n","\"\"\"\n","import os\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"codellama_cobol_finetune.ipynb\"\n","\n","import gc\n","import json\n","import warnings\n","from typing import Dict, List, Optional\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.distributed as dist\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    BitsAndBytesConfig,\n","    EarlyStoppingCallback,\n","    CodeLlamaTokenizer,  # Specific for CodeLlama\n","    LlamaForCausalLM     # Specific for CodeLlama\n",")\n","\n","from datasets import load_dataset, Dataset, concatenate_datasets\n","from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n","from trl import SFTTrainer\n","\n","# Monitoring and optimization\n","import wandb\n","import tensorrt as trt\n","import pynvml\n","from evaluate import load as load_metric\n","\n","# Suppress warnings for cleaner output\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"]},{"cell_type":"code","execution_count":null,"id":"6a1071f3-eec8-449c-9132-6addd972a8b3","metadata":{"id":"6a1071f3-eec8-449c-9132-6addd972a8b3","executionInfo":{"status":"aborted","timestamp":1750240645116,"user_tz":-330,"elapsed":20078,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# NVIDIA optimizations for RTX 4090\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Async kernel launches for speed\n","\n","# RTX 4090 specific optimizations\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = False\n","torch.backends.cuda.matmul.allow_tf32 = True  # Ada Lovelace TF32 support\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# Model configuration\n","# model_name = \"codellama/CodeLlama-7B-Instruct-hf\"\n","model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n","# output_model = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n","output_model = \"deepseek-coder-1.3b-instruct-COBOL-to-Python\"\n","\n","# Training configuration\n","max_seq_length = 2048  # Adjust based on your COBOL/Python code lengths\n","batch_size = 4         # Start with 4, can increase with 24GB VRAM\n","gradient_accumulation_steps = 4  # Effective batch size = 16\n","\n","print(f\"Using model: {model_name}\")\n","print(f\"Output will be saved as: {output_model}\")\n","print(f\"Max sequence length: {max_seq_length}\")"]},{"cell_type":"code","execution_count":null,"id":"c5724fd2-a2d6-46b3-b6a5-45e947868884","metadata":{"id":"c5724fd2-a2d6-46b3-b6a5-45e947868884","executionInfo":{"status":"aborted","timestamp":1750240645117,"user_tz":-330,"elapsed":20077,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from huggingface_hub import snapshot_download\n","\n","local_dir = snapshot_download(repo_id=model_name, local_dir=\"./models/codellama\")\n"]},{"cell_type":"code","execution_count":null,"id":"246b69f7-0d58-431b-bf2f-11ee5b4d2034","metadata":{"id":"246b69f7-0d58-431b-bf2f-11ee5b4d2034","executionInfo":{"status":"aborted","timestamp":1750240645117,"user_tz":-330,"elapsed":20076,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["%%capture\n","# Core packages for fine-tuning\n","%pip install -U -q transformers datasets peft accelerate huggingface_hub trl\n","\n","# Quantization and optimization\n","%pip install -U -q bitsandbytes optimum auto-gptq\n","\n","# Memory and attention optimization\n","%pip install -U -q xformers --no-deps\n","\n","# Flash attention for better performance (if not already installed)\n","# %pip install -U -q flash-attn --no-build-isolation\n","\n","# Evaluation metrics for code conversion\n","%pip install -U -q evaluate rouge-score sacrebleu"]},{"cell_type":"code","execution_count":null,"id":"a953deb9-5254-4ca3-b6a4-a0ffd4f0657f","metadata":{"id":"a953deb9-5254-4ca3-b6a4-a0ffd4f0657f","executionInfo":{"status":"aborted","timestamp":1750240645166,"user_tz":-330,"elapsed":48,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["class OptimizedCodeLlamaFineTuner:\n","    \"\"\"\n","    A comprehensive class designed for efficient fine-tuning of CodeLlama models,\n","    specifically optimized for tasks like COBOL to Python code translation.\n","\n","    This class leverages cutting-edge techniques such as Parameter-Efficient Fine-Tuning\n","    (PEFT) using LoRA (Low-Rank Adaptation) and 4-bit quantization with bitsandbytes\n","    to enable training large language models on consumer-grade GPUs or environments\n","    with limited memory.\n","\n","    Key Features:\n","    -   **Memory-Efficient Model Loading**: Supports loading CodeLlama models in 4-bit\n","        quantization (`bnb.nn.Linear4bit`) with configurable double quantization\n","        and compute data types (e.g., bfloat16 for RTX 40 series GPUs).\n","    -   **LoRA Integration**: Seamlessly configures and applies LoRA adapters to\n","        target specific linear layers (e.g., query, key, value projections) for\n","        efficient fine-tuning without modifying the full model weights.\n","    -   **Automated Target Module Detection**: Can automatically identify all\n","        4-bit linear layers for LoRA application if not explicitly specified.\n","    -   **Dataset Handling**: Facilitates loading and preprocessing of custom datasets,\n","        supporting instruction-tuning formats (e.g., \"### Instruction:\\n...### Response:\\n\").\n","    -   **Optimized Training Arguments**: Sets up `transformers.TrainingArguments`\n","        with best practices for memory efficiency (gradient accumulation, gradient\n","        checkpointing, paged optimizers, bfloat16/fp16 precision, mixed-precision training).\n","    -   **Supervised Fine-Tuning (SFT)**: Utilizes `trl.SFTTrainer` for streamlined\n","        supervised fine-tuning, handling data collators, tokenization, and training loops.\n","    -   **Flexible Evaluation**: Provides methods for generating predictions from the\n","        fine-tuned model and calculating key evaluation metrics like Exact Match Accuracy\n","        and Average Similarity Score (using SequenceMatcher).\n","    -   **Model Management**: Supports saving LoRA adapters, merging them with the\n","        base model for a standalone full model, and pushing the final model/tokenizer\n","        to the Hugging Face Hub for easy sharing and deployment.\n","    -   **CUDA Memory Optimization**: Includes environment variable settings to help\n","        avoid CUDA memory fragmentation, improving stability during training.\n","\n","    Usage Workflow:\n","    1.  **Initialization**: Instantiate `OptimizedCodeLlamaFineTuner` with a comprehensive\n","        configuration dictionary (`config_dict`) specifying model, LoRA, and training parameters.\n","    2.  **Load Model & Tokenizer**: Call `load_model_and_tokenizer()` to prepare the base model.\n","    3.  **Setup LoRA**: Invoke `setup_lora()` to apply PEFT adapters to the model.\n","    4.  **Load Datasets**: Use `load_your_specific_datasets()` to prepare your training and\n","        evaluation data.\n","    5.  **Create Trainer**: Call `create_and_run_trainer()` to get an initialized\n","        `SFTTrainer` instance.\n","    6.  **Train**: Execute `trainer.train()` to start the fine-tuning process.\n","    7.  **Evaluate**: Use `evaluate_model()` to generate predictions and\n","        `calculate_and_print_metrics()` to assess performance.\n","    8.  **Merge & Save**: Merge the LoRA adapters into the base model using `PeftModel.from_pretrained`\n","        and `merge_and_unload()`, then save the final model with `save_pretrained()`\n","        or push to the Hugging Face Hub with `push_to_hub()`.\n","\n","    Parameters:\n","        config (dict): A dictionary containing all necessary configuration parameters\n","                       for model loading, LoRA setup, and training. Example parameters\n","                       include `model_name`, `load_in_4bit`, `lora_r`, `lora_alpha`,\n","                       `num_train_epochs`, `per_device_train_batch_size`, `output_dir`, etc.\n","    \"\"\"\n","    def __init__(self, config):\n","        self.config = config\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.setup_nvidia_environment()\n","        self.tokenizer = None\n","        self.model = None\n","        self.datasets = []  # Support for multiple datasets\n","        self.combined_dataset = None\n","\n","    def setup_nvidia_environment(self):\n","        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n","        if not torch.cuda.is_available():\n","            raise RuntimeError(\"CUDA is not available!\")\n","\n","        # Initialize NVML for GPU monitoring\n","        pynvml.nvmlInit()\n","        gpu_count = pynvml.nvmlDeviceGetCount()\n","\n","        print(f\"NVIDIA Setup:\")\n","        print(f\"   - CUDA Version: {torch.version.cuda}\")\n","        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n","        print(f\"   - Available GPUs: {gpu_count}\")\n","\n","        for i in range(gpu_count):\n","            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n","            name = pynvml.nvmlDeviceGetName(handle)\n","            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n","            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n","\n","        # Enable TensorFloat-32 for RTX 4090 (Ada Lovelace)\n","        if torch.cuda.get_device_capability()[0] >= 8:\n","            torch.backends.cuda.matmul.allow_tf32 = True\n","            torch.backends.cudnn.allow_tf32 = True\n","            print(\"   - TensorFloat-32 enabled for RTX 4090\")\n","\n","    # Apply tokenization correctly before training\n","    # REPLACE your existing tokenize_function with this:\n","    def tokenize_function(self, examples):\n","        \"\"\"Tokenize examples with memory management\"\"\"\n","        result = self.tokenizer(\n","            examples[\"text\"],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.config.get(\"max_length\", 2048),\n","            return_tensors=None  # return plain Python objects\n","        )\n","\n","        # Force garbage collection for large batches to prevent memory buildup\n","        if len(examples[\"text\"]) > 500:\n","            gc.collect()\n","\n","        return result\n","\n","    # Add this at the top of your class or main script\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","\n","    # Check for flash attention availability\n","    def check_attention_backend():\n","        try:\n","            import flash_attn\n","            print(\"‚úÖ Using Flash Attention\")\n","            return \"flash_attn\"\n","        except ImportError:\n","            try:\n","                import xformers\n","                print(\"‚úÖ Using xformers attention\")\n","                return \"xformers\"\n","            except ImportError:\n","                print(\"‚ÑπÔ∏è Using native PyTorch attention (slower but stable)\")\n","                return \"native\"\n","\n","    # Add this to your model loading function\n","    def load_model_and_tokenizer(self):\n","        \"\"\"Load CodeLlama model and prepare for LoRA fine-tuning\"\"\"\n","\n","        # Check attention backend\n","        attention_backend = check_attention_backend()\n","\n","        model_name = self.config.get(\"model_name\", model_name)\n","\n","        print(\"üî§ Loading CodeLlama tokenizer...\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            model_name,\n","            trust_remote_code=True,\n","            padding_side=\"right\",\n","            add_eos_token=True,\n","            add_bos_token=True\n","        )\n","\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        print(\"üß† Loading CodeLlama model...\")\n","\n","        # Model loading configuration based on available attention\n","        model_kwargs = {\n","            \"torch_dtype\": torch.bfloat16,\n","            \"device_map\": \"auto\",\n","            \"trust_remote_code\": True,\n","            \"low_cpu_mem_usage\": True,\n","        }\n","\n","        # Add flash attention config if available\n","        if attention_backend == \"flash_attn\":\n","            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n","        elif attention_backend == \"native\":\n","            # Use native attention with optimizations\n","            model_kwargs[\"torch_compile\"] = False  # Disable compile for stability\n","\n","        self.model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            **model_kwargs\n","        )\n","\n","        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n","\n","        # Clear cache before LoRA\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        print(f\"‚úÖ Model loaded successfully with {attention_backend} attention\")\n","        self.setup_lora()\n","\n","    def load_your_specific_datasets(self):\n","        \"\"\"Load and prepare your specific datasets for COBOL to Python conversion\"\"\"\n","        print(\"Loading your specific datasets...\")\n","\n","        # Add size limits from the start\n","        max_samples_per_dataset = self.config.get(\"max_samples_per_dataset\", 5000)\n","\n","        # Load your datasets with limits\n","        print(\"Loading MainframeBench COBOL dataset...\")\n","        mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\",\n","                               split=f\"train[:{max_samples_per_dataset}]\")\n","\n","        # the-stack dataset - much smaller sample\n","        print(\"Loading COBOL from The Stack...\")\n","        stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\",\n","                                  split=f\"train[:{max_samples_per_dataset}]\")\n","\n","        print(\"Loading Python datasets...\")\n","        stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\",\n","                                   split=f\"train[:{max_samples_per_dataset}]\")\n","        python_set = load_dataset(\"jtatman/python-code-dataset-500k\",\n","                                 split=f\"train[:{max_samples_per_dataset}]\")\n","\n","        # Clear memory after loading\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Combine Python datasets\n","        python_combined = concatenate_datasets([stack_python, python_set])\n","\n","        # Prepare datasets for training\n","        all_datasets = []\n","\n","        # 1. MainframeBench - COBOL code summarization (convert to instruction format)\n","        mainframe_formatted = self.format_mainframe_dataset(mainframe)\n","        all_datasets.append(mainframe_formatted)\n","\n","        # Clear memory after formatting\n","        del mainframe\n","        gc.collect()\n","\n","        # 2. COBOL understanding dataset (from The Stack)\n","        cobol_formatted = self.format_cobol_understanding_dataset(stack_cobol)\n","        all_datasets.append(cobol_formatted)\n","\n","        # Clear memory after formatting\n","        del stack_cobol\n","        gc.collect()\n","\n","        # 3. Python generation dataset (teach Python syntax)\n","        python_formatted = self.format_python_teaching_dataset(python_combined)\n","        all_datasets.append(python_formatted)\n","\n","        # Clear memory after formatting\n","        del python_combined, stack_python, python_set\n","        gc.collect()\n","\n","        # Combine all datasets\n","        self.combined_dataset = concatenate_datasets(all_datasets)\n","\n","        # Clear intermediate datasets\n","        del all_datasets\n","        gc.collect()\n","\n","        # Filter by length and sample if needed\n","        max_length = self.config.get(\"max_length\", 2048)\n","        self.combined_dataset = self.combined_dataset.filter(\n","            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length\n","        )\n","\n","        if self.config.get(\"max_samples\"):\n","            self.combined_dataset = self.combined_dataset.select(\n","                range(min(len(self.combined_dataset), self.config[\"max_samples\"]))\n","            )\n","\n","        print(f\"Combined dataset prepared: {len(self.combined_dataset)} samples\")\n","\n","        # REPLACE THE OLD TOKENIZATION CODE WITH THIS:\n","        # Apply tokenization with smaller batches and memory management\n","        print(\"Applying tokenizer with padding and truncation...\")\n","        self.combined_dataset = self.combined_dataset.map(\n","            self.tokenize_function,\n","            batched=True,\n","            batch_size=1000,  # Add this to limit batch size\n","            remove_columns=[\"text\"],\n","            num_proc=2,  # Limit parallel processes\n","            desc=\"Tokenizing dataset\"  # Add progress description\n","        )\n","\n","        # Final memory cleanup\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print(f\"Tokenized dataset ready: {len(self.combined_dataset)} samples\")\n","\n","    def setup_lora(self):\n","        \"\"\"Setup LoRA configuration for CodeLlama fine-tuning\"\"\"\n","        lora_config = LoraConfig(\n","            task_type=TaskType.CAUSAL_LM,\n","            inference_mode=False,\n","            r=self.config.get(\"lora_r\", 16),\n","            lora_alpha=self.config.get(\"lora_alpha\", 32),\n","            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n","            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","            bias=\"none\",\n","        )\n","\n","        self.model = get_peft_model(self.model, lora_config)\n","\n","        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","        total_params = sum(p.numel() for p in self.model.parameters())\n","\n","        print(\"üîß LoRA Configuration:\")\n","        print(f\"   - Trainable parameters: {trainable_params:,}\")\n","        print(f\"   - Total parameters: {total_params:,}\")\n","        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n","\n","\n","    def load_your_specific_datasets(self):\n","        \"\"\"Load and prepare your specific datasets for COBOL to Python conversion\"\"\"\n","        print(\"Loading your specific datasets...\")\n","\n","        # Load your datasets you can change the %\n","        print(\"Loading MainframeBench COBOL dataset...\")\n","        mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n","\n","        # the-stack dataset more than 3 TB of data\n","        print(\"Loading COBOL from The Stack...\")\n","        stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:20%]\")\n","\n","        print(\"Loading Python datasets...\")\n","        stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:20%]\")\n","        python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n","\n","        # Combine Python datasets\n","        python_combined = concatenate_datasets([stack_python, python_set])\n","\n","        # Prepare datasets for training\n","        all_datasets = []\n","\n","        # 1. MainframeBench - COBOL code summarization (convert to instruction format)\n","        mainframe_formatted = self.format_mainframe_dataset(mainframe)\n","        all_datasets.append(mainframe_formatted)\n","\n","        # 2. COBOL understanding dataset (from The Stack)\n","        cobol_formatted = self.format_cobol_understanding_dataset(stack_cobol)\n","        all_datasets.append(cobol_formatted)\n","\n","        # 3. Python generation dataset (teach Python syntax)\n","        python_formatted = self.format_python_teaching_dataset(python_combined)\n","        all_datasets.append(python_formatted)\n","\n","        # Combine all datasets\n","        self.combined_dataset = concatenate_datasets(all_datasets)\n","\n","        # Filter by length and sample if needed\n","        max_length = self.config.get(\"max_length\", 2048)\n","        self.combined_dataset = self.combined_dataset.filter(\n","            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length\n","        )\n","\n","        if self.config.get(\"max_samples\"):\n","            self.combined_dataset = self.combined_dataset.select(\n","                range(min(len(self.combined_dataset), self.config[\"max_samples\"]))\n","            )\n","\n","        print(f\"Combined dataset prepared: {len(self.combined_dataset)} samples\")\n","\n","        # Apply tokenization with padding & truncation\n","        print(\"Applying tokenizer with padding and truncation...\")\n","        self.combined_dataset = self.combined_dataset.map(\n","            self.tokenize_function,\n","            batched=True,\n","            remove_columns=[\"text\"]\n","        )\n","\n","\n","    def format_mainframe_dataset(self, dataset):\n","        \"\"\"Format MainframeBench dataset for COBOL understanding\"\"\"\n","        def format_mainframe(examples):\n","            texts = []\n","            for code, summary in zip(examples['source'], examples['summary']):\n","                prompt = f\"\"\"### Task: Analyze and explain the following COBOL code\n","\n","### COBOL Code:\n","```cobol\n","{code.strip()}\n","```\n","\n","### Explanation:\n","{summary.strip()}\n","\n","### Convert to Python equivalent:\n","```python\n","# Python equivalent would be:\n","# This COBOL code performs: {summary.strip()}\n","```\"\"\"\n","                texts.append(prompt)\n","            return {\"text\": texts}\n","\n","        return dataset.map(format_mainframe, batched=True, remove_columns=dataset.column_names)\n","\n","    def format_cobol_understanding_dataset(self, dataset):\n","        \"\"\"Format COBOL dataset from The Stack for understanding\"\"\"\n","        def format_cobol(examples):\n","            texts = []\n","            for content in examples['content']:\n","                # Skip very short or very long files\n","                if len(content.strip()) < 100 or len(content.strip()) > 5000:\n","                    continue\n","\n","                prompt = f\"\"\"### Task: Understand this COBOL code and suggest Python equivalent structure\n","\n","### COBOL Code:\n","```cobol\n","{content.strip()}\n","```\n","\n","### Analysis:\n","This COBOL program demonstrates typical mainframe programming patterns.\n","\n","### Python Structure:\n","```python\n","# Python equivalent structure would involve:\n","# - Converting COBOL divisions to Python modules/classes\n","# - Replacing COBOL data structures with Python equivalents\n","# - Converting COBOL procedures to Python functions\n","```\"\"\"\n","                texts.append(prompt)\n","            return {\"text\": texts}\n","\n","        formatted = dataset.map(format_cobol, batched=True, remove_columns=dataset.column_names)\n","        # Take a subset to avoid overwhelming the model\n","        return formatted.select(range(min(10000, len(formatted))))\n","\n","    def format_python_teaching_dataset(self, dataset):\n","        \"\"\"Format Python dataset to teach Python syntax and patterns\"\"\"\n","        def format_python(examples):\n","            texts = []\n","            for content in examples['content']:\n","                if content is None:\n","                    continue\n","                content = content.strip()\n","\n","                # Skip very short or very long files\n","                if len(content.strip()) < 50 or len(content.strip()) > 3000:\n","                    continue\n","\n","                # Focus on clean, well-structured Python code\n","                if any(keyword in content.lower() for keyword in ['class ', 'def ', 'import ', 'for ', 'if ']):\n","                    prompt = f\"\"\"### Task: Learn Python programming patterns\n","\n","### Python Code:\n","```python\n","{content.strip()}\n","```\n","\n","### Explanation:\n","This Python code demonstrates modern programming practices that can be used when converting from COBOL.\"\"\"\n","                    texts.append(prompt)\n","            return {\"text\": texts}\n","\n","        formatted = dataset.map(format_python, batched=True, remove_columns=dataset.column_names)\n","        # Take a subset focused on quality code\n","        return formatted.select(range(min(15000, len(formatted))))\n","\n","    def setup_training_arguments(self):\n","        return TrainingArguments(\n","            output_dir=self.config.get(\"output_dir\", \"./finetuned\"),\n","            num_train_epochs=self.config.get(\"num_epochs\", 3),\n","            per_device_train_batch_size=self.config.get(\"batch_size\", 2),\n","            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 8),\n","            learning_rate=self.config.get(\"learning_rate\", 1e-4),\n","            lr_scheduler_type=\"cosine\",\n","            warmup_ratio=0.05,\n","            weight_decay=0.01,\n","            bf16=True,\n","            dataloader_pin_memory=True,\n","            dataloader_num_workers=4,\n","            gradient_checkpointing=True,\n","            optim=\"adamw_torch_fused\",\n","            max_grad_norm=1.0,\n","            save_steps=500,\n","            save_total_limit=2,\n","            logging_steps=25,\n","            remove_unused_columns=False,\n","            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n","            run_name=f\"codellama-cobol-python-{self.config.get('experiment_name', 'default')}\",\n","            dataloader_drop_last=True,\n","            ddp_find_unused_parameters=False,\n","        )\n","\n","    from transformers import pipeline\n","\n","    def setup_pipeline(self):\n","        if not hasattr(self, \"model\") or not hasattr(self, \"tokenizer\"):\n","            raise RuntimeError(\"Model and tokenizer must be loaded before setting up pipeline.\")\n","\n","        self.pipe = pipeline(\n","            \"text-generation\",\n","            model=self.model,\n","            tokenizer=self.tokenizer,\n","            device_map=\"auto\"\n","        )\n","\n","    def train(self):\n","        \"\"\"Execute the fine-tuning process\"\"\"\n","        print(\"üöÄ Starting CodeLlama COBOL‚ÜíPython fine-tuning...\")\n","\n","        # Initialize wandb if enabled\n","        if self.config.get(\"use_wandb\", False):\n","            wandb.init(\n","                project=self.config.get(\"wandb_project\", \"codellama-cobol-python\"),\n","                name=f\"codellama-{self.config.get('experiment_name', 'default')}\"\n","            )\n","\n","        # Setup training arguments\n","        training_args = self.setup_training_arguments()\n","\n","        # Data collator for code generation\n","        data_collator = DataCollatorForLanguageModeling(\n","            tokenizer=self.tokenizer,\n","            mlm=False,\n","            pad_to_multiple_of=8,  # Optimize for tensor cores\n","        )\n","\n","        # Initialize SFTTrainer\n","        trainer = SFTTrainer(\n","            model=self.model,\n","            train_dataset=self.combined_dataset,\n","            data_collator=data_collator,\n","            args=training_args,\n","        )\n","\n","        # Clear cache before training\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        # Monitor initial GPU usage\n","        self.monitor_gpu_usage()\n","\n","        # Start training\n","        print(\"üéØ Training started...\")\n","        trainer.train()\n","\n","        # Save the final model\n","        print(\"üíæ Saving model...\")\n","        trainer.save_model()\n","        self.tokenizer.save_pretrained(training_args.output_dir)\n","\n","        print(\"‚úÖ CodeLlama COBOL‚ÜíPython fine-tuning completed!\")\n","\n","    def evaluate_model(self, test_samples=None):\n","        \"\"\"Evaluate the fine-tuned model on COBOL to Python conversion\"\"\"\n","        if test_samples is None:\n","            # Use a small subset for quick evaluation\n","            test_samples = self.combined_dataset.select(range(min(10, len(self.combined_dataset))))\n","\n","        print(\"üîç Evaluating model performance...\")\n","\n","        self.model.eval()\n","        results = []\n","\n","        with torch.no_grad():\n","            for sample in test_samples:\n","                # Extract COBOL code from the sample\n","                text = sample['text']\n","                cobol_start = text.find('```cobol') + 8\n","                cobol_end = text.find('```', cobol_start)\n","                cobol_code = text[cobol_start:cobol_end].strip()\n","\n","                # Generate Python code\n","                prompt = f\"\"\"### Task: Convert the following COBOL code to Python\n","\n","### COBOL Code:\n","```cobol\n","{cobol_code}\n","```\n","\n","### Python Code:\n","```python\"\"\"\n","\n","                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n","                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n","\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_new_tokens=512,\n","                    temperature=0.7,\n","                    do_sample=True,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","                python_code = generated.split('```python')[-1].split('```')[0].strip()\n","\n","                results.append({\n","                    'cobol_code': cobol_code,\n","                    'generated_python': python_code\n","                })\n","\n","        print(f\"‚úÖ Evaluated {len(results)} samples\")\n","        return results\n","\n","    def monitor_gpu_usage(self):\n","        \"\"\"Monitor GPU usage during training\"\"\"\n","        if torch.cuda.is_available():\n","            for i in range(torch.cuda.device_count()):\n","                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n","                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n","                total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n","                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved, {total_memory:.2f}GB total\")\n","\n","    def _extract_true_python_code(self, sample_text):\n","        \"\"\"\n","        Helper to extract the true Python code from the 'text' field of a dataset sample.\n","        This needs to be robust to the different formatting styles you used\n","        across the combined datasets.\n","        \"\"\"\n","        # This is the same helper function as in Option 1\n","        python_code_start_marker_1 = \"### Python Code:\\n```python\"\n","        python_code_start_marker_2 = \"### Python Structure:\\n```python\"\n","        python_code_start_marker_3 = \"### Convert to Python equivalent:\\n```python\"\n","        python_end_marker = \"```\"\n","\n","        start_index = -1\n","        if python_code_start_marker_1 in sample_text:\n","            start_index = sample_text.find(python_code_start_marker_1) + len(python_code_start_marker_1)\n","        elif python_code_start_marker_2 in sample_text:\n","            start_index = sample_text.find(python_code_start_marker_2) + len(python_code_start_marker_2)\n","        elif python_code_start_marker_3 in sample_text:\n","            start_index = sample_text.find(python_code_start_marker_3) + len(python_code_start_marker_3)\n","\n","        if start_index != -1:\n","            end_index = sample_text.find(python_end_marker, start_index)\n","            if end_index != -1:\n","                return sample_text[start_index:end_index].strip()\n","        return \"\"\n","\n","\n","    def calculate_and_print_metrics(self, test_dataset, predictions, num_examples_to_show=3):\n","        \"\"\"\n","        Calculates and prints evaluation metrics for code translation.\n","\n","        Args:\n","            test_dataset (Dataset): The Hugging Face Dataset used for testing.\n","            predictions (list): A list of generated Python code strings (y_pred).\n","        \"\"\"\n","        if not isinstance(test_dataset, Dataset):\n","            raise TypeError(\"test_dataset must be a Hugging Face Dataset object.\")\n","\n","        if len(test_dataset) != len(predictions):\n","            raise ValueError(\"Mismatch in number of samples between test_dataset and predictions.\")\n","\n","        # Extract ground truth (y_true)\n","        y_true = [self._extract_true_python_code(sample[\"text\"]) for sample in test_dataset]\n","\n","        # Filter out samples where true_code could not be extracted\n","        valid_pairs = [(gt, pred) for gt, pred in zip(y_true, predictions) if gt]\n","\n","        if not valid_pairs:\n","            print(\"‚ùó No valid ground truth Python code found in the test dataset for evaluation metrics.\")\n","            return {}\n","\n","        y_true_filtered = [pair[0] for pair in valid_pairs]\n","        y_pred_filtered = [pair[1] for pair in valid_pairs]\n","\n","        print(f\"Evaluating metrics on {len(y_true_filtered)} valid samples.\")\n","\n","        def code_similarity(a, b):\n","            return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n","\n","        similarities = [code_similarity(gt, pred) for gt, pred in zip(y_true_filtered, y_pred_filtered)]\n","        avg_similarity = np.mean(similarities)\n","\n","        exact_matches = sum(1 for gt, pred in zip(y_true_filtered, y_pred_filtered) if gt.strip() == pred.strip())\n","        accuracy = exact_matches / len(y_true_filtered)\n","\n","        print(f\"\\n--- Evaluation Metrics ---\")\n","        print(f\"Exact Match Accuracy: {accuracy:.3f}\")\n","        print(f\"Average Similarity Score: {avg_similarity:.3f}\")\n","\n","        print(f\"\\n--- Sample Outputs ({min(num_examples_to_show, len(y_true_filtered))} examples) ---\")\n","        for i in range(min(num_examples_to_show, len(y_true_filtered))):\n","            print(f\"\\n--- Sample {i+1} ---\")\n","            print(\"True Output:\\n\", y_true_filtered[i])\n","            print(\"Predicted Output:\\n\", y_pred_filtered[i])\n","            print(\"Similarity Score:\", code_similarity(y_true_filtered[i], y_pred_filtered[i]))\n","\n","        return {\n","            \"exact_match_accuracy\": accuracy,\n","            \"average_similarity\": avg_similarity\n","        }"]},{"cell_type":"code","execution_count":null,"id":"5db95e3d-896a-4cb5-87e4-52e1c8750314","metadata":{"id":"5db95e3d-896a-4cb5-87e4-52e1c8750314","executionInfo":{"status":"aborted","timestamp":1750240645167,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["!pip install scikit-learn"]},{"cell_type":"code","execution_count":null,"id":"6cca336a-7a15-497a-9cc0-0ee64f16bbf2","metadata":{"id":"6cca336a-7a15-497a-9cc0-0ee64f16bbf2","executionInfo":{"status":"aborted","timestamp":1750240645168,"user_tz":-330,"elapsed":23,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import gc\n","import json\n","import warnings\n","from tqdm import tqdm\n","from typing import Dict, List, Optional, Union\n","\n","# Core ML libraries\n","import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    EarlyStoppingCallback,\n","    # CodeLlama specific\n","    CodeLlamaTokenizer,\n","    LlamaForCausalLM\n",")\n","\n","# Dataset handling\n","from datasets import Dataset, load_dataset, concatenate_datasets\n","from sklearn.model_selection import train_test_split\n","\n","# Fine-tuning libraries\n","import bitsandbytes as bnb\n","from peft import (\n","    LoraConfig,\n","    PeftConfig,\n","    get_peft_model,\n","    TaskType,\n","    prepare_model_for_kbit_training\n",")\n","from trl import SFTTrainer, setup_chat_format\n","\n","# Evaluation metrics\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix\n",")\n","from evaluate import load as load_metric\n","\n","# Monitoring and optimization\n","import wandb\n","import pynvml\n","from accelerate import Accelerator\n","\n","# Suppress warnings for cleaner output\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","transformers.logging.set_verbosity_error()\n","\n","# Set random seeds for reproducibility\n","def set_random_seeds(seed=42):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","set_random_seeds(42)\n","\n","print(\"üì¶ All libraries imported successfully!\")\n","print(f\"üî• PyTorch version: {torch.__version__}\")\n","print(f\"ü§ó Transformers version: {transformers.__version__}\")\n","print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"üíæ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"]},{"cell_type":"code","execution_count":null,"id":"06b60358-5c6d-4ecd-ad5f-2f435e390318","metadata":{"id":"06b60358-5c6d-4ecd-ad5f-2f435e390318","executionInfo":{"status":"aborted","timestamp":1750240645181,"user_tz":-330,"elapsed":16,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import os\n","\n","os.environ[\"HUGGING_FACE_WRITE_API_KEY\"] = \"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\"\n","os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n"]},{"cell_type":"code","execution_count":null,"id":"75db2a19-16b7-4e71-984e-f788970420fd","metadata":{"id":"75db2a19-16b7-4e71-984e-f788970420fd","executionInfo":{"status":"aborted","timestamp":1750240645182,"user_tz":-330,"elapsed":20134,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import os\n","from huggingface_hub import login\n","from huggingface_hub import CommitInfo\n","\n","# Set environment variables in your shell or .bashrc/.zshrc\n","# export HUGGING_FACE_WRITE_API_KEY=\"hf_your_write_token_here\"\n","# export HUGGINGFACE_TOKEN=\"hf_your_read_token_here\"\n","\n","# In your Python code\n","secret_value_0 = os.getenv(\"HUGGING_FACE_WRITE_API_KEY\")\n","secret_value_1 = os.getenv(\"HUGGINGFACE_TOKEN\")\n","\n","if not secret_value_0 or not secret_value_1:\n","    raise ValueError(\"Please set HUGGING_FACE_WRITE_API_KEY and HUGGINGFACE_TOKEN environment variables\")\n","\n","hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n","\n","# Login to Hugging Face Hub\n","login(token=hf_token)\n"]},{"cell_type":"code","execution_count":null,"id":"39314d5d-636a-4e4c-9637-277ec8622d02","metadata":{"id":"39314d5d-636a-4e4c-9637-277ec8622d02","executionInfo":{"status":"aborted","timestamp":1750240645182,"user_tz":-330,"elapsed":20132,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load model for inference\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16\n",")\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# Create pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16\n",")\n","\n","# Run test inference\n","prompt = \"Convert this COBOL code to Python:\\n\\nIDENTIFICATION DIVISION.\\nPROGRAM-ID. HELLO.\\nPROCEDURE DIVISION.\\nDISPLAY 'HELLO, WORLD'.\\nSTOP RUN.\"\n","outputs = pipe(prompt, max_new_tokens=200, do_sample=False)\n","\n","print(outputs[0][\"generated_text\"])\n"]},{"cell_type":"code","execution_count":null,"id":"7b506054-ce60-4031-a2eb-719706f859c4","metadata":{"id":"7b506054-ce60-4031-a2eb-719706f859c4","executionInfo":{"status":"aborted","timestamp":1750240645216,"user_tz":-330,"elapsed":20164,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from datasets import load_dataset, Dataset, concatenate_datasets\n","\n","# Load relevant subsets\n","mainframe_full = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n","mainframe = mainframe_full.shuffle(seed=42).select(range(2000)) # for kaggle\n","\n","# full dataset (3TB of data)\n","# ds = load_dataset(\"bigcode/the-stack\", split=\"train\")\n","\n","# specific language (e.g. Dockerfiles)\n","stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:10]\")\n","stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:10]\")\n","\n","python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train[:1000]\")\n","\n","# Combine relevant Python corpora\n","python_combined = concatenate_datasets([stack_python, python_set])\n","\n","# Now build translation dataset\n"]},{"cell_type":"code","execution_count":null,"id":"2225bf03-97b8-4d5c-8bd4-a997849bddfa","metadata":{"id":"2225bf03-97b8-4d5c-8bd4-a997849bddfa","executionInfo":{"status":"aborted","timestamp":1750240645217,"user_tz":-330,"elapsed":20164,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["print(mainframe.column_names)"]},{"cell_type":"code","execution_count":null,"id":"f3697f34-80f3-46fc-865a-07d23fe71c79","metadata":{"id":"f3697f34-80f3-46fc-865a-07d23fe71c79","executionInfo":{"status":"aborted","timestamp":1750240645237,"user_tz":-330,"elapsed":20184,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Format MainframeBench COBOL data\n","def format_mainframe(example):\n","    return {\n","        \"input\": example[\"source\"],\n","        \"output\": \"# Python translation to be generated or is unknown for now.\",\n","    }\n","\n","mainframe_formatted = mainframe.map(format_mainframe)\n","\n","# Build real COBOL ‚Üí Python pairs from The Stack\n","paired_data = []\n","for i in range(min(len(stack_cobol), len(python_combined))):\n","    paired_data.append({\n","        \"input\": stack_cobol[i][\"content\"],\n","        \"output\": python_combined[i][\"content\"]\n","    })\n","\n","# Combine both: real pairs + placeholder Mainframe data\n","combined_data = Dataset.from_list(paired_data + list(mainframe_formatted))\n"]},{"cell_type":"markdown","id":"e07dd3c4-99c8-42d2-924d-c0d340d9f030","metadata":{"id":"e07dd3c4-99c8-42d2-924d-c0d340d9f030"},"source":["## Combine dataset\n","| Dataset Name                    | HF ID                                            | Contains                  | Use                                |\n","| ------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------- |\n","| **MainframeBench**              | `Fsoft-AIC/MainframeBench`                       | COBOL code + descriptions | Base COBOL understanding           |\n","| **The Stack**                   | `bigcode/the-stack`                              | COBOL + other languages   | Language variety + COBOL samples   |\n","| **Python Code Dataset**         | `jtatman/python-code-dataset-500k`               | Python code               | Target code corpus                 |\n","| **SantaCoder Fine-tuned COBOL** | `muhtasham/santacoder-finetuned-the-stack-cobol` | Pretrained model          | Base model for COBOL understanding |\n","| **General Code**                | `codeparrot/github-code`                         | Multi-language            | Extra fine-tuning                  |\n"]},{"cell_type":"code","execution_count":null,"id":"4b69f2e6-b5c8-4289-9bc1-adb4bd59ab45","metadata":{"id":"4b69f2e6-b5c8-4289-9bc1-adb4bd59ab45","executionInfo":{"status":"aborted","timestamp":1750240645238,"user_tz":-330,"elapsed":20185,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from datasets import DatasetDict\n","\n","splits = combined_data.train_test_split(test_size=0.2, seed=42)\n","eval_test = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n","\n","dataset_dict = DatasetDict({\n","    \"train\": splits[\"train\"],\n","    \"eval\": eval_test[\"train\"],\n","    \"test\": eval_test[\"test\"]\n","})\n","\n","print(dataset_dict[\"train\"].features)"]},{"cell_type":"code","execution_count":null,"id":"299565f6-3dd7-4cdd-9da4-0b9658eff3aa","metadata":{"id":"299565f6-3dd7-4cdd-9da4-0b9658eff3aa","executionInfo":{"status":"aborted","timestamp":1750240645239,"user_tz":-330,"elapsed":20185,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import bitsandbytes\n","print(bitsandbytes.__file__)"]},{"cell_type":"code","execution_count":null,"id":"9e288b36-9bfe-4087-aff1-6abd04212cbf","metadata":{"id":"9e288b36-9bfe-4087-aff1-6abd04212cbf","executionInfo":{"status":"aborted","timestamp":1750240645239,"user_tz":-330,"elapsed":20185,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["LLAMA_COBOL_FINE_TUNE_CONFIG = {\n","    # === Model ===\n","    \"model_name\": model_name,  # or \"meta-llama/Llama-3.1-8B-Instruct\"\n","    \"load_in_4bit\": True,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_use_double_quant\": True,\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","\n","    # === LoRA PEFT ===\n","    \"lora_r\": 16,\n","    \"lora_alpha\": 32,\n","    \"lora_dropout\": 0.05,\n","    \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","\n","    # === Dataset ===\n","    \"dataset_name\": \"custom\",  # Will use your dataset loading logic\n","    \"dataset_text_field\": \"input\",  # Your format_mainframe() defines \"input\" and \"output\"\n","    \"max_length\": 2048,\n","    \"packing\": False,\n","\n","    # === Training ===\n","    \"num_train_epochs\": 3,\n","    \"per_device_train_batch_size\": 8,       # 8 * 2 GPUs * 4 grad_accum = 64 effective\n","    \"gradient_accumulation_steps\": 4,\n","    \"learning_rate\": 2e-5,\n","    \"lr_scheduler_type\": \"cosine\",\n","    \"warmup_steps\": 100,\n","    \"fp16\": False,\n","    \"bf16\": True,\n","    \"gradient_checkpointing\": True,\n","    \"optim\": \"paged_adamw_8bit\",\n","\n","    # === Evaluation & Saving ===\n","    \"evaluation_strategy\": \"epoch\",\n","    \"save_strategy\": \"epoch\",\n","    \"logging_steps\": 10,\n","    \"save_total_limit\": 2,\n","    \"load_best_model_at_end\": True,\n","\n","    # === Output ===\n","    \"output_dir\": \"./codellama-cobol-finetuned\",\n","\n","    # === Advanced ===\n","    \"use_flash_attention\": True,\n","    \"push_to_hub\": False,\n","    \"hub_model_id\": \"\",\n","\n","    # flash-attn does not yet support CUDA 12.1 officially\n","    \"use_flash_attention\": False\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"45590542-e9f0-4ac8-9174-b0f17e0a858f","metadata":{"id":"45590542-e9f0-4ac8-9174-b0f17e0a858f","executionInfo":{"status":"aborted","timestamp":1750240645240,"user_tz":-330,"elapsed":20186,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import os\n","import glob\n","\n","model_dir = \"./models/codellama\"\n","index_files = glob.glob(os.path.join(model_dir, \"*.index.json\"))\n","\n","# Only keep the first file, delete others\n","if len(index_files) > 1:\n","    print(\"üîç Found multiple .index.json files:\")\n","    for i, f in enumerate(index_files):\n","        print(f\"{i+1}. {f}\")\n","\n","    # Sort and keep the first one, delete the rest\n","    files_to_delete = index_files[1:]\n","    for f in files_to_delete:\n","        print(f\"üóëÔ∏è Deleting: {f}\")\n","        os.remove(f)\n","else:\n","    print(\"‚úÖ Only one .index.json file found. No action needed.\")\n"]},{"cell_type":"code","execution_count":null,"id":"99b29703-6f57-4346-8f0c-03b81c6ee49e","metadata":{"id":"99b29703-6f57-4346-8f0c-03b81c6ee49e","executionInfo":{"status":"aborted","timestamp":1750240645240,"user_tz":-330,"elapsed":20186,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Initialize fine-tuner\n","fine_tuner = OptimizedCodeLlamaFineTuner(LLAMA_COBOL_FINE_TUNE_CONFIG)\n"]},{"cell_type":"code","execution_count":null,"id":"33690a99-ae1f-4caa-94a6-ce8d30b4954c","metadata":{"id":"33690a99-ae1f-4caa-94a6-ce8d30b4954c","executionInfo":{"status":"aborted","timestamp":1750240645241,"user_tz":-330,"elapsed":20187,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# !pip uninstall -y flash-attn\n","# !pip install flash-attn --no-build-isolation\n"]},{"cell_type":"code","execution_count":null,"id":"204843f5-bb5d-4d26-9729-052e8996fbc8","metadata":{"id":"204843f5-bb5d-4d26-9729-052e8996fbc8","executionInfo":{"status":"aborted","timestamp":1750240645241,"user_tz":-330,"elapsed":20187,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Load model and tokenizer\n","fine_tuner.load_model_and_tokenizer()\n","\n","fine_tuner.setup_pipeline()\n","\n","# Setup LoRA\n","fine_tuner.setup_lora()\n","\n","# Load and prepare dataset\n","fine_tuner.load_your_specific_datasets()\n","\n","# Monitor initial GPU usage\n","print(\"üìä Initial GPU Memory Usage:\")\n","fine_tuner.monitor_gpu_usage()\n","print()\n"]},{"cell_type":"code","execution_count":null,"id":"92a73ad4-e195-4e32-ab3f-0a68108fabc5","metadata":{"id":"92a73ad4-e195-4e32-ab3f-0a68108fabc5","executionInfo":{"status":"aborted","timestamp":1750240645242,"user_tz":-330,"elapsed":20188,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Start training\n","fine_tuner.train()\n","\n","# Final GPU usage\n","print(\"üìä Final GPU Memory Usage:\")\n","fine_tuner.monitor_gpu_usage()\n","\n","# Optional: Optimize for inference\n","# fine_tuner.optimize_for_inference(config[\"output_dir\"])"]},{"cell_type":"code","execution_count":null,"id":"0c8f2bb5-2f92-469d-b57a-0eedd124164f","metadata":{"id":"0c8f2bb5-2f92-469d-b57a-0eedd124164f","executionInfo":{"status":"aborted","timestamp":1750240645242,"user_tz":-330,"elapsed":20187,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Save trained model and tokenizer\n","\n","# Get the output directory path directly from the trainer's arguments\n","# This ensures consistency with where the trainer has been saving checkpoints\n","final_model_output_path = trainer.args.output_dir\n","\n","trainer.save_model(final_model_output_path)\n","fine_tuner.tokenizer.save_pretrained(final_model_output_path)\n","\n","print(f\"Model and tokenizer saved to: {final_model_output_path}\")"]},{"cell_type":"code","execution_count":null,"id":"ea397617-f438-4caa-adda-20c5edbdd167","metadata":{"id":"ea397617-f438-4caa-adda-20c5edbdd167","executionInfo":{"status":"aborted","timestamp":1750240645243,"user_tz":-330,"elapsed":20188,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n","# and 'dataset_dict' contains your \"test\" split.\n","\n","print(\"Starting model evaluation...\")\n","\n","# 1. Generate predictions using the evaluate_model method\n","# This method generates predictions and extracts the true labels\n","# The method will return a list of dictionaries, where each dict might contain\n","# 'cobol_code', 'generated_python' (prediction), and potentially 'true_python_code' (ground truth).\n","evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n","\n","# From the evaluation_results, extract just the generated Python code (y_pred)\n","# and the true Python code (y_true) for passing to the metrics calculator.\n","# Note: The 'evaluate_model' method should already be structured to return these.\n","# If 'evaluate_model' returns predictions directly, then you extract y_pred from there.\n","# And y_true is extracted by calculate_and_print_metrics internally using test_dataset.\n","\n","# Example: If evaluation_results is a list of dictionaries with a 'generated_python' key\n","y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n","\n","# 2. Calculate and print metrics using the calculate_and_print_metrics method\n","# This method takes the test_dataset (to re-extract y_true internally) and the predictions.\n","fine_tuner.calculate_and_print_metrics(test_dataset=dataset_dict[\"test\"], predictions=y_pred_for_metrics)\n","\n","print(\"\\nEvaluation complete!\")"]},{"cell_type":"code","execution_count":null,"id":"3d6c5576-0e56-4bff-8b5d-616226122933","metadata":{"id":"3d6c5576-0e56-4bff-8b5d-616226122933","executionInfo":{"status":"aborted","timestamp":1750240645243,"user_tz":-330,"elapsed":20188,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n","# and the training has completed and model saved.\n","\n","# The base model name is stored in your fine_tuner's config\n","base_model_name = fine_tuner.config[\"model_name\"]\n","\n","# The output directory for the fine-tuned model is stored in the trainer's args\n","# which came from your fine_tuner's config.\n","fine_tuned_model_path = trainer.args.output_dir\n","\n","print(f\"Base Model: {base_model_name}\")\n","print(f\"Fine-tuned Model Saved At: {fine_tuned_model_path}\")"]},{"cell_type":"code","execution_count":null,"id":"e2b17279-8705-4477-a78d-729a7f47b125","metadata":{"id":"e2b17279-8705-4477-a78d-729a7f47b125","executionInfo":{"status":"aborted","timestamp":1750240645244,"user_tz":-330,"elapsed":20189,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Use the 'base_model_name' variable defined in the previous step\n","# (which came from fine_tuner.config[\"model_name\"])\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","base_model_reload = AutoModelForCausalLM.from_pretrained(\n","    base_model_name, # Use the defined base_model_name\n","    return_dict=True,\n","    low_cpu_mem_usage=True,\n","    # Use torch.bfloat16 for consistency with your config_dict's bnb_4bit_compute_dtype\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","print(f\"Base model '{base_model_name}' and its tokenizer reloaded successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"cd91e074-e627-4013-9482-8e4b8872b154","metadata":{"id":"cd91e074-e627-4013-9482-8e4b8872b154","executionInfo":{"status":"aborted","timestamp":1750240645244,"user_tz":-330,"elapsed":20189,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from peft import PeftModel\n","\n","# Merge adapter with base model\n","# 'base_model_reload' is the AutoModelForCausalLM you just reloaded.\n","# 'fine_tuned_model_path' is the path where your LoRA adapters were saved by the trainer.\n","model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path)\n","\n","# This merges the LoRA adapter weights into the base model weights\n","# and removes the adapter layers, leaving you with a single, merged model.\n","model = model.merge_and_unload()\n","\n","print(f\"LoRA adapters successfully merged into the base model. The merged model is ready.\")"]},{"cell_type":"code","execution_count":null,"id":"6ffc4f55-7be9-46ab-9a04-4ce54f262dab","metadata":{"id":"6ffc4f55-7be9-46ab-9a04-4ce54f262dab","executionInfo":{"status":"aborted","timestamp":1750240645245,"user_tz":-330,"elapsed":20190,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import pipeline # Ensure pipeline is imported\n","\n","cobol_code = \"\"\"\n","        IDENTIFICATION DIVISION.\n","        PROGRAM-ID. HELLO.\n","        PROCEDURE DIVISION.\n","            DISPLAY 'HELLO, WORLD'.\n","            STOP RUN.\n","\"\"\"\n","\n","prompt = f\"\"\"\n","### Instruction:\n","Convert the following COBOL code to Python:\n","\n","{cobol_code}\n","\n","### Response:\n","\"\"\".strip()\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,        # This is your merged model\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency with training and loading\n","    device_map=\"auto\"\n",")\n","\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1)\n","generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n","\n","print(\"Generated Python Code:\\n\")\n","print(generated_code)"]},{"cell_type":"code","execution_count":null,"id":"cd6deb8e-7beb-4425-8585-f4249c913ee4","metadata":{"id":"cd6deb8e-7beb-4425-8585-f4249c913ee4","executionInfo":{"status":"aborted","timestamp":1750240645245,"user_tz":-330,"elapsed":20190,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Define the directory where you want to save your final merged model\n","model_dir = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n","\n","# Save the merged model (which now contains the LoRA weights)\n","model.save_pretrained(model_dir)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(model_dir)\n","\n","print(f\"Your final merged model and tokenizer have been saved to: {model_dir}\")"]},{"cell_type":"code","execution_count":null,"id":"da409f8a-9b31-4845-8904-3433715cce49","metadata":{"id":"da409f8a-9b31-4845-8904-3433715cce49","executionInfo":{"status":"aborted","timestamp":1750240645246,"user_tz":-330,"elapsed":20191,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from peft import PeftModel\n","import torch\n","\n","# Assuming base_model_name and fine_tuned_model_path are defined from previous steps:\n","# For example:\n","# base_model_name = fine_tuner.config[\"model_name\"]\n","# fine_tuned_model_path = trainer.args.output_dir # or your chosen output directory for adapters\n","\n","print(f\"Loading tokenizer from: {base_model_name}\")\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","print(f\"Loading base model from: {base_model_name} with torch_dtype=torch.bfloat16\")\n","base_model_reload = AutoModelForCausalLM.from_pretrained(\n","    base_model_name, # Use the correct variable name for the base model identifier\n","    return_dict=True,\n","    low_cpu_mem_usage=True,\n","    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","print(f\"Loading fine-tuned adapters from: {fine_tuned_model_path}\")\n","model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path) # Use the correct path for adapters\n","\n","print(\"Merging adapters into the base model...\")\n","model = model.merge_and_unload()\n","\n","print(\"Model and adapters merged successfully!\")"]},{"cell_type":"code","execution_count":null,"id":"6cfe90ca-16c0-4d12-9b6c-26b0bddc3e66","metadata":{"id":"6cfe90ca-16c0-4d12-9b6c-26b0bddc3e66","executionInfo":{"status":"aborted","timestamp":1750240645246,"user_tz":-330,"elapsed":20190,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"id":"0fce598f-cc66-496c-9c48-4c9ba9cab666","metadata":{"id":"0fce598f-cc66-496c-9c48-4c9ba9cab666","executionInfo":{"status":"aborted","timestamp":1750240645247,"user_tz":-330,"elapsed":20191,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["# Save and register to Hugging Face Hub\n","# Ensure you are logged in to Hugging Face locally (huggingface-cli login)\n","\n","# Push the merged model to your Hugging Face repository\n","model.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n","\n","# Push the tokenizer to the same repository\n","tokenizer.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n","\n","print(\"\\nModel and tokenizer successfully pushed to Hugging Face Hub!\")\n","print(\"You can find them at: https://huggingface.co/dhirajpatra/codellama-cobol-python\")"]},{"cell_type":"code","execution_count":null,"id":"41378cf5-9db0-483e-a61c-49420fbca384","metadata":{"id":"41378cf5-9db0-483e-a61c-49420fbca384","executionInfo":{"status":"aborted","timestamp":1750240645247,"user_tz":-330,"elapsed":20191,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Assuming 'model_dir' is the string \"codellama-cobol-python\" you used when saving\n","# Or, even better, just use the full repo ID directly for clarity:\n","hf_repo_id = \"dhirajpatra/codellama-cobol-python\"\n","\n","model = AutoModelForCausalLM.from_pretrained(hf_repo_id)\n","tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n","\n","print(f\"Model and tokenizer loaded successfully from Hugging Face Hub: {hf_repo_id}\")"]},{"cell_type":"code","execution_count":null,"id":"810e2e2d-e07b-4e49-b1f1-725eb9326805","metadata":{"id":"810e2e2d-e07b-4e49-b1f1-725eb9326805","executionInfo":{"status":"aborted","timestamp":1750240645248,"user_tz":-330,"elapsed":20192,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Use the full repository ID directly for clarity and robustness\n","# Assuming model_dir was \"codellama-cobol-python\" from your save step\n","model_id = \"dhirajpatra/codellama-cobol-python\"\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","\n","print(f\"**Model and tokenizer loaded successfully from Hugging Face Hub:** `{model_id}`\")"]},{"cell_type":"code","execution_count":null,"id":"8f3c5cfe-9580-46ad-a0db-aed9ca12501f","metadata":{"id":"8f3c5cfe-9580-46ad-a0db-aed9ca12501f","executionInfo":{"status":"aborted","timestamp":1750240645248,"user_tz":-330,"elapsed":20192,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from transformers import pipeline # Ensure pipeline is imported\n","import torch # Ensure torch is imported if not already\n","\n","cobol_code = \"\"\"\n","        IDENTIFICATION DIVISION.\n","        PROGRAM-ID. HELLO.\n","        PROCEDURE DIVISION.\n","            DISPLAY 'HELLO, WORLD'.\n","            STOP RUN.\n","\"\"\"\n","\n","prompt = f\"\"\"### Instruction:\n","Convert the following COBOL code to Python:\n","\n","{cobol_code}\n","\n","### Response:\n","\"\"\".strip()\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16, # Changed to bfloat16 for consistency and performance\n","    device_map=\"auto\",\n",")\n","\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n","generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n","\n","print(\"Generated Python Code:\\n\")\n","print(generated_code)"]},{"cell_type":"code","execution_count":null,"id":"0ce3d25e-2e44-40b6-acb1-92656280dd4b","metadata":{"id":"0ce3d25e-2e44-40b6-acb1-92656280dd4b","executionInfo":{"status":"aborted","timestamp":1750240645249,"user_tz":-330,"elapsed":20193,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":["from datasets import load_metric, DatasetDict\n","# Assuming fine_tuner is your OptimizedCodeLlamaFineTuner instance\n","# and dataset_dict contains your \"test\" split.\n","# And assuming you've already run the evaluation_results step:\n","# evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n","# y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n","\n","# 1. Load the BLEU metric\n","bleu = load_metric(\"bleu\")\n","\n","# 2. Prepare predictions (preds) - This is your generated code\n","# Assuming y_pred_for_metrics holds the list of generated Python code strings\n","preds = y_pred_for_metrics\n","\n","# 3. Prepare references (refs) - This requires extracting the true Python code\n","# The calculate_and_print_metrics method has a helper for this.\n","# We need to call that helper or re-extract here.\n","# Let's re-extract the filtered ground truth for consistency with metrics.\n","y_true = [fine_tuner._extract_true_python_code(sample[\"text\"]) for sample in dataset_dict[\"test\"]]\n","valid_pairs = [(gt, pred) for gt, pred in zip(y_true, preds) if gt]\n","\n","# preds should be from the filtered valid_pairs as well for direct comparison\n","# If you ran fine_tuner.calculate_and_print_metrics, it already filtered.\n","# For BLEU, ensure preds and refs correspond to the same filtered set.\n","preds_filtered_for_bleu = [pair[1] for pair in valid_pairs]\n","refs_filtered_for_bleu = [[pair[0]] for pair in valid_pairs] # BLEU expects a list of lists for references\n","\n","if not preds_filtered_for_bleu:\n","    print(\"‚ùó No valid samples to compute BLEU score on.\")\n","else:\n","    # 4. Compute the BLEU score\n","    results = bleu.compute(predictions=preds_filtered_for_bleu, references=refs_filtered_for_bleu)\n","\n","    print(\"\\n--- BLEU Score ---\")\n","    print(f\"BLEU score: {results['bleu']:.4f}\")\n","    # You can also print other details if available in results, e.g., 'precisions'\n","    # print(results)"]},{"cell_type":"code","execution_count":null,"id":"82780390-3926-443f-af7f-41ca491c4f35","metadata":{"id":"82780390-3926-443f-af7f-41ca491c4f35","executionInfo":{"status":"aborted","timestamp":1750240645249,"user_tz":-330,"elapsed":20193,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"09c69e25-4189-40b6-ba4e-04c41d393330","metadata":{"id":"09c69e25-4189-40b6-ba4e-04c41d393330","executionInfo":{"status":"aborted","timestamp":1750240645250,"user_tz":-330,"elapsed":20193,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9f1e913f-5deb-4ecc-90db-f3eeac0c20fe","metadata":{"id":"9f1e913f-5deb-4ecc-90db-f3eeac0c20fe","executionInfo":{"status":"aborted","timestamp":1750240645258,"user_tz":-330,"elapsed":20201,"user":{"displayName":"Dhiraj Patra","userId":"10091544074869373592"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe52be3d-c6bf-4422-bb71-1c90dc8225e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check CUDA version first\n",
    "# !nvcc --version\n",
    "\n",
    "# # For CUDA 12.1+ (more likely on your server)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# # Core libraries\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install peft\n",
    "# !pip install trl\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate\n",
    "\n",
    "# # Flash attention (improved installation)\n",
    "# !pip install packaging wheel\n",
    "# !pip install flash-attn --no-build-isolation\n",
    "\n",
    "# # Tokenization and data handling\n",
    "# !pip install sentencepiece\n",
    "# !pip install protobuf\n",
    "\n",
    "# # Evaluation metrics\n",
    "# !pip install evaluate\n",
    "# !pip install rouge-score\n",
    "# !pip install sacrebleu\n",
    "\n",
    "# # Monitoring options\n",
    "# !pip install wandb\n",
    "# !pip install tensorboard\n",
    "# !pip install pynvml\n",
    "# !pip install psutil\n",
    "\n",
    "# # TensorRT optimization\n",
    "# !pip install tensorrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d7827-4123-45fd-9b94-5ac17c8c28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version first\n",
    "nvcc --version\n",
    "\n",
    "# For CUDA 12.1+ (more likely on your server)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core libraries\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "\n",
    "# Try flash-attn with different approaches\n",
    "echo \"Attempting Flash Attention installation...\"\n",
    "\n",
    "# Method 1: Try pre-compiled wheel first\n",
    "!pip install flash-attn --no-build-isolation --no-deps || echo \"Method 1 failed\"\n",
    "\n",
    "# Method 2: If that fails, try with specific CUDA environment\n",
    "if ! python -c \"import flash_attn\" 2>/dev/null; then\n",
    "    echo \"Trying with CUDA environment variables...\"\n",
    "    export CUDA_HOME=/usr/local/cuda\n",
    "    export PATH=$CUDA_HOME/bin:$PATH\n",
    "    export CUDA_VISIBLE_DEVICES=0\n",
    "    !pip install flash-attn --no-build-isolation --no-cache-dir || echo \"Method 2 failed\"\n",
    "fi\n",
    "\n",
    "# Method 3: If still fails, install alternative efficient attention\n",
    "if ! python -c \"import flash_attn\" 2>/dev/null; then\n",
    "    echo \"Flash Attention failed, installing xformers as alternative...\"\n",
    "    !pip install xformers || echo \"xformers installation failed\"\n",
    "fi\n",
    "\n",
    "# Method 4: If all fail, we'll use native attention (slower but works)\n",
    "if ! python -c \"import flash_attn\" 2>/dev/null && ! python -c \"import xformers\" 2>/dev/null; then\n",
    "    echo \"Using native PyTorch attention (slower but compatible)\"\n",
    "fi\n",
    "\n",
    "# Tokenization and data handling\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "\n",
    "# Evaluation metrics\n",
    "!pip install evaluate\n",
    "!pip install rouge-score\n",
    "!pip install sacrebleu\n",
    "\n",
    "# Monitoring options\n",
    "!pip install wandb\n",
    "!pip install tensorboard\n",
    "!pip install pynvml\n",
    "!pip install psutil\n",
    "\n",
    "# Skip TensorRT if not needed (can cause issues)\n",
    "# pip install tensorrt\n",
    "\n",
    "echo \"Installation complete!\"\n",
    "python -c \"\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU count: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import flash_attn\n",
    "    print('‚úÖ Flash Attention available')\n",
    "except ImportError:\n",
    "    print('‚ùå Flash Attention not available')\n",
    "    \n",
    "try:\n",
    "    import xformers\n",
    "    print('‚úÖ xformers available')\n",
    "except ImportError:\n",
    "    print('‚ùå xformers not available')\n",
    "    \n",
    "print('‚úÖ Using native PyTorch attention (always available)')\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e718c64-a0cb-47c8-8c41-676bbf532471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  GPU : NVIDIA GeForce RTX 4090 D\n",
      "üöÄ CUDA Version : 12.1\n",
      "üß† Total Memory : 23.43 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"codellama_cobol_finetune.ipynb\"\n",
    "\n",
    "# ‚úÖ Ensure CUDA is available\n",
    "assert torch.cuda.is_available(), \"CUDA GPU not available.\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# ‚úÖ Display GPU information\n",
    "print(f\"üñ•Ô∏è  GPU : {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"üöÄ CUDA Version : {torch.version.cuda}\")\n",
    "print(f\"üß† Total Memory : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Environment configs\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoid tokenizer deadlocks in multi-threading\n",
    "\n",
    "# ‚úÖ GPU optimization for RTX 4090 (TensorFloat-32 support)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ‚úÖ Clear GPU memory cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# (Optional) limit memory usage, uncomment if needed\n",
    "# torch.cuda.set_per_process_memory_fraction(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa8e1620-afef-4ded-8dc9-be22e2c979c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Fine-tuning Pipeline for CodeLlama COBOL to Python Conversion\n",
    "Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"codellama_cobol_finetune.ipynb\"\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    CodeLlamaTokenizer,  # Specific for CodeLlama\n",
    "    LlamaForCausalLM     # Specific for CodeLlama\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Monitoring and optimization\n",
    "import wandb\n",
    "import tensorrt as trt\n",
    "import pynvml\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1071f3-eec8-449c-9132-6addd972a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: codellama/CodeLlama-7B-Instruct-hf\n",
      "Output will be saved as: CodeLlama-7B-Instruct-COBOL-to-Python\n",
      "Max sequence length: 2048\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA optimizations for RTX 4090\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Async kernel launches for speed\n",
    "\n",
    "# RTX 4090 specific optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Ada Lovelace TF32 support\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"codellama/CodeLlama-7B-Instruct-hf\"\n",
    "output_model = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "\n",
    "# Training configuration\n",
    "max_seq_length = 2048  # Adjust based on your COBOL/Python code lengths\n",
    "batch_size = 4         # Start with 4, can increase with 24GB VRAM\n",
    "gradient_accumulation_steps = 4  # Effective batch size = 16\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(f\"Output will be saved as: {output_model}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5724fd2-a2d6-46b3-b6a5-45e947868884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:01<00:00, 15.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(repo_id=model_name, local_dir=\"./models/codellama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "246b69f7-0d58-431b-bf2f-11ee5b4d2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Core packages for fine-tuning\n",
    "%pip install -U -q transformers datasets peft accelerate huggingface_hub trl\n",
    "\n",
    "# Quantization and optimization\n",
    "%pip install -U -q bitsandbytes optimum auto-gptq\n",
    "\n",
    "# Memory and attention optimization\n",
    "%pip install -U -q xformers --no-deps\n",
    "\n",
    "# Flash attention for better performance (if not already installed)\n",
    "%pip install -U -q flash-attn --no-build-isolation\n",
    "\n",
    "# Evaluation metrics for code conversion\n",
    "%pip install -U -q evaluate rouge-score sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a953deb9-5254-4ca3-b6a4-a0ffd4f0657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCodeLlamaFineTuner:\n",
    "    \"\"\"\n",
    "    A comprehensive class designed for efficient fine-tuning of CodeLlama models,\n",
    "    specifically optimized for tasks like COBOL to Python code translation.\n",
    "\n",
    "    This class leverages cutting-edge techniques such as Parameter-Efficient Fine-Tuning\n",
    "    (PEFT) using LoRA (Low-Rank Adaptation) and 4-bit quantization with bitsandbytes\n",
    "    to enable training large language models on consumer-grade GPUs or environments\n",
    "    with limited memory.\n",
    "\n",
    "    Key Features:\n",
    "    -   **Memory-Efficient Model Loading**: Supports loading CodeLlama models in 4-bit\n",
    "        quantization (`bnb.nn.Linear4bit`) with configurable double quantization\n",
    "        and compute data types (e.g., bfloat16 for RTX 40 series GPUs).\n",
    "    -   **LoRA Integration**: Seamlessly configures and applies LoRA adapters to\n",
    "        target specific linear layers (e.g., query, key, value projections) for\n",
    "        efficient fine-tuning without modifying the full model weights.\n",
    "    -   **Automated Target Module Detection**: Can automatically identify all\n",
    "        4-bit linear layers for LoRA application if not explicitly specified.\n",
    "    -   **Dataset Handling**: Facilitates loading and preprocessing of custom datasets,\n",
    "        supporting instruction-tuning formats (e.g., \"### Instruction:\\n...### Response:\\n\").\n",
    "    -   **Optimized Training Arguments**: Sets up `transformers.TrainingArguments`\n",
    "        with best practices for memory efficiency (gradient accumulation, gradient\n",
    "        checkpointing, paged optimizers, bfloat16/fp16 precision, mixed-precision training).\n",
    "    -   **Supervised Fine-Tuning (SFT)**: Utilizes `trl.SFTTrainer` for streamlined\n",
    "        supervised fine-tuning, handling data collators, tokenization, and training loops.\n",
    "    -   **Flexible Evaluation**: Provides methods for generating predictions from the\n",
    "        fine-tuned model and calculating key evaluation metrics like Exact Match Accuracy\n",
    "        and Average Similarity Score (using SequenceMatcher).\n",
    "    -   **Model Management**: Supports saving LoRA adapters, merging them with the\n",
    "        base model for a standalone full model, and pushing the final model/tokenizer\n",
    "        to the Hugging Face Hub for easy sharing and deployment.\n",
    "    -   **CUDA Memory Optimization**: Includes environment variable settings to help\n",
    "        avoid CUDA memory fragmentation, improving stability during training.\n",
    "\n",
    "    Usage Workflow:\n",
    "    1.  **Initialization**: Instantiate `OptimizedCodeLlamaFineTuner` with a comprehensive\n",
    "        configuration dictionary (`config_dict`) specifying model, LoRA, and training parameters.\n",
    "    2.  **Load Model & Tokenizer**: Call `load_model_and_tokenizer()` to prepare the base model.\n",
    "    3.  **Setup LoRA**: Invoke `setup_lora()` to apply PEFT adapters to the model.\n",
    "    4.  **Load Datasets**: Use `load_your_specific_datasets()` to prepare your training and\n",
    "        evaluation data.\n",
    "    5.  **Create Trainer**: Call `create_and_run_trainer()` to get an initialized\n",
    "        `SFTTrainer` instance.\n",
    "    6.  **Train**: Execute `trainer.train()` to start the fine-tuning process.\n",
    "    7.  **Evaluate**: Use `evaluate_model()` to generate predictions and\n",
    "        `calculate_and_print_metrics()` to assess performance.\n",
    "    8.  **Merge & Save**: Merge the LoRA adapters into the base model using `PeftModel.from_pretrained`\n",
    "        and `merge_and_unload()`, then save the final model with `save_pretrained()`\n",
    "        or push to the Hugging Face Hub with `push_to_hub()`.\n",
    "\n",
    "    Parameters:\n",
    "        config (dict): A dictionary containing all necessary configuration parameters\n",
    "                       for model loading, LoRA setup, and training. Example parameters\n",
    "                       include `model_name`, `load_in_4bit`, `lora_r`, `lora_alpha`,\n",
    "                       `num_train_epochs`, `per_device_train_batch_size`, `output_dir`, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_nvidia_environment()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.datasets = []  # Support for multiple datasets\n",
    "        self.combined_dataset = None\n",
    "        \n",
    "    def setup_nvidia_environment(self):\n",
    "        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available!\")\n",
    "            \n",
    "        # Initialize NVML for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        \n",
    "        print(f\"NVIDIA Setup:\")\n",
    "        print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   - Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle)\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n",
    "            \n",
    "        # Enable TensorFloat-32 for RTX 4090 (Ada Lovelace)\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"   - TensorFloat-32 enabled for RTX 4090\")\n",
    "\n",
    "    # Apply tokenization correctly before training\n",
    "    # REPLACE your existing tokenize_function with this:\n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize examples with memory management\"\"\"\n",
    "        result = self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.config.get(\"max_length\", 2048),\n",
    "            return_tensors=None  # return plain Python objects\n",
    "        )\n",
    "        \n",
    "        # Force garbage collection for large batches to prevent memory buildup\n",
    "        if len(examples[\"text\"]) > 500:\n",
    "            gc.collect()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Add this at the top of your class or main script\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Check for flash attention availability\n",
    "    def check_attention_backend():\n",
    "        try:\n",
    "            import flash_attn\n",
    "            print(\"‚úÖ Using Flash Attention\")\n",
    "            return \"flash_attn\"\n",
    "        except ImportError:\n",
    "            try:\n",
    "                import xformers\n",
    "                print(\"‚úÖ Using xformers attention\")\n",
    "                return \"xformers\"\n",
    "            except ImportError:\n",
    "                print(\"‚ÑπÔ∏è Using native PyTorch attention (slower but stable)\")\n",
    "                return \"native\"\n",
    "    \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load CodeLlama model and prepare for LoRA fine-tuning\"\"\"\n",
    "\n",
    "        # Check attention backend\n",
    "        # CORRECTED LINE: Call the method using self.\n",
    "        attention_backend = self.check_attention_backend()\n",
    "\n",
    "        model_name = self.config.get(\"model_name\", \"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "\n",
    "        print(\"üî§ Loading CodeLlama tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True,\n",
    "            add_bos_token=True\n",
    "        )\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(\"üß† Loading CodeLlama model...\")\n",
    "\n",
    "        # Model loading configuration based on available attention\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "\n",
    "        # Add flash attention config if available\n",
    "        if attention_backend == \"flash_attn\":\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "        elif attention_backend == \"xformers\": # Added xformers check as per your function\n",
    "            model_kwargs[\"attn_implementation\"] = \"xformers\"\n",
    "        elif attention_backend == \"native\":\n",
    "            # Use native attention with optimizations\n",
    "            model_kwargs[\"torch_compile\"] = False  # Disable compile for stability\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Clear cache before LoRA\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"‚úÖ Model loaded successfully with {attention_backend} attention\")\n",
    "        # Removed self.setup_lora() from here, as it's called separately in your main execution flow\n",
    "    \n",
    "    def load_your_specific_datasets(self):\n",
    "        \"\"\"Load and prepare your specific datasets for COBOL to Python conversion\"\"\"\n",
    "        print(\"Loading your specific datasets...\")\n",
    "        \n",
    "        # Add size limits from the start\n",
    "        max_samples_per_dataset = self.config.get(\"max_samples_per_dataset\", 5000)\n",
    "        \n",
    "        # Load your datasets with limits\n",
    "        print(\"Loading MainframeBench COBOL dataset...\")\n",
    "        mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", \n",
    "                               split=f\"train[:{max_samples_per_dataset}]\")\n",
    "    \n",
    "        # the-stack dataset - much smaller sample\n",
    "        print(\"Loading COBOL from The Stack...\")\n",
    "        stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", \n",
    "                                  split=f\"train[:{max_samples_per_dataset}]\")\n",
    "        \n",
    "        print(\"Loading Python datasets...\")\n",
    "        stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", \n",
    "                                   split=f\"train[:{max_samples_per_dataset}]\")\n",
    "        python_set = load_dataset(\"jtatman/python-code-dataset-500k\", \n",
    "                                 split=f\"train[:{max_samples_per_dataset}]\")\n",
    "        \n",
    "        # Clear memory after loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine Python datasets\n",
    "        python_combined = concatenate_datasets([stack_python, python_set])\n",
    "        \n",
    "        # Prepare datasets for training\n",
    "        all_datasets = []\n",
    "        \n",
    "        # 1. MainframeBench - COBOL code summarization (convert to instruction format)\n",
    "        mainframe_formatted = self.format_mainframe_dataset(mainframe)\n",
    "        all_datasets.append(mainframe_formatted)\n",
    "        \n",
    "        # Clear memory after formatting\n",
    "        del mainframe\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. COBOL understanding dataset (from The Stack)\n",
    "        cobol_formatted = self.format_cobol_understanding_dataset(stack_cobol)\n",
    "        all_datasets.append(cobol_formatted)\n",
    "        \n",
    "        # Clear memory after formatting\n",
    "        del stack_cobol\n",
    "        gc.collect()\n",
    "        \n",
    "        # 3. Python generation dataset (teach Python syntax)\n",
    "        python_formatted = self.format_python_teaching_dataset(python_combined)\n",
    "        all_datasets.append(python_formatted)\n",
    "        \n",
    "        # Clear memory after formatting\n",
    "        del python_combined, stack_python, python_set\n",
    "        gc.collect()\n",
    "        \n",
    "        # Combine all datasets\n",
    "        self.combined_dataset = concatenate_datasets(all_datasets)\n",
    "        \n",
    "        # Clear intermediate datasets\n",
    "        del all_datasets\n",
    "        gc.collect()\n",
    "        \n",
    "        # Filter by length and sample if needed\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        self.combined_dataset = self.combined_dataset.filter(\n",
    "            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"max_samples\"):\n",
    "            self.combined_dataset = self.combined_dataset.select(\n",
    "                range(min(len(self.combined_dataset), self.config[\"max_samples\"]))\n",
    "            )\n",
    "            \n",
    "        print(f\"Combined dataset prepared: {len(self.combined_dataset)} samples\")\n",
    "    \n",
    "        # REPLACE THE OLD TOKENIZATION CODE WITH THIS:\n",
    "        # Apply tokenization with smaller batches and memory management\n",
    "        print(\"Applying tokenizer with padding and truncation...\")\n",
    "        self.combined_dataset = self.combined_dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,  # Add this to limit batch size\n",
    "            remove_columns=[\"text\"],\n",
    "            num_proc=2,  # Limit parallel processes\n",
    "            desc=\"Tokenizing dataset\"  # Add progress description\n",
    "        )\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Tokenized dataset ready: {len(self.combined_dataset)} samples\")\n",
    "\n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA configuration for CodeLlama fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get(\"lora_r\", 16),\n",
    "            lora_alpha=self.config.get(\"lora_alpha\", 32),\n",
    "            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "    \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "    \n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "    \n",
    "        print(\"üîß LoRA Configuration:\")\n",
    "        print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   - Total parameters: {total_params:,}\")\n",
    "        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "        \n",
    "    def load_your_specific_datasets(self):\n",
    "        \"\"\"Load and prepare your specific datasets for COBOL to Python conversion\"\"\"\n",
    "        print(\"Loading your specific datasets...\")\n",
    "        \n",
    "        # Load your datasets you can change the %\n",
    "        print(\"Loading MainframeBench COBOL dataset...\")\n",
    "        mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n",
    "\n",
    "        # the-stack dataset more than 3 TB of data\n",
    "        print(\"Loading COBOL from The Stack...\")\n",
    "        stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:20%]\")\n",
    "        \n",
    "        print(\"Loading Python datasets...\")\n",
    "        stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:20%]\")\n",
    "        python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "        \n",
    "        # Combine Python datasets\n",
    "        python_combined = concatenate_datasets([stack_python, python_set])\n",
    "        \n",
    "        # Prepare datasets for training\n",
    "        all_datasets = []\n",
    "        \n",
    "        # 1. MainframeBench - COBOL code summarization (convert to instruction format)\n",
    "        mainframe_formatted = self.format_mainframe_dataset(mainframe)\n",
    "        all_datasets.append(mainframe_formatted)\n",
    "        \n",
    "        # 2. COBOL understanding dataset (from The Stack)\n",
    "        cobol_formatted = self.format_cobol_understanding_dataset(stack_cobol)\n",
    "        all_datasets.append(cobol_formatted)\n",
    "        \n",
    "        # 3. Python generation dataset (teach Python syntax)\n",
    "        python_formatted = self.format_python_teaching_dataset(python_combined)\n",
    "        all_datasets.append(python_formatted)\n",
    "        \n",
    "        # Combine all datasets\n",
    "        self.combined_dataset = concatenate_datasets(all_datasets)\n",
    "        \n",
    "        # Filter by length and sample if needed\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        self.combined_dataset = self.combined_dataset.filter(\n",
    "            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"max_samples\"):\n",
    "            self.combined_dataset = self.combined_dataset.select(\n",
    "                range(min(len(self.combined_dataset), self.config[\"max_samples\"]))\n",
    "            )\n",
    "            \n",
    "        print(f\"Combined dataset prepared: {len(self.combined_dataset)} samples\")\n",
    "\n",
    "        # Apply tokenization with padding & truncation\n",
    "        print(\"Applying tokenizer with padding and truncation...\")\n",
    "        self.combined_dataset = self.combined_dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "        \n",
    "    def format_mainframe_dataset(self, dataset):\n",
    "        \"\"\"Format MainframeBench dataset for COBOL understanding\"\"\"\n",
    "        def format_mainframe(examples):\n",
    "            texts = []\n",
    "            for code, summary in zip(examples['source'], examples['summary']):\n",
    "                prompt = f\"\"\"### Task: Analyze and explain the following COBOL code\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{code.strip()}\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "{summary.strip()}\n",
    "\n",
    "### Convert to Python equivalent:\n",
    "```python\n",
    "# Python equivalent would be:\n",
    "# This COBOL code performs: {summary.strip()}\n",
    "```\"\"\"\n",
    "                texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        return dataset.map(format_mainframe, batched=True, remove_columns=dataset.column_names)\n",
    "    \n",
    "    def format_cobol_understanding_dataset(self, dataset):\n",
    "        \"\"\"Format COBOL dataset from The Stack for understanding\"\"\"\n",
    "        def format_cobol(examples):\n",
    "            texts = []\n",
    "            for content in examples['content']:\n",
    "                # Skip very short or very long files\n",
    "                if len(content.strip()) < 100 or len(content.strip()) > 5000:\n",
    "                    continue\n",
    "                \n",
    "                prompt = f\"\"\"### Task: Understand this COBOL code and suggest Python equivalent structure\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{content.strip()}\n",
    "```\n",
    "\n",
    "### Analysis:\n",
    "This COBOL program demonstrates typical mainframe programming patterns. \n",
    "\n",
    "### Python Structure:\n",
    "```python\n",
    "# Python equivalent structure would involve:\n",
    "# - Converting COBOL divisions to Python modules/classes\n",
    "# - Replacing COBOL data structures with Python equivalents\n",
    "# - Converting COBOL procedures to Python functions\n",
    "```\"\"\"\n",
    "                texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        formatted = dataset.map(format_cobol, batched=True, remove_columns=dataset.column_names)\n",
    "        # Take a subset to avoid overwhelming the model\n",
    "        return formatted.select(range(min(10000, len(formatted))))\n",
    "    \n",
    "    def format_python_teaching_dataset(self, dataset):\n",
    "        \"\"\"Format Python dataset to teach Python syntax and patterns\"\"\"\n",
    "        def format_python(examples):\n",
    "            texts = []\n",
    "            for content in examples['content']:\n",
    "                if content is None:\n",
    "                    continue\n",
    "                content = content.strip()\n",
    "                \n",
    "                # Skip very short or very long files\n",
    "                if len(content.strip()) < 50 or len(content.strip()) > 3000:\n",
    "                    continue\n",
    "                \n",
    "                # Focus on clean, well-structured Python code\n",
    "                if any(keyword in content.lower() for keyword in ['class ', 'def ', 'import ', 'for ', 'if ']):\n",
    "                    prompt = f\"\"\"### Task: Learn Python programming patterns\n",
    "\n",
    "### Python Code:\n",
    "```python\n",
    "{content.strip()}\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "This Python code demonstrates modern programming practices that can be used when converting from COBOL.\"\"\"\n",
    "                    texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        formatted = dataset.map(format_python, batched=True, remove_columns=dataset.column_names)\n",
    "        # Take a subset focused on quality code\n",
    "        return formatted.select(range(min(15000, len(formatted))))\n",
    "\n",
    "    def setup_training_arguments(self):\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.get(\"output_dir\", \"./finetuned\"),\n",
    "            num_train_epochs=self.config.get(\"num_epochs\", 3),\n",
    "            per_device_train_batch_size=self.config.get(\"batch_size\", 2),\n",
    "            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 8),\n",
    "            learning_rate=self.config.get(\"learning_rate\", 1e-4),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.05,\n",
    "            weight_decay=0.01,\n",
    "            bf16=True,\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=4,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",\n",
    "            max_grad_norm=1.0,\n",
    "            save_steps=500,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=25,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n",
    "            run_name=f\"codellama-cobol-python-{self.config.get('experiment_name', 'default')}\",\n",
    "            dataloader_drop_last=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "        )\n",
    "\n",
    "    from transformers import pipeline\n",
    "\n",
    "    def setup_pipeline(self):\n",
    "        if not hasattr(self, \"model\") or not hasattr(self, \"tokenizer\"):\n",
    "            raise RuntimeError(\"Model and tokenizer must be loaded before setting up pipeline.\")\n",
    "    \n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Execute the fine-tuning process\"\"\"\n",
    "        print(\"üöÄ Starting CodeLlama COBOL‚ÜíPython fine-tuning...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get(\"use_wandb\", False):\n",
    "            wandb.init(\n",
    "                project=self.config.get(\"wandb_project\", \"codellama-cobol-python\"),\n",
    "                name=f\"codellama-{self.config.get('experiment_name', 'default')}\"\n",
    "            )\n",
    "            \n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_arguments()\n",
    "        \n",
    "        # Data collator for code generation\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    "        )\n",
    "        \n",
    "        # Initialize SFTTrainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.combined_dataset,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "        )\n",
    "        \n",
    "        # Clear cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Monitor initial GPU usage\n",
    "        self.monitor_gpu_usage()\n",
    "        \n",
    "        # Start training\n",
    "        print(\"üéØ Training started...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"üíæ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(\"‚úÖ CodeLlama COBOL‚ÜíPython fine-tuning completed!\")\n",
    "        \n",
    "    def evaluate_model(self, test_samples=None):\n",
    "        \"\"\"Evaluate the fine-tuned model on COBOL to Python conversion\"\"\"\n",
    "        if test_samples is None:\n",
    "            # Use a small subset for quick evaluation\n",
    "            test_samples = self.combined_dataset.select(range(min(10, len(self.combined_dataset))))\n",
    "            \n",
    "        print(\"üîç Evaluating model performance...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sample in test_samples:\n",
    "                # Extract COBOL code from the sample\n",
    "                text = sample['text']\n",
    "                cobol_start = text.find('```cobol') + 8\n",
    "                cobol_end = text.find('```', cobol_start)\n",
    "                cobol_code = text[cobol_start:cobol_end].strip()\n",
    "                \n",
    "                # Generate Python code\n",
    "                prompt = f\"\"\"### Task: Convert the following COBOL code to Python\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{cobol_code}\n",
    "```\n",
    "\n",
    "### Python Code:\n",
    "```python\"\"\"\n",
    "                \n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                python_code = generated.split('```python')[-1].split('```')[0].strip()\n",
    "                \n",
    "                results.append({\n",
    "                    'cobol_code': cobol_code,\n",
    "                    'generated_python': python_code\n",
    "                })\n",
    "                \n",
    "        print(f\"‚úÖ Evaluated {len(results)} samples\")\n",
    "        return results\n",
    "        \n",
    "    def monitor_gpu_usage(self):\n",
    "        \"\"\"Monitor GPU usage during training\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved, {total_memory:.2f}GB total\")\n",
    "\n",
    "    def _extract_true_python_code(self, sample_text):\n",
    "        \"\"\"\n",
    "        Helper to extract the true Python code from the 'text' field of a dataset sample.\n",
    "        This needs to be robust to the different formatting styles you used\n",
    "        across the combined datasets.\n",
    "        \"\"\"\n",
    "        # This is the same helper function as in Option 1\n",
    "        python_code_start_marker_1 = \"### Python Code:\\n```python\"\n",
    "        python_code_start_marker_2 = \"### Python Structure:\\n```python\"\n",
    "        python_code_start_marker_3 = \"### Convert to Python equivalent:\\n```python\"\n",
    "        python_end_marker = \"```\"\n",
    "\n",
    "        start_index = -1\n",
    "        if python_code_start_marker_1 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_1) + len(python_code_start_marker_1)\n",
    "        elif python_code_start_marker_2 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_2) + len(python_code_start_marker_2)\n",
    "        elif python_code_start_marker_3 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_3) + len(python_code_start_marker_3)\n",
    "\n",
    "        if start_index != -1:\n",
    "            end_index = sample_text.find(python_end_marker, start_index)\n",
    "            if end_index != -1:\n",
    "                return sample_text[start_index:end_index].strip()\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def calculate_and_print_metrics(self, test_dataset, predictions, num_examples_to_show=3):\n",
    "        \"\"\"\n",
    "        Calculates and prints evaluation metrics for code translation.\n",
    "\n",
    "        Args:\n",
    "            test_dataset (Dataset): The Hugging Face Dataset used for testing.\n",
    "            predictions (list): A list of generated Python code strings (y_pred).\n",
    "        \"\"\"\n",
    "        if not isinstance(test_dataset, Dataset):\n",
    "            raise TypeError(\"test_dataset must be a Hugging Face Dataset object.\")\n",
    "\n",
    "        if len(test_dataset) != len(predictions):\n",
    "            raise ValueError(\"Mismatch in number of samples between test_dataset and predictions.\")\n",
    "\n",
    "        # Extract ground truth (y_true)\n",
    "        y_true = [self._extract_true_python_code(sample[\"text\"]) for sample in test_dataset]\n",
    "\n",
    "        # Filter out samples where true_code could not be extracted\n",
    "        valid_pairs = [(gt, pred) for gt, pred in zip(y_true, predictions) if gt]\n",
    "\n",
    "        if not valid_pairs:\n",
    "            print(\"‚ùó No valid ground truth Python code found in the test dataset for evaluation metrics.\")\n",
    "            return {}\n",
    "\n",
    "        y_true_filtered = [pair[0] for pair in valid_pairs]\n",
    "        y_pred_filtered = [pair[1] for pair in valid_pairs]\n",
    "\n",
    "        print(f\"Evaluating metrics on {len(y_true_filtered)} valid samples.\")\n",
    "\n",
    "        def code_similarity(a, b):\n",
    "            return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n",
    "\n",
    "        similarities = [code_similarity(gt, pred) for gt, pred in zip(y_true_filtered, y_pred_filtered)]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "\n",
    "        exact_matches = sum(1 for gt, pred in zip(y_true_filtered, y_pred_filtered) if gt.strip() == pred.strip())\n",
    "        accuracy = exact_matches / len(y_true_filtered)\n",
    "\n",
    "        print(f\"\\n--- Evaluation Metrics ---\")\n",
    "        print(f\"Exact Match Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Average Similarity Score: {avg_similarity:.3f}\")\n",
    "\n",
    "        print(f\"\\n--- Sample Outputs ({min(num_examples_to_show, len(y_true_filtered))} examples) ---\")\n",
    "        for i in range(min(num_examples_to_show, len(y_true_filtered))):\n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            print(\"True Output:\\n\", y_true_filtered[i])\n",
    "            print(\"Predicted Output:\\n\", y_pred_filtered[i])\n",
    "            print(\"Similarity Score:\", code_similarity(y_true_filtered[i], y_pred_filtered[i]))\n",
    "\n",
    "        return {\n",
    "            \"exact_match_accuracy\": accuracy,\n",
    "            \"average_similarity\": avg_similarity\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5db95e3d-896a-4cb5-87e4-52e1c8750314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cca336a-7a15-497a-9cc0-0ee64f16bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All libraries imported successfully!\n",
      "üî• PyTorch version: 2.5.1+cu121\n",
      "ü§ó Transformers version: 4.52.4\n",
      "üöÄ CUDA available: True\n",
      "üíæ GPU: NVIDIA GeForce RTX 4090 D\n",
      "üéØ GPU Memory: 23.4 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    pipeline, \n",
    "    logging,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    # CodeLlama specific\n",
    "    CodeLlamaTokenizer,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "\n",
    "# Dataset handling\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fine-tuning libraries\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# Monitoring and optimization\n",
    "import wandb\n",
    "import pynvml\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06b60358-5c6d-4ecd-ad5f-2f435e390318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGING_FACE_WRITE_API_KEY\"] = \"\"\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75db2a19-16b7-4e71-984e-f788970420fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import CommitInfo\n",
    "\n",
    "# Set environment variables in your shell or .bashrc/.zshrc\n",
    "# export HUGGING_FACE_WRITE_API_KEY=\"hf_your_write_token_here\"\n",
    "# export HUGGINGFACE_TOKEN=\"hf_your_read_token_here\"\n",
    "\n",
    "# In your Python code\n",
    "secret_value_0 = os.getenv(\"HUGGING_FACE_WRITE_API_KEY\")\n",
    "secret_value_1 = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not secret_value_0 or not secret_value_1:\n",
    "    raise ValueError(\"Please set HUGGING_FACE_WRITE_API_KEY and HUGGINGFACE_TOKEN environment variables\")\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39314d5d-636a-4e4c-9637-277ec8622d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert this COBOL code to Python:\n",
      "\n",
      "IDENTIFICATION DIVISION.\n",
      "PROGRAM-ID. HELLO.\n",
      "PROCEDURE DIVISION.\n",
      "DISPLAY 'HELLO, WORLD'.\n",
      "STOP RUN.\n",
      "\n",
      "Comment: What have you tried so far?\n",
      "\n",
      "Comment: I tried to convert it to Python but I am not sure how to do it.\n",
      "\n",
      "Comment: What is the problem you are facing?\n",
      "\n",
      "Comment: I am not sure how to convert it to Python.\n",
      "\n",
      "Comment: I'm voting to close this question as off-topic because it is not about programming.\n",
      "\n",
      "Answer: \\begin{code}\n",
      "print('HELLO, WORLD')\n",
      "\\end{code}\n",
      "\n",
      "Comment: I tried this but it is not working.\n",
      "\n",
      "Comment: @user3601234: What do you mean by \"not working\"? What error message do you get?\n",
      "\n",
      "Comment: I am getting a syntax error.\n",
      "\n",
      "Comment: @user3601234: What is the exact error message?\n",
      "\n",
      "Comment: @user3601234: You need to\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"  # or your local path e.g. \"./models/codellama\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model for inference\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Run test inference\n",
    "prompt = \"Convert this COBOL code to Python:\\n\\nIDENTIFICATION DIVISION.\\nPROGRAM-ID. HELLO.\\nPROCEDURE DIVISION.\\nDISPLAY 'HELLO, WORLD'.\\nSTOP RUN.\"\n",
    "outputs = pipe(prompt, max_new_tokens=200, do_sample=False)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b506054-ce60-4031-a2eb-719706f859c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Load relevant subsets\n",
    "mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n",
    "# mainframe_small = mainframe.shuffle(seed=42).select(range(1000)) # for kaggle\n",
    "\n",
    "# full dataset (3TB of data)\n",
    "# ds = load_dataset(\"bigcode/the-stack\", split=\"train\")\n",
    "\n",
    "# specific language (e.g. Dockerfiles)\n",
    "stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:5%]\")\n",
    "stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:5%]\")\n",
    "\n",
    "python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "\n",
    "# Combine relevant Python corpora\n",
    "python_combined = concatenate_datasets([stack_python, python_set])\n",
    "\n",
    "# Now build translation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2225bf03-97b8-4d5c-8bd4-a997849bddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'prompt', 'source', 'summary']\n"
     ]
    }
   ],
   "source": [
    "print(mainframe.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3697f34-80f3-46fc-865a-07d23fe71c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format MainframeBench COBOL data\n",
    "def format_mainframe(example):\n",
    "    return {\n",
    "        \"input\": example[\"source\"],\n",
    "        \"output\": \"# Python translation to be generated or is unknown for now.\",\n",
    "    }\n",
    "\n",
    "mainframe_formatted = mainframe.map(format_mainframe)\n",
    "\n",
    "# Build real COBOL ‚Üí Python pairs from The Stack\n",
    "paired_data = []\n",
    "for i in range(min(len(stack_cobol), len(python_combined))):\n",
    "    paired_data.append({\n",
    "        \"input\": stack_cobol[i][\"content\"],\n",
    "        \"output\": python_combined[i][\"content\"]\n",
    "    })\n",
    "\n",
    "# Combine both: real pairs + placeholder Mainframe data\n",
    "combined_data = Dataset.from_list(paired_data + list(mainframe_formatted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07dd3c4-99c8-42d2-924d-c0d340d9f030",
   "metadata": {},
   "source": [
    "## Combine dataset \n",
    "| Dataset Name                    | HF ID                                            | Contains                  | Use                                |\n",
    "| ------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------- |\n",
    "| **MainframeBench**              | `Fsoft-AIC/MainframeBench`                       | COBOL code + descriptions | Base COBOL understanding           |\n",
    "| **The Stack**                   | `bigcode/the-stack`                              | COBOL + other languages   | Language variety + COBOL samples   |\n",
    "| **Python Code Dataset**         | `jtatman/python-code-dataset-500k`               | Python code               | Target code corpus                 |\n",
    "| **SantaCoder Fine-tuned COBOL** | `muhtasham/santacoder-finetuned-the-stack-cobol` | Pretrained model          | Base model for COBOL understanding |\n",
    "| **General Code**                | `codeparrot/github-code`                         | Multi-language            | Extra fine-tuning                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b69f2e6-b5c8-4289-9bc1-adb4bd59ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "splits = combined_data.train_test_split(test_size=0.2, seed=42)\n",
    "eval_test = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"eval\": eval_test[\"train\"],\n",
    "    \"test\": eval_test[\"test\"]\n",
    "})\n",
    "\n",
    "print(dataset_dict[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "299565f6-3dd7-4cdd-9da4-0b9658eff3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/system/miniconda3/envs/torch_env/lib/python3.10/site-packages/bitsandbytes/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e288b36-9bfe-4087-aff1-6abd04212cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_COBOL_FINE_TUNE_CONFIG = {\n",
    "    # === Model ===\n",
    "    \"model_name\": \"codellama/CodeLlama-7b-Instruct-hf\",  # or \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "\n",
    "    # === LoRA PEFT ===\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "\n",
    "    # === Dataset ===\n",
    "    \"dataset_name\": \"custom\",  # Will use your dataset loading logic\n",
    "    \"dataset_text_field\": \"input\",  # Your format_mainframe() defines \"input\" and \"output\"\n",
    "    \"max_length\": 2048,\n",
    "    \"packing\": False,\n",
    "\n",
    "    # === Training ===\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 8,       # 8 * 2 GPUs * 4 grad_accum = 64 effective\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_steps\": 100,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "\n",
    "    # === Evaluation & Saving ===\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"load_best_model_at_end\": True,\n",
    "\n",
    "    # === Output ===\n",
    "    \"output_dir\": \"./codellama-cobol-finetuned\",\n",
    "\n",
    "    # === Advanced ===\n",
    "    \"use_flash_attention\": True,\n",
    "    \"push_to_hub\": False,\n",
    "    \"hub_model_id\": \"\",\n",
    "\n",
    "    # flash-attn does not yet support CUDA 12.1 officially\n",
    "    \"use_flash_attention\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45590542-e9f0-4ac8-9174-b0f17e0a858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Only one .index.json file found. No action needed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "model_dir = \"./models/codellama\"\n",
    "index_files = glob.glob(os.path.join(model_dir, \"*.index.json\"))\n",
    "\n",
    "# Only keep the first file, delete others\n",
    "if len(index_files) > 1:\n",
    "    print(\"üîç Found multiple .index.json files:\")\n",
    "    for i, f in enumerate(index_files):\n",
    "        print(f\"{i+1}. {f}\")\n",
    "    \n",
    "    # Sort and keep the first one, delete the rest\n",
    "    files_to_delete = index_files[1:]\n",
    "    for f in files_to_delete:\n",
    "        print(f\"üóëÔ∏è Deleting: {f}\")\n",
    "        os.remove(f)\n",
    "else:\n",
    "    print(\"‚úÖ Only one .index.json file found. No action needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99b29703-6f57-4346-8f0c-03b81c6ee49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA Setup:\n",
      "   - CUDA Version: 12.1\n",
      "   - cuDNN Version: 90100\n",
      "   - Available GPUs: 2\n",
      "   - GPU 0: NVIDIA GeForce RTX 4090 D (23 GB)\n",
      "   - GPU 1: NVIDIA GeForce RTX 4090 D (23 GB)\n",
      "   - TensorFloat-32 enabled for RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Initialize fine-tuner\n",
    "fine_tuner = OptimizedCodeLlamaFineTuner(LLAMA_COBOL_FINE_TUNE_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33690a99-ae1f-4caa-94a6-ce8d30b4954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y flash-attn\n",
    "# !pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204843f5-bb5d-4d26-9729-052e8996fbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Loading CodeLlama tokenizer...\n",
      "üß† Loading CodeLlama model (multi-GPU, full precision)...\n",
      "‚úÖ Model loaded on devices: {'': 'cpu'}\n",
      "üîß LoRA Configuration:\n",
      "   - Trainable parameters: 39,976,960\n",
      "   - Total parameters: 6,778,523,648\n",
      "   - Trainable %: 0.59%\n",
      "üîß LoRA Configuration:\n",
      "   - Trainable parameters: 39,976,960\n",
      "   - Total parameters: 6,778,523,648\n",
      "   - Trainable %: 0.59%\n",
      "Loading your specific datasets...\n",
      "Loading MainframeBench COBOL dataset...\n",
      "Loading COBOL from The Stack...\n",
      "Loading Python datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 15000/18089 [00:07<00:01, 2243.97 examples/s]"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "fine_tuner.load_model_and_tokenizer()\n",
    "\n",
    "fine_tuner.setup_pipeline()\n",
    "\n",
    "# Setup LoRA\n",
    "fine_tuner.setup_lora()\n",
    "\n",
    "# Load and prepare dataset\n",
    "fine_tuner.load_your_specific_datasets()\n",
    "\n",
    "# Monitor initial GPU usage\n",
    "print(\"üìä Initial GPU Memory Usage:\")\n",
    "fine_tuner.monitor_gpu_usage()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a73ad4-e195-4e32-ab3f-0a68108fabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "fine_tuner.train()\n",
    "\n",
    "# Final GPU usage\n",
    "print(\"üìä Final GPU Memory Usage:\")\n",
    "fine_tuner.monitor_gpu_usage()\n",
    "\n",
    "# Optional: Optimize for inference\n",
    "# fine_tuner.optimize_for_inference(config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f2bb5-2f92-469d-b57a-0eedd124164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "\n",
    "# Get the output directory path directly from the trainer's arguments\n",
    "# This ensures consistency with where the trainer has been saving checkpoints\n",
    "final_model_output_path = trainer.args.output_dir\n",
    "\n",
    "trainer.save_model(final_model_output_path)\n",
    "fine_tuner.tokenizer.save_pretrained(final_model_output_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {final_model_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea397617-f438-4caa-adda-20c5edbdd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n",
    "# and 'dataset_dict' contains your \"test\" split.\n",
    "\n",
    "print(\"Starting model evaluation...\")\n",
    "\n",
    "# 1. Generate predictions using the evaluate_model method\n",
    "# This method generates predictions and extracts the true labels\n",
    "# The method will return a list of dictionaries, where each dict might contain\n",
    "# 'cobol_code', 'generated_python' (prediction), and potentially 'true_python_code' (ground truth).\n",
    "evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n",
    "\n",
    "# From the evaluation_results, extract just the generated Python code (y_pred)\n",
    "# and the true Python code (y_true) for passing to the metrics calculator.\n",
    "# Note: The 'evaluate_model' method should already be structured to return these.\n",
    "# If 'evaluate_model' returns predictions directly, then you extract y_pred from there.\n",
    "# And y_true is extracted by calculate_and_print_metrics internally using test_dataset.\n",
    "\n",
    "# Example: If evaluation_results is a list of dictionaries with a 'generated_python' key\n",
    "y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n",
    "\n",
    "# 2. Calculate and print metrics using the calculate_and_print_metrics method\n",
    "# This method takes the test_dataset (to re-extract y_true internally) and the predictions.\n",
    "fine_tuner.calculate_and_print_metrics(test_dataset=dataset_dict[\"test\"], predictions=y_pred_for_metrics)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c5576-0e56-4bff-8b5d-616226122933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n",
    "# and the training has completed and model saved.\n",
    "\n",
    "# The base model name is stored in your fine_tuner's config\n",
    "base_model_name = fine_tuner.config[\"model_name\"]\n",
    "\n",
    "# The output directory for the fine-tuned model is stored in the trainer's args\n",
    "# which came from your fine_tuner's config.\n",
    "fine_tuned_model_path = trainer.args.output_dir\n",
    "\n",
    "print(f\"Base Model: {base_model_name}\")\n",
    "print(f\"Fine-tuned Model Saved At: {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b17279-8705-4477-a78d-729a7f47b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use the 'base_model_name' variable defined in the previous step\n",
    "# (which came from fine_tuner.config[\"model_name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, # Use the defined base_model_name\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # Use torch.bfloat16 for consistency with your config_dict's bnb_4bit_compute_dtype\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model '{base_model_name}' and its tokenizer reloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91e074-e627-4013-9482-8e4b8872b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Merge adapter with base model\n",
    "# 'base_model_reload' is the AutoModelForCausalLM you just reloaded.\n",
    "# 'fine_tuned_model_path' is the path where your LoRA adapters were saved by the trainer.\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path)\n",
    "\n",
    "# This merges the LoRA adapter weights into the base model weights\n",
    "# and removes the adapter layers, leaving you with a single, merged model.\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"LoRA adapters successfully merged into the base model. The merged model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc4f55-7be9-46ab-9a04-4ce54f262dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # Ensure pipeline is imported\n",
    "\n",
    "cobol_code = \"\"\"\n",
    "        IDENTIFICATION DIVISION.\n",
    "        PROGRAM-ID. HELLO.\n",
    "        PROCEDURE DIVISION.\n",
    "            DISPLAY 'HELLO, WORLD'.\n",
    "            STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,        # This is your merged model\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency with training and loading\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Generated Python Code:\\n\")\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6deb8e-7beb-4425-8585-f4249c913ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save your final merged model\n",
    "model_dir = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "\n",
    "# Save the merged model (which now contains the LoRA weights)\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"Your final merged model and tokenizer have been saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da409f8a-9b31-4845-8904-3433715cce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Assuming base_model_name and fine_tuned_model_path are defined from previous steps:\n",
    "# For example:\n",
    "# base_model_name = fine_tuner.config[\"model_name\"]\n",
    "# fine_tuned_model_path = trainer.args.output_dir # or your chosen output directory for adapters\n",
    "\n",
    "print(f\"Loading tokenizer from: {base_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(f\"Loading base model from: {base_model_name} with torch_dtype=torch.bfloat16\")\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, # Use the correct variable name for the base model identifier\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading fine-tuned adapters from: {fine_tuned_model_path}\")\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path) # Use the correct path for adapters\n",
    "\n",
    "print(\"Merging adapters into the base model...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"Model and adapters merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe90ca-16c0-4d12-9b6c-26b0bddc3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce598f-cc66-496c-9c48-4c9ba9cab666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and register to Hugging Face Hub\n",
    "# Ensure you are logged in to Hugging Face locally (huggingface-cli login)\n",
    "\n",
    "# Push the merged model to your Hugging Face repository\n",
    "model.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n",
    "\n",
    "# Push the tokenizer to the same repository\n",
    "tokenizer.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n",
    "\n",
    "print(\"\\nModel and tokenizer successfully pushed to Hugging Face Hub!\")\n",
    "print(\"You can find them at: https://huggingface.co/dhirajpatra/codellama-cobol-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41378cf5-9db0-483e-a61c-49420fbca384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Assuming 'model_dir' is the string \"codellama-cobol-python\" you used when saving\n",
    "# Or, even better, just use the full repo ID directly for clarity:\n",
    "hf_repo_id = \"dhirajpatra/codellama-cobol-python\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
    "\n",
    "print(f\"Model and tokenizer loaded successfully from Hugging Face Hub: {hf_repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e2e2d-e07b-4e49-b1f1-725eb9326805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use the full repository ID directly for clarity and robustness\n",
    "# Assuming model_dir was \"codellama-cobol-python\" from your save step\n",
    "model_id = \"dhirajpatra/codellama-cobol-python\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "print(f\"**Model and tokenizer loaded successfully from Hugging Face Hub:** `{model_id}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5cfe-9580-46ad-a0db-aed9ca12501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # Ensure pipeline is imported\n",
    "import torch # Ensure torch is imported if not already\n",
    "\n",
    "cobol_code = \"\"\"\n",
    "        IDENTIFICATION DIVISION.\n",
    "        PROGRAM-ID. HELLO.\n",
    "        PROCEDURE DIVISION.\n",
    "            DISPLAY 'HELLO, WORLD'.\n",
    "            STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16, # Changed to bfloat16 for consistency and performance\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Generated Python Code:\\n\")\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3d25e-2e44-40b6-acb1-92656280dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, DatasetDict\n",
    "# Assuming fine_tuner is your OptimizedCodeLlamaFineTuner instance\n",
    "# and dataset_dict contains your \"test\" split.\n",
    "# And assuming you've already run the evaluation_results step:\n",
    "# evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n",
    "# y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n",
    "\n",
    "# 1. Load the BLEU metric\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "# 2. Prepare predictions (preds) - This is your generated code\n",
    "# Assuming y_pred_for_metrics holds the list of generated Python code strings\n",
    "preds = y_pred_for_metrics\n",
    "\n",
    "# 3. Prepare references (refs) - This requires extracting the true Python code\n",
    "# The calculate_and_print_metrics method has a helper for this.\n",
    "# We need to call that helper or re-extract here.\n",
    "# Let's re-extract the filtered ground truth for consistency with metrics.\n",
    "y_true = [fine_tuner._extract_true_python_code(sample[\"text\"]) for sample in dataset_dict[\"test\"]]\n",
    "valid_pairs = [(gt, pred) for gt, pred in zip(y_true, preds) if gt]\n",
    "\n",
    "# preds should be from the filtered valid_pairs as well for direct comparison\n",
    "# If you ran fine_tuner.calculate_and_print_metrics, it already filtered.\n",
    "# For BLEU, ensure preds and refs correspond to the same filtered set.\n",
    "preds_filtered_for_bleu = [pair[1] for pair in valid_pairs]\n",
    "refs_filtered_for_bleu = [[pair[0]] for pair in valid_pairs] # BLEU expects a list of lists for references\n",
    "\n",
    "if not preds_filtered_for_bleu:\n",
    "    print(\"‚ùó No valid samples to compute BLEU score on.\")\n",
    "else:\n",
    "    # 4. Compute the BLEU score\n",
    "    results = bleu.compute(predictions=preds_filtered_for_bleu, references=refs_filtered_for_bleu)\n",
    "\n",
    "    print(\"\\n--- BLEU Score ---\")\n",
    "    print(f\"BLEU score: {results['bleu']:.4f}\")\n",
    "    # You can also print other details if available in results, e.g., 'precisions'\n",
    "    # print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780390-3926-443f-af7f-41ca491c4f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c69e25-4189-40b6-ba4e-04c41d393330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e913f-5deb-4ecc-90db-f3eeac0c20fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
>>>>>>> 98cddf65 (updated)
