{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install transformers and related libraries\n",
    "pip install transformers>=4.36.0\n",
    "pip install datasets\n",
    "pip install peft>=0.7.0\n",
    "pip install trl\n",
    "pip install bitsandbytes\n",
    "pip install accelerate\n",
    "pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Optional: For monitoring\n",
    "pip install wandb\n",
    "pip install pynvml\n",
    "\n",
    "# For TensorRT optimization (optional)\n",
    "pip install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Fine-tuning Pipeline for Meta Llama 3.2 3B\n",
    "Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import tensorrt as trt\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-06-13T05:37:25.674524Z",
     "iopub.status.idle": "2025-06-13T05:37:25.675276Z",
     "shell.execute_reply": "2025-06-13T05:37:25.675121Z",
     "shell.execute_reply.started": "2025-06-13T05:37:25.675106Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Fetch token from Kaggle Secrets\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")  # or use os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA optimizations\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "model_name = \"codellama/CodeLlama-7B-Instruct-hf\"\n",
    "output_model = \"CodeLlama-7B-Instruct-COBOL-to-Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:35:22.508266Z",
     "iopub.status.busy": "2025-06-13T05:35:22.508024Z",
     "iopub.status.idle": "2025-06-13T05:35:41.124716Z",
     "shell.execute_reply": "2025-06-13T05:35:41.123954Z",
     "shell.execute_reply.started": "2025-06-13T05:35:22.508249Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U -q --no-deps xformers trl transformers datasets peft accelerate huggingface_hub bitsandbytes optimum auto-gptq gekko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedLlamaFineTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_nvidia_environment()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        \n",
    "    def setup_nvidia_environment(self):\n",
    "        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available!\")\n",
    "            \n",
    "        # Initialize NVML for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        \n",
    "        print(f\"ðŸš€ NVIDIA Setup:\")\n",
    "        print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   - Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode()\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n",
    "            \n",
    "        # Enable TensorFloat-32 for A100/RTX 30xx series\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"   - TensorFloat-32 enabled for Ampere+ GPUs\")\n",
    "            \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load codellama 7B with optimized quantization\"\"\"\n",
    "        \n",
    "        # BitsAndBytesConfig for 4-bit quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“¥ Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(\"ðŸ“¥ Loading model with 4-bit quantization...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\" if self.config.get(\"use_flash_attention\", True) else \"eager\"\n",
    "        )\n",
    "        \n",
    "        # Prepare model for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        print(f\"âœ… Model loaded on: {self.model.device}\")\n",
    "        \n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA configuration for efficient fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get(\"lora_r\", 16),\n",
    "            lora_alpha=self.config.get(\"lora_alpha\", 32),\n",
    "            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"ðŸŽ¯ LoRA Configuration:\")\n",
    "        print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   - Total parameters: {total_params:,}\")\n",
    "        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "        \n",
    "    def load_and_prepare_dataset(self):\n",
    "        \"\"\"Load and prepare dataset for training\"\"\"\n",
    "        dataset_name = self.config.get(\"dataset_name\", \"tatsu-lab/alpaca\")\n",
    "        \n",
    "        print(f\"ðŸ“Š Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        if isinstance(dataset_name, str):\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        else:\n",
    "            # Handle custom dataset loading\n",
    "            dataset = dataset_name\n",
    "            \n",
    "        # Prepare dataset based on format\n",
    "        if \"instruction\" in dataset.column_names and \"output\" in dataset.column_names:\n",
    "            # Alpaca format\n",
    "            def format_alpaca(examples):\n",
    "                texts = []\n",
    "                for instruction, input_text, output in zip(\n",
    "                    examples[\"instruction\"], \n",
    "                    examples.get(\"input\", [\"\"] * len(examples[\"instruction\"])), \n",
    "                    examples[\"output\"]\n",
    "                ):\n",
    "                    if input_text:\n",
    "                        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "                    else:\n",
    "                        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "                    texts.append(prompt)\n",
    "                return {\"text\": texts}\n",
    "                \n",
    "            dataset = dataset.map(format_alpaca, batched=True, remove_columns=dataset.column_names)\n",
    "            \n",
    "        elif \"text\" not in dataset.column_names:\n",
    "            raise ValueError(\"Dataset must have 'text' column or Alpaca format (instruction/output)\")\n",
    "            \n",
    "        # Filter by length to avoid OOM\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        dataset = dataset.filter(lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length)\n",
    "        \n",
    "        # Take subset if specified\n",
    "        if self.config.get(\"max_samples\"):\n",
    "            dataset = dataset.select(range(min(len(dataset), self.config[\"max_samples\"])))\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        print(f\"âœ… Dataset prepared: {len(dataset)} samples\")\n",
    "        \n",
    "    def setup_training_arguments(self):\n",
    "        \"\"\"Setup optimized training arguments\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.get(\"output_dir\", \"./\" + output_model),\n",
    "            \n",
    "            # Training hyperparameters\n",
    "            num_train_epochs=self.config.get(\"num_epochs\", 3),\n",
    "            per_device_train_batch_size=self.config.get(\"batch_size\", 4),\n",
    "            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 4),\n",
    "            learning_rate=self.config.get(\"learning_rate\", 2e-4),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            \n",
    "            # NVIDIA optimizations\n",
    "            bf16=True,  # Use bfloat16 for Ampere+ GPUs\n",
    "            tf32=True,  # Enable TensorFloat-32\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=4,\n",
    "            \n",
    "            # Memory optimizations\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",  # Fused optimizer for NVIDIA GPUs\n",
    "            \n",
    "            # Logging and saving\n",
    "            logging_steps=10,\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            evaluation_strategy=\"no\",  # Disable eval to save memory\n",
    "            \n",
    "            # Additional optimizations\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n",
    "            run_name=f\"llama-3.2-3b-finetune-{self.config.get('experiment_name', 'default')}\",\n",
    "            \n",
    "            # DDP settings (if using multiple GPUs)\n",
    "            ddp_find_unused_parameters=False,\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Execute the fine-tuning process\"\"\"\n",
    "        print(\"ðŸš€ Starting fine-tuning process...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get(\"use_wandb\", False):\n",
    "            wandb.init(\n",
    "                project=self.config.get(\"wandb_project\", \"llama-finetune\"),\n",
    "                name=f\"llama-3.2-3b-{self.config.get('experiment_name', 'default')}\"\n",
    "            )\n",
    "            \n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_arguments()\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize SFTTrainer (optimized for instruction tuning)\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.config.get(\"max_length\", 2048),\n",
    "            packing=False,  # Disable packing to avoid issues with instruction format\n",
    "        )\n",
    "        \n",
    "        # Clear cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Start training\n",
    "        print(\"ðŸŽ¯ Training started...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"ðŸ’¾ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(\"âœ… Fine-tuning completed!\")\n",
    "        \n",
    "    def optimize_for_inference(self, model_path):\n",
    "        \"\"\"Optimize model for inference using TensorRT (optional)\"\"\"\n",
    "        print(\"âš¡ Optimizing model for inference...\")\n",
    "        \n",
    "        # Load the fine-tuned model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "        model.eval()\n",
    "        \n",
    "        # Convert to TorchScript\n",
    "        traced_model = torch.jit.trace(model, example_inputs=(torch.randint(0, 1000, (1, 512)),))\n",
    "        \n",
    "        # Save optimized model\n",
    "        optimized_path = f\"{model_path}_optimized\"\n",
    "        traced_model.save(f\"{optimized_path}/traced_model.pt\")\n",
    "        \n",
    "        print(f\"âœ… Optimized model saved to: {optimized_path}\")\n",
    "        \n",
    "    def monitor_gpu_usage(self):\n",
    "        \"\"\"Monitor GPU usage during training\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:35:41.126357Z",
     "iopub.status.busy": "2025-06-13T05:35:41.126039Z",
     "iopub.status.idle": "2025-06-13T05:36:08.969576Z",
     "shell.execute_reply": "2025-06-13T05:36:08.969017Z",
     "shell.execute_reply.started": "2025-06-13T05:35:41.126311Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 05:35:53.835404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749792954.026516      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749792954.077577      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging,\n",
    "                          Trainer,\n",
    "                          DataCollatorForLanguageModeling)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:08.970729Z",
     "iopub.status.busy": "2025-06-13T05:36:08.970188Z",
     "iopub.status.idle": "2025-06-13T05:36:09.214670Z",
     "shell.execute_reply": "2025-06-13T05:36:09.214092Z",
     "shell.execute_reply.started": "2025-06-13T05:36:08.970709Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HUGGING_FACE_WRITE_API_KEY\")\n",
    "secret_value_1 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:09.215479Z",
     "iopub.status.busy": "2025-06-13T05:36:09.215251Z",
     "iopub.status.idle": "2025-06-13T05:36:09.396240Z",
     "shell.execute_reply": "2025-06-13T05:36:09.395413Z",
     "shell.execute_reply.started": "2025-06-13T05:36:09.215461Z"
    }
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # Model details\n",
    "# model_id = \"codellama/CodeLlama-7B-Instruct-hf\"\n",
    "# local_dir = \"/kaggle/working/codellama\"\n",
    "\n",
    "# # Download full snapshot locally\n",
    "# model_path = snapshot_download(\n",
    "#     repo_id=model_id,\n",
    "#     local_dir=local_dir,\n",
    "#     local_dir_use_symlinks=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:09.398941Z",
     "iopub.status.busy": "2025-06-13T05:36:09.398725Z",
     "iopub.status.idle": "2025-06-13T05:36:32.049812Z",
     "shell.execute_reply": "2025-06-13T05:36:32.048810Z",
     "shell.execute_reply.started": "2025-06-13T05:36:09.398924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621d2b1eea874d86ae3a90ec972ee43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99d835aa48b42188020bbb00c9e93bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b78b907cd64ae8863e90cd1c6d10f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2698fd7b0e434af8b041b50cbe91ef68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709ca35213cd4aa3bdc238cf80235298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720ed4faa5584a4d96aec30013b98129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llama.py:   0%|          | 0.00/8.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GPTQ:\n",
      "- configuration_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1840623b7ff149cc9bfbe8359b8b302f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quantize_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1825e9ad7c945e6a4271060e217f292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6131b026e2f24e098153366fbfcdd1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llama.py:   0%|          | 0.00/45.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GPTQ:\n",
      "- modeling_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TheBloke/CodeLlama-7B-Instruct-GPTQ\"\n",
    "\n",
    "device_target = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    device=device_target,  # ðŸ’¡ explicitly resolved string\n",
    "    use_triton=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:32.051054Z",
     "iopub.status.busy": "2025-06-13T05:36:32.050759Z",
     "iopub.status.idle": "2025-06-13T05:36:32.056034Z",
     "shell.execute_reply": "2025-06-13T05:36:32.055192Z",
     "shell.execute_reply.started": "2025-06-13T05:36:32.051012Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # running on kaggle\n",
    "# # Load tokenizer and model from local downloaded path\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     return_dict=True,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:32.057057Z",
     "iopub.status.busy": "2025-06-13T05:36:32.056794Z",
     "iopub.status.idle": "2025-06-13T05:36:39.603094Z",
     "shell.execute_reply": "2025-06-13T05:36:39.602226Z",
     "shell.execute_reply.started": "2025-06-13T05:36:32.057036Z"
    }
   },
   "outputs": [],
   "source": [
    "# # if not running on kaggle\n",
    "# model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:39.604207Z",
     "iopub.status.busy": "2025-06-13T05:36:39.603908Z",
     "iopub.status.idle": "2025-06-13T05:36:40.397839Z",
     "shell.execute_reply": "2025-06-13T05:36:40.397247Z",
     "shell.execute_reply.started": "2025-06-13T05:36:39.604183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM', 'LlamaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# Create inference pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:37:46.150629Z",
     "iopub.status.busy": "2025-06-13T05:37:46.150348Z",
     "iopub.status.idle": "2025-06-13T05:39:05.774919Z",
     "shell.execute_reply": "2025-06-13T05:39:05.774090Z",
     "shell.execute_reply.started": "2025-06-13T05:37:46.150609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to convert COBOL code into Python?\n",
      "\n",
      "I have a COBOL code which I want to convert into Python. Can someone please help me understand the conversion process?\n",
      "\n",
      "Comment: Is the COBOL code in a file? If so, have you attempted anything yet?\n",
      "\n",
      "Comment: Yes, it is in a file. I have not attempted anything yet. I am a newbie to python and COBOL.\n",
      "\n",
      "Comment: No, not really. I am looking for a step by step conversion guide.\n",
      "\n",
      "Comment: I've put together a step-by-step guide.  It\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to convert COBOL code into Python?\"\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Combine dataset \n",
    "| Dataset Name                    | HF ID                                            | Contains                  | Use                                |\n",
    "| ------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------- |\n",
    "| **MainframeBench**              | `Fsoft-AIC/MainframeBench`                       | COBOL code + descriptions | Base COBOL understanding           |\n",
    "| **The Stack**                   | `bigcode/the-stack`                              | COBOL + other languages   | Language variety + COBOL samples   |\n",
    "| **Python Code Dataset**         | `jtatman/python-code-dataset-500k`               | Python code               | Target code corpus                 |\n",
    "| **SantaCoder Fine-tuned COBOL** | `muhtasham/santacoder-finetuned-the-stack-cobol` | Pretrained model          | Base model for COBOL understanding |\n",
    "| **General Code**                | `codeparrot/github-code`                         | Multi-language            | Extra fine-tuning                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T05:39:05.776643Z",
     "iopub.status.busy": "2025-06-13T05:39:05.776382Z",
     "iopub.status.idle": "2025-06-13T05:39:05.780490Z",
     "shell.execute_reply": "2025-06-13T05:39:05.779709Z",
     "shell.execute_reply.started": "2025-06-13T05:39:05.776622Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Fsoft-AIC/MainframeBench\", split=\"train[:100]\")  # sample subset\n",
    "\n",
    "# # Dummy Python generation for demo (replace with real aligned translations)\n",
    "# dataset = dataset.map(lambda x: {\"tgt\": \"# Python translation of: \" + x[\"code\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T05:39:05.781486Z",
     "iopub.status.busy": "2025-06-13T05:39:05.781249Z",
     "iopub.status.idle": "2025-06-13T05:39:05.876056Z",
     "shell.execute_reply": "2025-06-13T05:39:05.875368Z",
     "shell.execute_reply.started": "2025-06-13T05:39:05.781466Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.038Z",
     "iopub.execute_input": "2025-06-13T05:39:05.877011Z",
     "iopub.status.busy": "2025-06-13T05:39:05.876817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aac7e8665ce4e29bf26314ca6801aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6982cb72aaa477a87b51918ca4c4dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "summary.csv:   0%|          | 0.00/3.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcd820dded14e79859206fb9702cff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333c61db063543ceab8d5f24fa5125e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/19.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d1fad65f234bfeb6c7f74586bc616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/8.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c771b92ce94ae7b75c23f655428bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf14fc2db124b20bbd93280be046bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a01171a62bc464bba3cbbdf9cf5e5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/206 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d2a757a80b4ea38769663cd0ff9210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c8f66af03f42ed90e09cbec627df5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15812d7325164cef9aea78cfb7f840b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68827a5a33b4564bf22ddef46ab58c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00206.parquet:   0%|          | 0.00/387M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234f2137f80e42e6972221b63f0f6bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0838a7a2c1aa455b9806d88ceec33fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00005-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b5c539180b4aaa8bfc167ecc1c17ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00006-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dd126fff664e73996a5f6c9aa888a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00007-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e26ec423974519826cc817f864f1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00008-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e135c85ccd4000a02b66206d556e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00009-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2270696d1eb4de2b29ae347bf07f5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00010-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a11a0e9874a0b91c9ab198bc1494c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00011-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fce37c95b94481a9ae23f7e131aa288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00012-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91714a82cef6403c91cd00fbf98936d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00013-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21d12d8c4914e8699515bf99ab62db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00014-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb94e122594b42c69da691a1057f1d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00015-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3aee9fb67f4b3b91038479707259cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00016-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418a0e58116f4ef1a20654e1b25f7580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00017-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0c0e6d077a4e6dbdeff500d463df4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00018-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069e97fb211a414187a54aea2ddefac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00019-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015a1170cd374705ac69a5651a9f17b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00020-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ba479b6fa8459d91ab22436c0b320a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00021-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d8b5c5006c41689d92146e8614f8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00022-of-00206.parquet:   0%|          | 0.00/387M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea98a700f0a4a09809953b2937bf61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00023-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c0bdcdc97d4ddaaa1f50e548526a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00024-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831bb59a44244d259a7e7b52665c355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00025-of-00206.parquet:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecace0dbb7047e1a4407ec879d560a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00026-of-00206.parquet:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf6b3986bae4d4f8d75535d3dbcb2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00027-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b91905c5e7468dae3814cdc11a66c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00028-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bbfaf9575c49f59a3cd491b16fffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00029-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea561c78c424c889892276a0c758da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00030-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a9e4991be64bc1ac9367acdc466bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00031-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446defa3398b4bf6a44d8af6390b66a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00032-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd59ede51542da80f56dea312854fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00033-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f25c6940ddd450ba0b995ed0a0080d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00034-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493676ce72c14e8eb77232ec52a6a8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00035-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0b3a1df22e4efa86caa94780447a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00036-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7021348d46449c9d8176a0763b5b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00037-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e0a2d859df4b6e8067c71aa3c5350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00038-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0e14d59ef8435b8508580d06d13cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00039-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe33f6b1c9a41f0a1d48b20f48743cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00040-of-00206.parquet:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b891a3d8c13464ab1e66fdb3657287c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00041-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb052f4d6e6410089308350ef375a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00042-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c1d5ec4e9f4e67b59b61a30e504a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00043-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a060edf5b3d45fbaed3478a59b46894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00044-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881f88e3ddce4ccfa49f8bd0db1f4977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00045-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44434819fad14ebda4dcdef54f580b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00046-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ffac76c0374b35a0c24be91ac2c5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00047-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aca8743a9940d88487a88c3afcff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00048-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a8ba49dad4eb1be0ece2193400d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00049-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2ffa3b7dfb4af3bb699625e621cce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00050-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e67e5caefc4bf88e7a2477802dc974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00051-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d542d9180e964d4a91a3903715a497a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00052-of-00206.parquet:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088ebbdcb76a46799d0805ffa4d11fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00053-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e8673e81894026a256a2b51afbb78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00054-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b99453179ad4b17b1bd2d49eb12d717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00055-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef07ac72548948098d8fb2dd2015d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00056-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c60d872ef93464eb5330412df9c02ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00057-of-00206.parquet:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370f3acef3849b68fd2c1d2f1b16e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00058-of-00206.parquet:   0%|          | 0.00/389M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c535f1581d5b4bfab751ebcb98363fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00059-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b369ebd73b54d3ab12e4c0f782573ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00060-of-00206.parquet:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13217935d87418ba544f12d736be99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00061-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815369f1fc94437d8145501eccd4bdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00062-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711ef755e8ba4fef85b19abe97417f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00063-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f1654a0b094ab8a55190ed836df9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00064-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d59805ce14546918986738ba52e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00065-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d5c00215a643d6b26d86d5f4bef693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00066-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41023c82c06e481d8cb2c29a8cdf4183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00067-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5c79072f3547fda80f286e1c342402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00068-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb2638f03f849d2b9afac608f88d6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00069-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa04767dacc4d3e9e9b933786b99f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00070-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844ec53329ea496589aad13eedd204d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00071-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fad5bb0c2f74b2096d6ca2e294edb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00072-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2970eed85f40feb1004e52172b0cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00073-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71df907106844c0899a07da4e0c5fd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00074-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c36d9b84cf49a59b84f9b3d678f48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00075-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc4772fe74046baaeb350efc11b1ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00076-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f81001654463aaa4ea120f9cc781e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00077-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fb6323cfdc446f90b25244452f6ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00078-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22761a40af5403c9863dc3ab16cc93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00079-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452eb204473b44f6890921a38571f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00080-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ebc76d0c8741708fa14d1c982693d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00081-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af94cf07a0da49e3ad9f3b4dfd61e4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00082-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eb6d2d119642ce8c6fdfcda1e64289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00083-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6d6ac799e44c4b8c7972be61c86186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00084-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ee598569bd4a7db24f53d6cb104129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00085-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54361caa68d44cb6908acf57df669f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00086-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34f7c99353346ebb238f3d47241a4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00087-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6544010937cd401aabfe4b4f42c67984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00088-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77f6812b61347918cb7c45e9f56720e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00089-of-00206.parquet:   0%|          | 0.00/387M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a540034218431fbf5f812cbc22ed03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00090-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da88e8b1150547a281d2644305736b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00091-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698ea766330347578541e97b141838d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00092-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4f3b0a7f874af6a558dbbaaa38f8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00093-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a877bc3102a847b89a07e315ba81c34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00094-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff42203ef616436881c565f1b6236dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00095-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97386e2f27b4cd5913b61a77e360e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00096-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b0c140c56b46d5ad0511164db1b5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00097-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428c16e1e01a491ca0910b3da664b8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00098-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2106d477984d399b7a01e24a31fc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00099-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5a0506e1e6486ead2515ecba2fc94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00100-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497694a34c72477bbeec6fbec338b20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00101-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06d083a5b2b422fbd826ad21b7575f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00102-of-00206.parquet:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c651d354e13a4fc9b4fb476f140b4f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00103-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfd0f862be943acadabc801bd73b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00104-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684e74e07d674b8d9960b72a845c48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00105-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407733df3e2f4a66858daa34b99a65b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00106-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83791b2a4bd94759b197e124b71b997e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00107-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e69974e79a34abeae7cb69ae4dce27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00108-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49819cbba3b4a1491b8807ce4116b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00109-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7bb4e7646480dafa9d960301aa0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00110-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8c160b0941410fa38b268036434c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00111-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7441a8fea98949f6ae45e2f832c97479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00112-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb105aecbcfa45bfb7b8b874f123294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00113-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7650d2f66e74cf89283d5425cb741de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00114-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6495a9104144d00bae749538d00bcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00115-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db1a635288142b6a1b29729bdb717e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00116-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328b0e3b802c4196a599bc2d08f9e976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00117-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aafafe804f44d89d6528c193c0340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00118-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c9e2b10082430992e57a55468320e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00119-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7b768ec2dc4a88a851810d0c2125dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00120-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722b46782c7248a284a345901797155b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00121-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549c3fba34ce4292883b0e2056a9a8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00122-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b3d283763344958e330d3b4eda82c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00123-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1589dd22d7604d3bb59123c6e2080193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00124-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80a01f6c6484091ae21ceaecc5d4f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00125-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3a37a3020f4c5f9d1a52fa45d20563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00126-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c634c10f03479ca2dd63cc3da9581c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00127-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d04022d2b594a0a81ad7de12fb67d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00128-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb48532b9834de5b39c9f45458d0c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00129-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29949a08fb35485b9bf7ef76fb79d8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00130-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17dff29acf44f07860b13d2968d282f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00131-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea83962bf5bc4069a38f77dc02927ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00132-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d693e1b674469d82a47cdbe4f4304c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00133-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5aa7a633614ae797ede715129cd6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00134-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7465520df34fc09c3c6d33a7a03f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00135-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea6471521e4ce4a029ad08f23f24b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00136-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce4ab0a418548cd8cec9f7a1abf1da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00137-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d2be54beb04499bc507d4db74071d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00138-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b277f8018d4c2b9c9e96371604d785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00139-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb7dae5bd2148ba8bd2e3984e24bbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00140-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dfd104cfb64147a751bb7dcb11ac71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00141-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580fdfb3b0214f11b78737ce299e5306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00142-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7297ed426ca466aafb1bda21595cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00143-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977e9a66a105483ba4677525b216ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00144-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09bfec12fbc42b885d8097b143aa5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00145-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c6d8c4b5a44c183867d2d485b1926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00146-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd761f05b9a4e5e95279e011295cf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00147-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397533760f694727810e3f1e6993a827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00148-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dc76a6a025444d89ab3574df1dee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00149-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e627bd0bd840cd9951b213f3e7267c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00150-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13666160a92e4df68b193d69f22d0149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00151-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845c8d8475354cd5a631792bb680dffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00152-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37d7a429c8c4c229b47e55b79535b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00153-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1861bbf1d504a1cac2bd55ea93d3fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00154-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd69ab254cd4d39a69f2e4177abccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00155-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328b998b8e90489c873c3b68171fcf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00156-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840f62b1a00a41c7ae25ea1725b48091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00157-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc0bb904bed42f3a31ea1710dab4b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00158-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e540c8d40a445ea84cd0891e6e7371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00159-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36250b062be641c7b279ba77e6bcc1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00160-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bfb8d3215142a3a57feef3547c439f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00161-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0aa6a5131947eeb253c78fa9a5dfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00162-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8699aa7a0bd649ed8910fa83c9382cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00163-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77d0ed49e3344909acae0f9002cbb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00164-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819b07d4920e490bb1bccd06ee4dad0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00165-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6314d26de5ac4ab187451ad0b64f7204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00166-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95f3b72d56f4482b9240e61fff65572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00167-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e10fa1f35846e085623da12aeb2c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00168-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc8b63d27d649ad8dc830724529c903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00169-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede6b1121d940ec8af36aacead15140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00170-of-00206.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27941575071d48eaa23743fb78a0c18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00171-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48641c1845484a6680f98408bec82ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00172-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38edaf7372645448648e68a0fbf87ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00173-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a729f2322e4aa698dac86aefb788e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00174-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f99e7c528f474da18c0014d438aef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00175-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421fc6f3cb934959ade13a94500dba20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00176-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f93fb762d54133b73fd258c9450af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00177-of-00206.parquet:   0%|          | 0.00/388M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856d9cd79b534deca0e2f4b8ff8dcb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00178-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c19dae55804648bf9f27672ec581fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00179-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7d566042ff4c379d11fd2a814a7aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00180-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7990a872d2674798ba7b5556cb5d901d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00181-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e8ec71237b4181878291776ae35d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00182-of-00206.parquet:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df2d21da94465cad2189566f28e86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00183-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d05ac182fa4aa7a23a09ca94dbd8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00184-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ca19310f5d4962bdf9523aadd3235b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00185-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b858a783f7cc4e90b4f21c3d215dec69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00186-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9ea594738144ef9f263b4d5cbb4de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00187-of-00206.parquet:   0%|          | 0.00/384M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10e59a865904577810e0f127255f2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00188-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3815130a794b8a8def13c784bb3bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00189-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff415504ed42da869c0490b893dbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00190-of-00206.parquet:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb68003cc2c40e8869808bbf02e9bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00191-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987be716220d4d0c8ba81daf7f73b2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00192-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3ab3d981554563a560798e32278377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00193-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117f032b9b5e4131be56e8c3bb7be82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00194-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f8f1adace54dfe85ec69d6e80da069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00195-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2526c942b34447b48a609c3a004dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00196-of-00206.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050d5f793ea45c2a76ad38a545fa214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00197-of-00206.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb158dbb35a4f7eb3d4856ea6580934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00198-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2ad4c2fd8149cca1596e73a3deee35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00199-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde4546863334801b454150d9faf81d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00200-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d6462506964626ade7216d064da54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00201-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b31cb71a5e4c91b9b61db739932d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00202-of-00206.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a641ef79a6490c87bb7cff08653123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00203-of-00206.parquet:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d7112142a04c60a0e4f0be7ae5e05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00204-of-00206.parquet:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf074bc5e2b4441abc2b8bd7aa78e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00205-of-00206.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a416d8cfe4a423482a8b4e0263c813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Load relevant subsets\n",
    "mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n",
    "# mainframe_small = mainframe.shuffle(seed=42).select(range(1000)) # for kaggle\n",
    "\n",
    "# full dataset (3TB of data)\n",
    "# ds = load_dataset(\"bigcode/the-stack\", split=\"train\")\n",
    "\n",
    "# specific language (e.g. Dockerfiles)\n",
    "stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:20%]\")\n",
    "stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:20%]\")\n",
    "\n",
    "python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "\n",
    "# Combine relevant Python corpora\n",
    "python_combined = concatenate_datasets([stack_python, python_set])\n",
    "\n",
    "# Now build translation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format MainframeBench COBOL data\n",
    "def format_mainframe(example):\n",
    "    return {\n",
    "        \"input\": example[\"cobol_code\"],\n",
    "        \"output\": \"# Python translation to be generated or is unknown for now.\",\n",
    "    }\n",
    "\n",
    "mainframe_formatted = mainframe.map(format_mainframe)\n",
    "\n",
    "# Build real COBOL â†’ Python pairs from The Stack\n",
    "paired_data = []\n",
    "for i in range(min(len(stack_cobol), len(python_combined))):\n",
    "    paired_data.append({\n",
    "        \"input\": stack_cobol[i][\"content\"],\n",
    "        \"output\": python_combined[i][\"content\"]\n",
    "    })\n",
    "\n",
    "# Combine both: real pairs + placeholder Mainframe data\n",
    "combined_data = Dataset.from_list(paired_data + list(mainframe_formatted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "translation_dataset = combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split 80% train, 10% eval, 10% test\n",
    "splits = translation_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "eval_test = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"eval\": eval_test[\"train\"],\n",
    "    \"test\": eval_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# # we can upload custom dataset like following\n",
    "# {\"src\": \"COBOL code here\", \"tgt\": \"Equivalent Python code here\"}\n",
    "# {\"src\": \"Another COBOL snippet\", \"tgt\": \"Translated Python code\"}\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": \"cobol_python_dataset.jsonl\"})[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{example['src']}\n",
    "\n",
    "### Response:\n",
    "{example['tgt']}\"\"\",\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# optional to save dataset locally\n",
    "tokenized_dataset.to_json(\"codellama_cobol2python_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Random split: 80% train, 10% eval, 10% test\n",
    "splits = translation_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "eval_test = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"eval\": eval_test[\"train\"],\n",
    "    \"test\": eval_test[\"test\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the DataFrame\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "\n",
    "# Calculate sizes\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(dataset_dict))\n",
    "\n",
    "# Split the data\n",
    "X_train = dataset_dict[:train_end]\n",
    "X_eval = dataset_dict[train_end:eval_end]\n",
    "X_test = dataset_dict[eval_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the prompt generation functions for COBOL â†’ Python conversion\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{data_point[\"src\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"tgt\"]}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{data_point[\"src\"]}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate prompts for training and evaluation data\n",
    "X_train = X_train.assign(text=X_train.apply(generate_prompt, axis=1))\n",
    "X_eval = X_eval.assign(text=X_eval.apply(generate_prompt, axis=1))\n",
    "\n",
    "# Generate test prompts and extract true labels\n",
    "y_true = X_test['status'].copy()\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to datasets\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Set env var to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "base_model_name = local_dir  # your snapshot_download path\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,  # helps reduce memory\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model in 4-bit with automatic device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map={\"\": 0},\n",
    "    max_memory={0: \"13GiB\"}, # for kaggle space constraint\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Memory optimizations\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()  # saves memory during training\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create pipeline outside the loop for efficiency\n",
    "def predict_code_translation(test_df, model, tokenizer):\n",
    "    predictions = []\n",
    "\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(test_df))):\n",
    "        cobol_code = test_df.iloc[i][\"src\"]\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "        result = pipe(prompt)\n",
    "        generated_code = result[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "        predictions.append(generated_code)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Usage:\n",
    "y_pred = predict_code_translation(X_test, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_code_translation(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred), \"Mismatch in number of samples\"\n",
    "    \n",
    "    def code_similarity(a, b):\n",
    "        return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n",
    "    \n",
    "    similarities = [code_similarity(gt, pred) for gt, pred in zip(y_true, y_pred)]\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    \n",
    "    exact_matches = sum(1 for gt, pred in zip(y_true, y_pred) if gt.strip() == pred.strip())\n",
    "    accuracy = exact_matches / len(y_true)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Average Similarity Score: {avg_similarity:.3f}\")\n",
    "    \n",
    "    # Optionally, show some examples\n",
    "    for i in range(min(3, len(y_true))):\n",
    "        print(\"\\n--- Sample\", i+1)\n",
    "        print(\"True Output:\\n\", y_true[i])\n",
    "        print(\"Predicted Output:\\n\", y_pred[i])\n",
    "        print(\"Similarity Score:\", code_similarity(y_true[i], y_pred[i]))\n",
    "\n",
    "# Example usage:\n",
    "evaluate_code_translation(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Avoid CUDA memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "output_dir = \"CodeLlama-7B-Instruct-fine-tuned-model\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules  # You must define `modules`, e.g., [\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,     # Path to save checkpoints\n",
    "    per_device_train_batch_size=2,             # Small batch for large models\n",
    "    gradient_accumulation_steps=4,             # Effective batch size = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    bf16=True,                                 # If available, use bf16\n",
    "    fp16=False,                                # Disable fp16 to avoid conflict with bf16\n",
    "\n",
    "    gradient_checkpointing=True,               # Save memory\n",
    "    optim=\"paged_adamw_32bit\",                 # Efficient optimizer\n",
    "    max_grad_norm=0.3,                         # Gradient clipping\n",
    "\n",
    "    lr_scheduler_type=\"cosine\",                # Cosine annealing\n",
    "    warmup_ratio=0.03,                         # Warmup steps\n",
    "\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "\n",
    "    report_to=\"none\",                          # Avoid wandb/logging integrations\n",
    "    disable_tqdm=False                         # Show progress bars\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=128,       # smaller = less memory\n",
    "    # tokenizer=tokenizer,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # tokenizer=tokenizer,\n",
    "    # max_seq_length=128,\n",
    "    # packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# To start the fine-tuning process:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load your Hugging Face token from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = base_model_name\n",
    "fine_tuned_model = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "cobol_code = \"\"\"\n",
    "       IDENTIFICATION DIVISION.\n",
    "       PROGRAM-ID. HELLO.\n",
    "       PROCEDURE DIVISION.\n",
    "           DISPLAY 'HELLO, WORLD'.\n",
    "           STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# folder_path = \"/kaggle/working/llama-3.2-fine-tuned-model\"\n",
    "\n",
    "# if os.path.exists(folder_path):\n",
    "#     shutil.rmtree(folder_path)\n",
    "#     print(\"Folder removed.\")\n",
    "# else:\n",
    "#     print(\"Folder does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import CommitInfo\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load your Hugging Face token from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load base model with the same architecture\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "\n",
    "# Merge the adapter into the base model weights (final full model)\n",
    "model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# save and resgister to huggingface\n",
    "model.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n",
    "tokenizer.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dhirajpatra/\" + model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dhirajpatra/\" + model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.041Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"dhirajpatra/\" + model_dir\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.041Z"
    }
   },
   "outputs": [],
   "source": [
    "cobol_code = \"\"\"\n",
    "       IDENTIFICATION DIVISION.\n",
    "       PROGRAM-ID. HELLO.\n",
    "       PROCEDURE DIVISION.\n",
    "           DISPLAY 'HELLO, WORLD'.\n",
    "           STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.041Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "bleu = load_metric(\"bleu\")\n",
    "results = bleu.compute(predictions=preds, references=refs)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
