{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462d430-8e6c-42a2-a9bc-1a1dc00e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch (CUDA 12.1 for RTX 4090 - better than 11.8)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Transformers ecosystem\n",
    "pip install transformers>=4.36.0\n",
    "pip install datasets\n",
    "pip install peft>=0.7.0\n",
    "pip install trl\n",
    "pip install accelerate\n",
    "\n",
    "# Memory optimization\n",
    "pip install bitsandbytes  # Still useful even without quantization\n",
    "pip install flash-attn --no-build-isolation  # Critical for performance\n",
    "\n",
    "# Monitoring\n",
    "pip install wandb\n",
    "pip install pynvml\n",
    "\n",
    "# For TensorRT inference optimization\n",
    "pip install tensorrt\n",
    "\n",
    "# For better performance monitoring\n",
    "pip install gpustat\n",
    "pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5c33e-806c-4e9a-90b9-e246f72c9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Fine-tuning Pipeline for Meta Llama 3.2 8B\n",
    "Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import tensorrt as trt\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d57822-9727-41b8-bdaa-907319a7c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA optimizations\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257667e-6fb0-4c73-b95a-fd674112d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedLlamaFineTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_nvidia_environment()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        self.is_multi_gpu = torch.cuda.device_count() > 1\n",
    "        \n",
    "    def setup_nvidia_environment(self):\n",
    "        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available!\")\n",
    "            \n",
    "        # Initialize NVML for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        \n",
    "        print(f\"ðŸš€ NVIDIA Setup:\")\n",
    "        print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   - Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode()\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n",
    "            \n",
    "        # Enable TensorFloat-32 for A100/RTX 30xx series\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"   - TensorFloat-32 enabled for Ampere+ GPUs\")\n",
    "            \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load Llama 3.1 8B with optimized settings for dual RTX 4090s\"\"\"\n",
    "        model_name = self.config.get(\"model_name\", \"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "        \n",
    "        # With 48GB VRAM, we can afford full precision for 8B model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "        ) if self.config.get(\"use_quantization\", False) else None\n",
    "        \n",
    "        print(f\"ðŸ“¥ Loading tokenizer for {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"ðŸ“¥ Loading {model_name} optimized for dual RTX 4090s...\")\n",
    "        \n",
    "        # Load 8B model without quantization for maximum quality\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\" if self.is_multi_gpu else None,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,  # Best precision for Ada Lovelace\n",
    "            attn_implementation=\"flash_attention_2\" if self.config.get(\"use_flash_attention\", True) else \"eager\",\n",
    "            use_cache=False,  # Disable for training\n",
    "        )\n",
    "        \n",
    "        # Prepare model for training (only if using quantization)\n",
    "        if bnb_config:\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        print(f\"âœ… Model loaded. Multi-GPU: {self.is_multi_gpu}\")\n",
    "        if self.is_multi_gpu:\n",
    "            print(f\"   Model distributed across {torch.cuda.device_count()} GPUs\")\n",
    "        \n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA configuration for efficient fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get(\"lora_r\", 16),\n",
    "            lora_alpha=self.config.get(\"lora_alpha\", 32),\n",
    "            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"ðŸŽ¯ LoRA Configuration:\")\n",
    "        print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   - Total parameters: {total_params:,}\")\n",
    "        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "        \n",
    "    # def load_and_prepare_dataset(self):\n",
    "    #     \"\"\"Load and prepare dataset for training\"\"\"\n",
    "    #     dataset_name = self.config.get(\"dataset_name\", \"tatsu-lab/alpaca\")\n",
    "        \n",
    "    #     print(f\"ðŸ“Š Loading dataset: {dataset_name}\")\n",
    "        \n",
    "    #     # Load dataset\n",
    "    #     if isinstance(dataset_name, str):\n",
    "    #         dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    #     else:\n",
    "    #         # Handle custom dataset loading\n",
    "    #         dataset = dataset_name\n",
    "            \n",
    "    #     # Prepare dataset based on format\n",
    "    #     if \"instruction\" in dataset.column_names and \"output\" in dataset.column_names:\n",
    "    #         # Alpaca format\n",
    "    #         def format_alpaca(examples):\n",
    "    #             texts = []\n",
    "    #             for instruction, input_text, output in zip(\n",
    "    #                 examples[\"instruction\"], \n",
    "    #                 examples.get(\"input\", [\"\"] * len(examples[\"instruction\"])), \n",
    "    #                 examples[\"output\"]\n",
    "    #             ):\n",
    "    #                 if input_text:\n",
    "    #                     prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "    #                 else:\n",
    "    #                     prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    #                 texts.append(prompt)\n",
    "    #             return {\"text\": texts}\n",
    "                \n",
    "    #         dataset = dataset.map(format_alpaca, batched=True, remove_columns=dataset.column_names)\n",
    "            \n",
    "    #     elif \"text\" not in dataset.column_names:\n",
    "    #         raise ValueError(\"Dataset must have 'text' column or Alpaca format (instruction/output)\")\n",
    "            \n",
    "    #     # Filter by length to avoid OOM\n",
    "    #     max_length = self.config.get(\"max_length\", 2048)\n",
    "    #     dataset = dataset.filter(lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length)\n",
    "        \n",
    "    #     # Take subset if specified\n",
    "    #     if self.config.get(\"max_samples\"):\n",
    "    #         dataset = dataset.select(range(min(len(dataset), self.config[\"max_samples\"])))\n",
    "            \n",
    "    #     self.dataset = dataset\n",
    "    #     print(f\"âœ… Dataset prepared: {len(dataset)} samples\")\n",
    "\n",
    "    def load_and_prepare_dataset(self):\n",
    "        \"\"\"Load and prepare dataset for training\"\"\"\n",
    "        dataset_name = self.config.get(\"dataset_name\", \"tatsu-lab/alpaca\")\n",
    "        # Add dataset_config to handle configurations like \"ehr_rel_bigbio_pairs\"\n",
    "        dataset_config_name = self.config.get(\"dataset_config_name\", None) \n",
    "        \n",
    "        print(f\"ðŸ“Š Loading dataset: {dataset_name} (config: {dataset_config_name if dataset_config_name else 'None'})\")\n",
    "        \n",
    "        # Load dataset\n",
    "        if isinstance(dataset_name, str):\n",
    "            # Pass dataset_config_name if provided\n",
    "            if dataset_config_name:\n",
    "                dataset = load_dataset(dataset_name, dataset_config_name, split=\"train\")\n",
    "            else:\n",
    "                dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        else:\n",
    "            # Handle custom dataset loading (if dataset_name is already a loaded Dataset object)\n",
    "            dataset = dataset_name\n",
    "            \n",
    "        print(f\"Dataset columns before preparation: {dataset.column_names}\")\n",
    "\n",
    "        # --- Prepare dataset based on format ---\n",
    "        # Prioritize 'text' column if it exists directly\n",
    "        if \"text\" in dataset.column_names:\n",
    "            print(\"Dataset has a 'text' column. Using it directly.\")\n",
    "            # No specific formatting needed, it's already in 'text'\n",
    "            pass \n",
    "        elif \"instruction\" in dataset.column_names and \"output\" in dataset.column_names:\n",
    "            # Alpaca format\n",
    "            print(\"Dataset detected as Alpaca format.\")\n",
    "            def format_alpaca(examples):\n",
    "                texts = []\n",
    "                for instruction, input_text, output in zip(\n",
    "                    examples[\"instruction\"], \n",
    "                    examples.get(\"input\", [\"\"] * len(examples[\"instruction\"])), \n",
    "                    examples[\"output\"]\n",
    "                ):\n",
    "                    if input_text:\n",
    "                        # Ensure proper newline characters and strip whitespace\n",
    "                        prompt = f\"### Instruction:\\n{instruction.strip()}\\n\\n### Input:\\n{input_text.strip()}\\n\\n### Response:\\n{output.strip()}\"\n",
    "                    else:\n",
    "                        prompt = f\"### Instruction:\\n{instruction.strip()}\\n\\n### Response:\\n{output.strip()}\"\n",
    "                    texts.append(prompt)\n",
    "                return {\"text\": texts}\n",
    "                \n",
    "            dataset = dataset.map(format_alpaca, batched=True, remove_columns=dataset.column_names)\n",
    "            \n",
    "        elif all(col in dataset.column_names for col in [\"concept1\", \"concept2\", \"relation_type\"]):\n",
    "            # Specific handling for 'bigbio/ehr_rel' or similar relation extraction datasets\n",
    "            print(\"Dataset detected as 'bigbio/ehr_rel' or similar relation extraction format.\")\n",
    "            def format_ehr_rel(examples):\n",
    "                texts = []\n",
    "                for c1, c2, rel_type in zip(\n",
    "                    examples[\"concept1\"], \n",
    "                    examples[\"concept2\"], \n",
    "                    examples[\"relation_type\"]\n",
    "                ):\n",
    "                    # Define how to represent the relation as text for your LLM.\n",
    "                    # This example creates a descriptive sentence.\n",
    "                    text_representation = f\"The medical concept '{c1.strip()}' has a '{rel_type.strip()}' relationship with '{c2.strip()}'.\"\n",
    "                    texts.append(text_representation)\n",
    "                return {\"text\": texts}\n",
    "\n",
    "            dataset = dataset.map(format_ehr_rel, batched=True, remove_columns=dataset.column_names)\n",
    "            \n",
    "        else:\n",
    "            # Fallback for other formats: you'll need to define how to convert them to a 'text' column.\n",
    "            # Or raise an error if no known format is matched.\n",
    "            raise ValueError(\n",
    "                \"Dataset format not recognized. \"\n",
    "                \"It must have a 'text' column, Alpaca format (instruction/output), \"\n",
    "                \"or a specifically handled format like 'bigbio/ehr_rel'.\"\n",
    "                f\"Available columns: {dataset.column_names}\"\n",
    "            )\n",
    "            \n",
    "        print(f\"Dataset columns after preparation: {dataset.column_names}\")\n",
    "\n",
    "        # Filter by length to avoid OOM\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        # Ensure 'text' column exists before encoding and filtering\n",
    "        if \"text\" in dataset.column_names:\n",
    "            dataset = dataset.filter(lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length)\n",
    "        else:\n",
    "            # This case should ideally not happen if the above logic is correct,\n",
    "            # but it's a safeguard.\n",
    "            print(\"Warning: 'text' column not found after preparation. Skipping length filter.\")\n",
    "        \n",
    "        # Take subset if specified\n",
    "        if self.config.get(\"max_samples\"):\n",
    "            dataset = dataset.select(range(min(len(dataset), self.config[\"max_samples\"])))\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        print(f\"âœ… Dataset prepared: {len(dataset)} samples\")\n",
    "        \n",
    "    def setup_training_arguments(self):\n",
    "        \"\"\"Setup optimized training arguments for dual RTX 4090s\"\"\"\n",
    "        # Calculate optimal batch size for dual RTX 4090s (48GB total VRAM)\n",
    "        base_batch_size = self.config.get(\"batch_size\", 16)  # Increased for high VRAM\n",
    "        per_device_batch_size = base_batch_size // max(1, torch.cuda.device_count())\n",
    "        \n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.get(\"output_dir\", \"./llama-3.2-8b-finetuned\"),\n",
    "            \n",
    "            # Training hyperparameters optimized for RTX 4090s\n",
    "            num_train_epochs=self.config.get(\"num_epochs\", 3),\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 2),  # Reduced due to higher batch size\n",
    "            learning_rate=self.config.get(\"learning_rate\", 2e-4),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            weight_decay=0.01,\n",
    "            \n",
    "            # RTX 4090 Ada Lovelace optimizations\n",
    "            bf16=True,  # Native bfloat16 support on Ada Lovelace\n",
    "            tf32=True,  # Enable TensorFloat-32 for maximum performance\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=8,  # Higher due to powerful CPU\n",
    "            \n",
    "            # Memory optimizations (less aggressive due to abundant VRAM)\n",
    "            gradient_checkpointing=self.config.get(\"gradient_checkpointing\", False),  # Optional with 48GB\n",
    "            optim=\"adamw_torch_fused\",  # Fused optimizer for NVIDIA Ada Lovelace\n",
    "            max_grad_norm=1.0,\n",
    "            \n",
    "            # Multi-GPU settings\n",
    "            ddp_backend=\"nccl\" if self.is_multi_gpu else None,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            \n",
    "            # Logging and saving\n",
    "            logging_steps=5,  # More frequent logging for 12-hour testing\n",
    "            save_steps=250,   # More frequent saves\n",
    "            save_total_limit=5,\n",
    "            evaluation_strategy=\"no\",\n",
    "            \n",
    "            # Performance monitoring\n",
    "            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n",
    "            run_name=f\"llama-3.2-8b-dual-4090-{self.config.get('experiment_name', 'default')}\",\n",
    "            \n",
    "            # Additional Ada Lovelace optimizations\n",
    "            remove_unused_columns=False,\n",
    "            prediction_loss_only=True,\n",
    "            disable_tqdm=False,\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Execute the fine-tuning process\"\"\"\n",
    "        print(\"ðŸš€ Starting fine-tuning process...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get(\"use_wandb\", False):\n",
    "            wandb.init(\n",
    "                project=self.config.get(\"wandb_project\", \"llama-finetune\"),\n",
    "                name=f\"llama-3.2-8b-{self.config.get('experiment_name', 'default')}\"\n",
    "            )\n",
    "            \n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_arguments()\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize SFTTrainer (optimized for instruction tuning)\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.config.get(\"max_length\", 2048),\n",
    "            packing=False,  # Disable packing to avoid issues with instruction format\n",
    "        )\n",
    "        \n",
    "        # Clear cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Start training\n",
    "        print(\"ðŸŽ¯ Training started...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"ðŸ’¾ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(\"âœ… Fine-tuning completed!\")\n",
    "        \n",
    "    def optimize_for_inference(self, model_path):\n",
    "        \"\"\"Optimize model for inference using TensorRT (optional)\"\"\"\n",
    "        print(\"âš¡ Optimizing model for inference...\")\n",
    "        \n",
    "        # Load the fine-tuned model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "        model.eval()\n",
    "        \n",
    "        # Convert to TorchScript\n",
    "        traced_model = torch.jit.trace(model, example_inputs=(torch.randint(0, 1000, (1, 512)),))\n",
    "        \n",
    "        # Save optimized model\n",
    "        optimized_path = f\"{model_path}_optimized\"\n",
    "        traced_model.save(f\"{optimized_path}/traced_model.pt\")\n",
    "        \n",
    "        print(f\"âœ… Optimized model saved to: {optimized_path}\")\n",
    "        \n",
    "    def monitor_gpu_usage(self):\n",
    "        \"\"\"Monitor GPU usage during training\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c84b8d-4329-4fac-b71f-37119ae5bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Best Overall Performance\n",
    "LLAMA_8B_DOLLY_CONFIG = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"dataset_name\": \"databricks/databricks-dolly-15k\",\n",
    "    \"dataset_name\": \"bigbio/ehr_rel\", # hugging face dataset\n",
    "    \"experiment_name\": \"llama_8b_dolly_premium\",\n",
    "    \n",
    "    # Training settings optimized for dual RTX 4090s\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 24,  # Higher for 8B model\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 5e-5,  # Lower for larger model\n",
    "    \"max_length\": 2048,\n",
    "    \n",
    "    # LoRA settings for 8B model\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Optimizations\n",
    "    \"use_quantization\": False,\n",
    "    \"use_flash_attention\": True,\n",
    "    \"gradient_checkpointing\": True,  # Enable for 8B model\n",
    "    \n",
    "    # Expected time: ~4-5 hours\n",
    "    \"output_dir\": \"./llama-8b-dolly-finetuned\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718e1ff-b77e-4917-a5b9-2622ce4d6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: Fastest Training (Good for testing)\n",
    "LLAMA_3B_DOLLY_CONFIG = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"dataset_name\": \"databricks/databricks-dolly-15k\",\n",
    "    \"experiment_name\": \"llama_3b_dolly_fast\",\n",
    "    \n",
    "    # Aggressive settings for fast training\n",
    "    \"num_epochs\": 5,  # More epochs since it's faster\n",
    "    \"batch_size\": 32,  # Very high batch size\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_length\": 2048,\n",
    "    \n",
    "    # Higher LoRA rank for 3B model\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Optimizations\n",
    "    \"use_quantization\": False,\n",
    "    \"use_flash_attention\": True,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \n",
    "    # Expected time: ~2-3 hours\n",
    "    \"output_dir\": \"./llama-3b-dolly-finetuned\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a9491-f4ce-47e4-bc45-e841df48f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: Code-Focused Fine-tuning\n",
    "LLAMA_8B_CODE_CONFIG = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"dataset_name\": \"sahil2801/CodeAlpaca-20k\",\n",
    "    \"experiment_name\": \"llama_8b_code_specialist\",\n",
    "    \n",
    "    # Code-specific settings\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"max_length\": 4096,  # Longer context for code\n",
    "    \n",
    "    # LoRA for code tasks\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \n",
    "    # Optimizations\n",
    "    \"use_quantization\": False,\n",
    "    \"use_flash_attention\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \n",
    "    # Expected time: ~3-4 hours\n",
    "    \"output_dir\": \"./llama-8b-code-finetuned\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf0511-7a5a-4b45-8ef0-770ea67fc2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(config=LLAMA_8B_DOLLY_CONFIG):\n",
    "    config = config\n",
    "\n",
    "    print(\"ðŸš€ Dual RTX 4090 Fine-tuning Configuration (FREE VERSION):\")\n",
    "    print(f\"   - Model: Llama 3.1 8B Instruct (Latest)\")\n",
    "    print(f\"   - Dataset: Databricks Dolly 15K (Human-written)\")\n",
    "    print(f\"   - Total VRAM: 48GB\")\n",
    "    print(f\"   - Batch size: {config['batch_size']}\")\n",
    "    print(f\"   - Context length: {config['max_length']}\")\n",
    "    print(f\"   - LoRA rank: {config['lora_r']}\")\n",
    "    print(f\"   - Quantization: {'Disabled' if not config['use_quantization'] else 'Enabled'}\")\n",
    "    print(f\"   - Monitoring: Local only (no external services)\")\n",
    "    print(f\"   - Expected training time: 4-5 hours\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0728eec-59d5-4540-a9b9-66f175a6a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize fine-tuner\n",
    "    fine_tuner = OptimizedLlamaFineTuner(config)\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        fine_tuner.load_model_and_tokenizer()\n",
    "        \n",
    "        # Setup LoRA\n",
    "        fine_tuner.setup_lora()\n",
    "        \n",
    "        # Load and prepare dataset\n",
    "        fine_tuner.load_and_prepare_dataset()\n",
    "        \n",
    "        # Monitor initial GPU usage\n",
    "        print(\"ðŸ“Š Initial GPU Memory Usage:\")\n",
    "        fine_tuner.monitor_gpu_usage()\n",
    "        print()\n",
    "        \n",
    "        # Start training\n",
    "        fine_tuner.train()\n",
    "        \n",
    "        # Final GPU usage\n",
    "        print(\"ðŸ“Š Final GPU Memory Usage:\")\n",
    "        fine_tuner.monitor_gpu_usage()\n",
    "        \n",
    "        # Optional: Optimize for inference\n",
    "        # fine_tuner.optimize_for_inference(config[\"output_dir\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during fine-tuning: {e}\")\n",
    "        # Print GPU memory for debugging\n",
    "        fine_tuner.monitor_gpu_usage()\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750d613-dc5b-40e4-9f5f-1cfbaff0415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training function optimized for dual RTX 4090s\"\"\"\n",
    "    set_config()\n",
    "    train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b9bde-24bc-4b8d-970b-77f4c6af0bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921dd784-4d04-4f30-a4ac-636da73616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     print(\"ðŸš€ Dual RTX 4090 Testing Configurations\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     strategy = get_testing_strategy()\n",
    "#     for phase, details in strategy.items():\n",
    "#         print(f\"\\n{phase.upper()}:\")\n",
    "#         print(f\"  Duration: {details['duration']}\")\n",
    "#         print(f\"  Purpose: {details['purpose']}\")\n",
    "#         if 'config' in details:\n",
    "#             config = details['config']\n",
    "#             print(f\"  Model: {config['model_name']}\")\n",
    "#             print(f\"  Dataset: {config['dataset_name']}\")\n",
    "#             print(f\"  Expected VRAM: {EXPECTED_METRICS.get(config['model_name'].split('/')[-1].lower().replace('-', '_').split('_')[1] + 'b', {}).get('vram_usage', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
