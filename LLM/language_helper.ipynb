{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786c5e6b-c085-4df9-8c5c-426f2a9bed71",
   "metadata": {},
   "source": [
    "## Engish Langaguge Model for Grammer Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3466a916-db0f-4a7c-b474-64d506296cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain_nvidia_ai_endpoints\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de0a43ee-ecc5-48db-b56f-be71fe0466b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import re\n",
    "from typing import List, Union\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import textwrap\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Include the NVIDIA API key\n",
    "load_dotenv()\n",
    "nvidia_api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# client = openai.OpenAI(\n",
    "#   base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "#   api_key = nvidia_api_key\n",
    "# )\n",
    "# print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fefce83-05bf-41f8-9481-26772ac406fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"microsoft/phi-3-mini-128k-instruct\"\n",
    "# llm = ChatNVIDIA(model=model_name, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf3f9cde-474d-4136-90b0-0666345d5526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.1,  # Control the randomness of the predictions\n",
    "    \"max_length\": 1000   # Set the maximum length for the generated sequences\n",
    "}\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=250,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49003320-2769-47b6-b7f9-f3b461b5a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model kwargs with desired parameters\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.1,  # Control the randomness of the predictions\n",
    "    \"max_length\": 1000   # Set the maximum length for the generated sequences\n",
    "}\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe, model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b1b1e60-a385-4d58-aa13-714201fb877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Please correct my sentence 'I could had been a engineering student'?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e9345b5-d109-4921-acd9-d02107ec4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is grammer and how can I learn it properly?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13b1c178-47de-434b-93e1-abbb38fb9ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhira\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League's two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints. USHL completes expansion draft\"), Document(page_content='Major League Baseball Commissioner Bud Selig will be speaking at St. Norbert College next month. Bud Selig to speak at St. Norbert College')]\n",
      "\n",
      "Total chunks: 180799\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFaceEmbeddings\n",
    "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Define embedding path\n",
    "embedding_path = \"embed/\"\n",
    "\n",
    "# Load the community-datasets/sentence-compression dataset from Hugging Face\n",
    "dataset = load_dataset(\"embedding-data/sentence-compression\")\n",
    "\n",
    "documents = []\n",
    "for example in dataset['train']:\n",
    "    document = \" \".join(example['set'])  # Concatenate list of strings into a single string\n",
    "    documents.append(Document(page_content=document, metadata={}))\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = character_splitter.split_documents(documents)\n",
    "\n",
    "print(docs[:2])\n",
    "print(f\"\\nTotal chunks: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0e589aa-870d-48e0-91a5-0fb1169f7be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman has been seriously injured in a collision with a police van in north devon. woman seriously injured in collision with police van\n",
      "\n",
      "Total chunks: 180799\n"
     ]
    }
   ],
   "source": [
    "# Initialize token splitter\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for doc in docs:\n",
    "    # Convert each chunk into the expected format\n",
    "    split_chunks = token_splitter.split_text(doc.page_content)\n",
    "    for chunk in split_chunks:\n",
    "        token_split_texts.append({\"page_content\": chunk})\n",
    "\n",
    "# Print the shortened text of the 10th chunk and the total number of chunks\n",
    "print(textwrap.shorten(token_split_texts[10][\"page_content\"], width=200))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a4896-89de-4fb6-81f7-396ee23e6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create embeddings for the documents\n",
    "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "# Define file paths\n",
    "index_file_path = \"language_helper_faiss_index.bin\"\n",
    "metadata_file_path = \"language_helper_metadata.pkl\"\n",
    "\n",
    "# Recreate the HuggingFaceEmbeddings object\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Step 3: Create a FAISS vector store from the documents and embeddings\n",
    "# Check if the FAISS index and metadata files exist\n",
    "if not os.path.isfile(index_file_path) and os.path.isfile(metadata_file_path):\n",
    "    print(\"Loading existing FAISS index and metadata...\")\n",
    "\n",
    "    # Load the FAISS index from the file\n",
    "    faiss_index = faiss.read_index(index_file_path)\n",
    "\n",
    "    # Load the metadata and embeddings\n",
    "    with open(metadata_file_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    docs = metadata[\"docs\"]\n",
    "    index_to_docstore_id = metadata[\"index_to_docstore_id\"]\n",
    "\n",
    "    # Recreate the document store\n",
    "    docstore = InMemoryDocstore(docs)\n",
    "\n",
    "    # Recreate the FAISS vector store\n",
    "    db = FAISS(\n",
    "        embedding_function=embeddings.embed_query,\n",
    "        index=faiss_index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "    )\n",
    "\n",
    "    print(\"FAISS index and metadata loaded successfully.\")\n",
    "else:\n",
    "    print(\"Creating new FAISS index and saving metadata...\")\n",
    "\n",
    "    # Assuming `docs` and `embeddings` are already defined before this step\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    # Save the FAISS index to a file\n",
    "    faiss.write_index(db.index, index_file_path)\n",
    "\n",
    "    # Save the document store and index_to_docstore_id\n",
    "    docstore = db.docstore\n",
    "    index_to_docstore_id = db.index_to_docstore_id\n",
    "\n",
    "    # Save the metadata\n",
    "    metadata = {\n",
    "        \"docs\": docstore._dict,\n",
    "        \"index_to_docstore_id\": index_to_docstore_id,\n",
    "        \"embedding_model_name\": model_path,\n",
    "    }\n",
    "    with open(metadata_file_path, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(\"FAISS index and metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df52a29-392e-4549-8e14-2425575cd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "\n",
    "\n",
    "# Creating a PromptTemplate\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# LangChain Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# Define the context and question for the query\n",
    "context = \"Correct the following sentence: 'Proof-of-work is an consensus mechanism using in blockchain technology to become achieve agreement among distributed participants.'\"\n",
    "question = \"\"Correct the following sentence: 'what does proof-of-work solve the majority decision-making problem? Explain like I am five.'\"\n",
    "\n",
    "# Get the result\n",
    "result = qa_chain({\"context\": context, \"question\": question})\n",
    "\n",
    "# Print the result\n",
    "print(result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e18035-e7d0-4ed0-b8e4-420417f1dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define the QA prompt template\n",
    "template = \"\"\"Please utilize the context provided below to formulate and improve the sentense a well-informed response to the question posed at the end. If uncertain about the answer, simply acknowledge it rather than speculating. Aim for clarity and conciseness in your response.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Detailed Answer:\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ec578-f7dd-4861-9803-3dcecea82eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538061f-4014-4f67-8a91-ba7dd1db871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=model_name)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "chat = ChatNVIDIA(model=\"mixtral_8x7b\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "doc_chain = load_qa_chain(chat , chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    question_generator=question_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d000c-8ffa-43fc-8795-016f23bec392",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Correct my sentence 'You need to go to school with your uncle who go to market yesterday'?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f501faa-3e18-4b18-b2e6-c3f2feb06c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=model_name, temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "qa_prompt=QA_PROMPT\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae363093-920b-47b7-92eb-1aee2b836b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Correct my sentence 'Near the school there is a many good book stores'?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee30fab-9df1-414f-9689-1e641fcac7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313681d-c588-424e-adb7-8238059a8516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
