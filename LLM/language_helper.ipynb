{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786c5e6b-c085-4df9-8c5c-426f2a9bed71",
   "metadata": {},
   "source": [
    "## Engish Langaguge Model for Grammer Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3466a916-db0f-4a7c-b474-64d506296cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain_nvidia_ai_endpoints\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0a43ee-ecc5-48db-b56f-be71fe0466b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import re\n",
    "from typing import List, Union\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import textwrap\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Include the NVIDIA API key\n",
    "load_dotenv()\n",
    "nvidia_api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# client = openai.OpenAI(\n",
    "#   base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "#   api_key = nvidia_api_key\n",
    "# )\n",
    "# print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fefce83-05bf-41f8-9481-26772ac406fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"microsoft/phi-3-mini-128k-instruct\"\n",
    "# llm = ChatNVIDIA(model=model_name, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3f9cde-474d-4136-90b0-0666345d5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.1,  # Control the randomness of the predictions\n",
    "    \"max_length\": 1000   # Set the maximum length for the generated sequences\n",
    "}\n",
    "\n",
    "# Define terminators for the text generation\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"\")\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=model_kwargs[\"temperature\"],\n",
    "    # max_length=model_kwargs[\"max_length\"],\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49003320-2769-47b6-b7f9-f3b461b5a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the HuggingFacePipeline with the defined pipeline\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1b1e60-a385-4d58-aa13-714201fb877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could have been a engineering student\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Please correct my sentence 'I could had been a engineering student'?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e9345b5-d109-4921-acd9-d02107ec4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a grammatically correct word\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is grammer and how can I learn it properly?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b1c178-47de-434b-93e1-abbb38fb9ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhira\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League's two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints. USHL completes expansion draft\"), Document(page_content='Major League Baseball Commissioner Bud Selig will be speaking at St. Norbert College next month. Bud Selig to speak at St. Norbert College')]\n",
      "\n",
      "Total chunks: 180799\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFaceEmbeddings\n",
    "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Define embedding path\n",
    "embedding_path = \"embed/\"\n",
    "\n",
    "# Load the community-datasets/sentence-compression dataset from Hugging Face\n",
    "dataset = load_dataset(\"embedding-data/sentence-compression\")\n",
    "\n",
    "documents = []\n",
    "for example in dataset['train']:\n",
    "    document = \" \".join(example['set'])  # Concatenate list of strings into a single string\n",
    "    documents.append(Document(page_content=document, metadata={}))\n",
    "\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = character_splitter.split_documents(documents)\n",
    "\n",
    "print(docs[:2])\n",
    "print(f\"\\nTotal chunks: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0e589aa-870d-48e0-91a5-0fb1169f7be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman has been seriously injured in a collision with a police van in north devon. woman seriously injured in collision with police van\n",
      "\n",
      "Total chunks: 180799\n"
     ]
    }
   ],
   "source": [
    "# Initialize token splitter\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for doc in docs:\n",
    "    # Convert each chunk into the expected format\n",
    "    split_chunks = token_splitter.split_text(doc.page_content)\n",
    "    for chunk in split_chunks:\n",
    "        token_split_texts.append({\"page_content\": chunk})\n",
    "\n",
    "# Print the shortened text of the 10th chunk and the total number of chunks\n",
    "print(textwrap.shorten(token_split_texts[10][\"page_content\"], width=200))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7a4896-89de-4fb6-81f7-396ee23e6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index and saving metadata...\n",
      "FAISS index and metadata saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create embeddings for the documents\n",
    "model_path = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "# Define file paths\n",
    "index_file_path = \"language_helper_faiss_index.bin\"\n",
    "metadata_file_path = \"language_helper_metadata.pkl\"\n",
    "\n",
    "# Recreate the HuggingFaceEmbeddings object\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Step 3: Create a FAISS vector store from the documents and embeddings\n",
    "# Check if the FAISS index and metadata files exist\n",
    "if os.path.isfile(index_file_path) and os.path.isfile(metadata_file_path):\n",
    "    print(\"Loading existing FAISS index and metadata...\")\n",
    "\n",
    "    # Load the FAISS index from the file\n",
    "    faiss_index = faiss.read_index(index_file_path)\n",
    "\n",
    "    # Load the metadata and embeddings\n",
    "    with open(metadata_file_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    docs = metadata[\"docs\"]\n",
    "    index_to_docstore_id = metadata[\"index_to_docstore_id\"]\n",
    "\n",
    "    # Recreate the document store\n",
    "    docstore = InMemoryDocstore(docs)\n",
    "\n",
    "    # Recreate the FAISS vector store\n",
    "    db = FAISS(\n",
    "        embedding_function=embeddings.embed_query,\n",
    "        index=faiss_index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "    )\n",
    "\n",
    "    print(\"FAISS index and metadata loaded successfully.\")\n",
    "else:\n",
    "    print(\"Creating new FAISS index and saving metadata...\")\n",
    "\n",
    "    # Assuming `docs` and `embeddings` are already defined before this step\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    # Save the FAISS index to a file\n",
    "    faiss.write_index(db.index, index_file_path)\n",
    "\n",
    "    # Save the document store and index_to_docstore_id\n",
    "    docstore = db.docstore\n",
    "    index_to_docstore_id = db.index_to_docstore_id\n",
    "\n",
    "    # Save the metadata\n",
    "    metadata = {\n",
    "        \"docs\": docstore._dict,\n",
    "        \"index_to_docstore_id\": index_to_docstore_id,\n",
    "        \"embedding_model_name\": model_path,\n",
    "    }\n",
    "    with open(metadata_file_path, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(\"FAISS index and metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1df52a29-392e-4549-8e14-2425575cd8b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correct_my_sentence(question):\n",
    "    # Define the template for the prompt\n",
    "    template = \"\"\"\n",
    "    Context: {context}\n",
    "    \n",
    "    Sentence: {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating a PromptTemplate\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # LangChain Chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 1}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "    )\n",
    "    \n",
    "    # Define the context and question for the query\n",
    "    context = \"Correct the grammer of the following sentence:\"\n",
    "    query = f\"Context: {context}\\nQuestion: {question}\"\n",
    "    \n",
    "    # Get the result\n",
    "    result = qa_chain({\"query\": query})\n",
    "\n",
    "    return result[\"result\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d30d000c-8ffa-43fc-8795-016f23bec392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a boy\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = 'I are a boy'\n",
    "result = correct_my_sentence(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ae363093-920b-47b7-92eb-1aee2b836b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitney Houston\n"
     ]
    }
   ],
   "source": [
    "question = \"I could had been went there\"\n",
    "result = correct_my_sentence(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee30fab-9df1-414f-9689-1e641fcac7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
