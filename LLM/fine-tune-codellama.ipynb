{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:41.972226Z",
     "iopub.status.busy": "2025-06-07T13:05:41.971624Z",
     "iopub.status.idle": "2025-06-07T13:05:41.976962Z",
     "shell.execute_reply": "2025-06-07T13:05:41.976071Z",
     "shell.execute_reply.started": "2025-06-07T13:05:41.972193Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:41.978586Z",
     "iopub.status.busy": "2025-06-07T13:05:41.978232Z",
     "iopub.status.idle": "2025-06-07T13:05:43.310446Z",
     "shell.execute_reply": "2025-06-07T13:05:43.309537Z",
     "shell.execute_reply.started": "2025-06-07T13:05:41.978562Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "working_dir = \"/kaggle/working\"\n",
    "\n",
    "# for deletion of wworking_dir\n",
    "for filename in os.listdir(working_dir):\n",
    "    file_path = os.path.join(working_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {file_path}. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:43.312022Z",
     "iopub.status.busy": "2025-06-07T13:05:43.311692Z",
     "iopub.status.idle": "2025-06-07T13:05:46.964274Z",
     "shell.execute_reply": "2025-06-07T13:05:46.963203Z",
     "shell.execute_reply.started": "2025-06-07T13:05:43.311979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install -U -q --no-deps xformers trl transformers datasets peft accelerate huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging,\n",
    "                          Trainer,\n",
    "                          DataCollatorForLanguageModeling)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:46.965536Z",
     "iopub.status.busy": "2025-06-07T13:05:46.965289Z",
     "iopub.status.idle": "2025-06-07T13:05:47.083360Z",
     "shell.execute_reply": "2025-06-07T13:05:47.082499Z",
     "shell.execute_reply.started": "2025-06-07T13:05:46.965514Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"your_hugging_face_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:47.085774Z",
     "iopub.status.busy": "2025-06-07T13:05:47.085525Z",
     "iopub.status.idle": "2025-06-07T13:05:47.216903Z",
     "shell.execute_reply": "2025-06-07T13:05:47.216212Z",
     "shell.execute_reply.started": "2025-06-07T13:05:47.085757Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"HUGGING_FACE_WRITE_API_KEY\"\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:47.217940Z",
     "iopub.status.busy": "2025-06-07T13:05:47.217682Z",
     "iopub.status.idle": "2025-06-07T13:05:54.070892Z",
     "shell.execute_reply": "2025-06-07T13:05:54.070193Z",
     "shell.execute_reply.started": "2025-06-07T13:05:47.217921Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c03fd6c5e1643da967dbb7eaad2f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318f767c7e5d4acab9e14bbcc5f5f05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a67203db3045a6978c2c051ec2f258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE.txt:   0%|          | 0.00/7.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15806d755c3643faac72ab8f76c349f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/consolidated.00.pth:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b774a8b862b44ec85e80c346d205ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0d1000d11840e7b13645950334b2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c72ad16826e4d7c9f7a66bf1a38f4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86246ff92b446c0977ae71acc6268b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86bf6e0bf9d4b1dbaf17411ed8be42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9ed66333934613bf0a29cc3df90fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708c6dc31aca490eb092bdd641e2e280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08e22f73e13445ab3aebaeb8463b4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "USE_POLICY.md:   0%|          | 0.00/6.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b950fa93451e4380946f6fd32f3d59be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e2d784e4ff44fea85b5660ca01a6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Model details\n",
    "model_id = \"codellama/CodeLlama-7B-Instruct-hf\"\n",
    "local_dir = \"/kaggle/working/codellama\"\n",
    "\n",
    "# Download full snapshot locally\n",
    "model_path = snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:54.072241Z",
     "iopub.status.busy": "2025-06-07T13:05:54.071926Z",
     "iopub.status.idle": "2025-06-07T13:05:57.043338Z",
     "shell.execute_reply": "2025-06-07T13:05:57.042413Z",
     "shell.execute_reply.started": "2025-06-07T13:05:54.072214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# running on kaggle\n",
    "# Load tokenizer and model from local downloaded path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if not running on kaggle\n",
    "# model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:05:57.046144Z",
     "iopub.status.busy": "2025-06-07T13:05:57.045136Z",
     "iopub.status.idle": "2025-06-07T13:05:59.889269Z",
     "shell.execute_reply": "2025-06-07T13:05:59.888263Z",
     "shell.execute_reply.started": "2025-06-07T13:05:57.046114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the tallest building in the world? What is the most expensive building in the world? The answer to these questions are two of the world's most expensive buildings, the Burj Khalifa in Dubai and the Burj Al Arab in Dubai. Both of these buildings are over 600 meters high, which makes them the tallest buildings in the world. The Burj Khalifa is also the tallest building in the world for a long time, until the Burj Dubai was built.\n",
      "The Burj Khalifa is also the world's tallest building when measured from the top of the building to the top of the antenna. The Burj Al Arab\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to convert COBOL code into Python?\"\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataset \n",
    "| Dataset Name                    | HF ID                                            | Contains                  | Use                                |\n",
    "| ------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------- |\n",
    "| **MainframeBench**              | `Fsoft-AIC/MainframeBench`                       | COBOL code + descriptions | Base COBOL understanding           |\n",
    "| **The Stack**                   | `bigcode/the-stack`                              | COBOL + other languages   | Language variety + COBOL samples   |\n",
    "| **Python Code Dataset**         | `jtatman/python-code-dataset-500k`               | Python code               | Target code corpus                 |\n",
    "| **SantaCoder Fine-tuned COBOL** | `muhtasham/santacoder-finetuned-the-stack-cobol` | Pretrained model          | Base model for COBOL understanding |\n",
    "| **General Code**                | `codeparrot/github-code`                         | Multi-language            | Extra fine-tuning                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.094231Z",
     "iopub.status.busy": "2025-06-07T13:06:05.092671Z",
     "iopub.status.idle": "2025-06-07T13:06:05.771088Z",
     "shell.execute_reply": "2025-06-07T13:06:05.770321Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.094204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Fsoft-AIC/MainframeBench\", split=\"train[:100]\")  # sample subset\n",
    "\n",
    "# # Dummy Python generation for demo (replace with real aligned translations)\n",
    "# dataset = dataset.map(lambda x: {\"tgt\": \"# Python translation of: \" + x[\"code\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Load relevant subsets\n",
    "mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", split=\"train\")\n",
    "stack = load_dataset(\"bigcode/the-stack\", split=\"train[:1%]\")  # 1% for feasibility\n",
    "python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "\n",
    "# Filter COBOL and Python\n",
    "stack_cobol = stack.filter(lambda x: x[\"lang\"] == \"cobol\")\n",
    "stack_python = stack.filter(lambda x: x[\"lang\"] == \"python\")\n",
    "\n",
    "# Combine relevant Python corpora\n",
    "python_combined = concatenate_datasets([stack_python, python_set])\n",
    "\n",
    "# Now build translation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy pairing for training (replace with aligned pairs if you have)\n",
    "paired_data = []\n",
    "\n",
    "for i in range(min(len(stack_cobol), len(python_combined))):\n",
    "    paired_data.append({\n",
    "        \"input\": stack_cobol[i][\"content\"],\n",
    "        \"output\": python_combined[i][\"content\"]\n",
    "    })\n",
    "\n",
    "translation_dataset = Dataset.from_list(paired_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we can upload custom dataset like following\n",
    "# {\"src\": \"COBOL code here\", \"tgt\": \"Equivalent Python code here\"}\n",
    "# {\"src\": \"Another COBOL snippet\", \"tgt\": \"Translated Python code\"}\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": \"cobol_python_dataset.jsonl\"})[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{example['src']}\n",
    "\n",
    "### Response:\n",
    "{example['tgt']}\"\"\",\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional to save dataset locally\n",
    "tokenized_dataset.to_json(\"codellama_cobol2python_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.772197Z",
     "iopub.status.busy": "2025-06-07T13:06:05.771832Z",
     "iopub.status.idle": "2025-06-07T13:06:05.792555Z",
     "shell.execute_reply": "2025-06-07T13:06:05.791811Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.772167Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Shuffle the DataFrame and select only 3000 rows\n",
    "# df = dataset.sample(frac=1, random_state=85).reset_index(drop=True).head(3000)\n",
    "\n",
    "# # Split the DataFrame\n",
    "# train_size = 0.8\n",
    "# eval_size = 0.1\n",
    "\n",
    "# # Calculate sizes\n",
    "# train_end = int(train_size * len(df))\n",
    "# eval_end = train_end + int(eval_size * len(df))\n",
    "\n",
    "# # Split the data\n",
    "# X_train = df[:train_end]\n",
    "# X_eval = df[train_end:eval_end]\n",
    "# X_test = df[eval_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.793482Z",
     "iopub.status.busy": "2025-06-07T13:06:05.793225Z",
     "iopub.status.idle": "2025-06-07T13:06:05.806990Z",
     "shell.execute_reply": "2025-06-07T13:06:05.806182Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.793460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the prompt generation functions for COBOL → Python conversion\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{data_point[\"src\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"tgt\"]}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{data_point[\"src\"]}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.808364Z",
     "iopub.status.busy": "2025-06-07T13:06:05.808106Z",
     "iopub.status.idle": "2025-06-07T13:06:05.883511Z",
     "shell.execute_reply": "2025-06-07T13:06:05.882041Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.808341Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691/3727835688.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
      "/tmp/ipykernel_691/3727835688.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# # Generate prompts for training and evaluation data\n",
    "# X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "# X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "\n",
    "# # Generate test prompts and extract true labels\n",
    "# y_true = X_test.loc[:,'status']\n",
    "# X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate prompts for training and evaluation data\n",
    "# X_train = X_train.assign(text=X_train.apply(generate_prompt, axis=1))\n",
    "# X_eval = X_eval.assign(text=X_eval.apply(generate_prompt, axis=1))\n",
    "\n",
    "# # Generate test prompts and extract true labels\n",
    "# y_true = X_test['status'].copy()\n",
    "# X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.884600Z",
     "iopub.status.busy": "2025-06-07T13:06:05.884349Z",
     "iopub.status.idle": "2025-06-07T13:06:05.891340Z",
     "shell.execute_reply": "2025-06-07T13:06:05.890579Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.884577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Normal        1028\n",
       "Depression     938\n",
       "Anxiety        258\n",
       "Bipolar        176\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.892418Z",
     "iopub.status.busy": "2025-06-07T13:06:05.892091Z",
     "iopub.status.idle": "2025-06-07T13:06:05.936794Z",
     "shell.execute_reply": "2025-06-07T13:06:05.934564Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.892391Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert to datasets\n",
    "# train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "# eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:05.937867Z",
     "iopub.status.busy": "2025-06-07T13:06:05.937557Z",
     "iopub.status.idle": "2025-06-07T13:06:05.961514Z",
     "shell.execute_reply": "2025-06-07T13:06:05.960755Z",
     "shell.execute_reply.started": "2025-06-07T13:06:05.937843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classify the text into Normal, Depression, Anxiety, Bipolar, and return the answer as the corresponding mental health disorder label.\\ntext: I am so sad. Everything in my work life is going fine, but my personal life is a wreck. No one ever takes me seriously because I am the funny friend. I do not want to talk to anyone anymore. I just want to die sometimes. Please help me. I have never had this feeling in my entire life. Why am I so sad\\nlabel: Depression'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:10.258962Z",
     "iopub.status.busy": "2025-06-07T13:06:10.258640Z",
     "iopub.status.idle": "2025-06-07T13:06:10.264409Z",
     "shell.execute_reply": "2025-06-07T13:06:10.263475Z",
     "shell.execute_reply.started": "2025-06-07T13:06:10.258930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:10.265651Z",
     "iopub.status.busy": "2025-06-07T13:06:10.265454Z",
     "iopub.status.idle": "2025-06-07T13:06:21.729860Z",
     "shell.execute_reply": "2025-06-07T13:06:21.729067Z",
     "shell.execute_reply.started": "2025-06-07T13:06:10.265636Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Set env var to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "base_model_name = local_dir  # your snapshot_download path\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,  # helps reduce memory\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model in 4-bit with automatic device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map={\"\": 0},\n",
    "    max_memory={0: \"13GiB\"}, # for kaggle space constraint\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Memory optimizations\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()  # saves memory during training\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:21.730906Z",
     "iopub.status.busy": "2025-06-07T13:06:21.730680Z",
     "iopub.status.idle": "2025-06-07T13:06:52.889706Z",
     "shell.execute_reply": "2025-06-07T13:06:52.888867Z",
     "shell.execute_reply.started": "2025-06-07T13:06:21.730889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]Device set to use cuda:0\n",
      "  0%|          | 1/300 [00:00<00:39,  7.50it/s]Device set to use cuda:0\n",
      "  1%|          | 2/300 [00:00<00:35,  8.40it/s]Device set to use cuda:0\n",
      "  1%|          | 3/300 [00:00<00:32,  9.00it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  2%|▏         | 5/300 [00:00<00:30,  9.58it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  2%|▏         | 7/300 [00:00<00:29,  9.88it/s]Device set to use cuda:0\n",
      "  3%|▎         | 8/300 [00:00<00:29,  9.84it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  3%|▎         | 10/300 [00:01<00:29,  9.98it/s]Device set to use cuda:0\n",
      "  4%|▎         | 11/300 [00:01<00:29,  9.83it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  4%|▍         | 13/300 [00:01<00:28, 10.01it/s]Device set to use cuda:0\n",
      "  5%|▍         | 14/300 [00:01<00:28,  9.95it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  5%|▌         | 16/300 [00:01<00:28, 10.09it/s]Device set to use cuda:0\n",
      "  6%|▌         | 17/300 [00:01<00:28, 10.06it/s]Device set to use cuda:0\n",
      "  6%|▌         | 18/300 [00:01<00:33,  8.45it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  7%|▋         | 20/300 [00:02<00:30,  9.16it/s]Device set to use cuda:0\n",
      "  7%|▋         | 21/300 [00:02<00:31,  9.00it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  8%|▊         | 23/300 [00:02<00:29,  9.49it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  8%|▊         | 25/300 [00:02<00:27,  9.87it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "  9%|▉         | 27/300 [00:02<00:26, 10.12it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 10%|▉         | 29/300 [00:02<00:26, 10.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 10%|█         | 31/300 [00:03<00:26, 10.31it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 11%|█         | 33/300 [00:03<00:31,  8.59it/s]Device set to use cuda:0\n",
      " 11%|█▏        | 34/300 [00:03<00:30,  8.76it/s]Device set to use cuda:0\n",
      " 12%|█▏        | 35/300 [00:03<00:29,  8.89it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 12%|█▏        | 37/300 [00:03<00:28,  9.34it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 13%|█▎        | 39/300 [00:04<00:26,  9.76it/s]Device set to use cuda:0\n",
      " 13%|█▎        | 40/300 [00:04<00:26,  9.67it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 14%|█▍        | 42/300 [00:04<00:25,  9.99it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 15%|█▍        | 44/300 [00:04<00:25, 10.09it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 15%|█▌        | 46/300 [00:04<00:24, 10.27it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 16%|█▌        | 48/300 [00:04<00:24, 10.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 17%|█▋        | 50/300 [00:05<00:25,  9.72it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 17%|█▋        | 52/300 [00:05<00:25,  9.84it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 18%|█▊        | 54/300 [00:05<00:24,  9.93it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 19%|█▊        | 56/300 [00:05<00:26,  9.35it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 19%|█▉        | 58/300 [00:06<00:25,  9.67it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 20%|██        | 60/300 [00:06<00:24,  9.86it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 21%|██        | 62/300 [00:06<00:23,  9.92it/s]Device set to use cuda:0\n",
      " 21%|██        | 63/300 [00:06<00:24,  9.50it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 22%|██▏       | 65/300 [00:06<00:23,  9.83it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 22%|██▏       | 67/300 [00:06<00:23,  9.85it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 23%|██▎       | 69/300 [00:07<00:23,  9.97it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 24%|██▎       | 71/300 [00:07<00:22, 10.14it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 24%|██▍       | 73/300 [00:07<00:22, 10.13it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 25%|██▌       | 75/300 [00:07<00:22, 10.15it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 26%|██▌       | 77/300 [00:07<00:21, 10.24it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 26%|██▋       | 79/300 [00:08<00:21, 10.20it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 27%|██▋       | 81/300 [00:08<00:21, 10.24it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 28%|██▊       | 83/300 [00:08<00:21, 10.27it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 28%|██▊       | 85/300 [00:08<00:20, 10.33it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 29%|██▉       | 87/300 [00:08<00:21, 10.03it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 30%|██▉       | 89/300 [00:09<00:25,  8.29it/s]Device set to use cuda:0\n",
      " 30%|███       | 90/300 [00:09<00:24,  8.47it/s]Device set to use cuda:0\n",
      " 30%|███       | 91/300 [00:09<00:24,  8.53it/s]Device set to use cuda:0\n",
      " 31%|███       | 92/300 [00:09<00:24,  8.63it/s]Device set to use cuda:0\n",
      " 31%|███       | 93/300 [00:09<00:25,  8.16it/s]Device set to use cuda:0\n",
      " 31%|███▏      | 94/300 [00:09<00:24,  8.51it/s]Device set to use cuda:0\n",
      " 32%|███▏      | 95/300 [00:09<00:23,  8.66it/s]Device set to use cuda:0\n",
      " 32%|███▏      | 96/300 [00:10<00:22,  8.91it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 33%|███▎      | 98/300 [00:10<00:21,  9.62it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 33%|███▎      | 100/300 [00:10<00:20,  9.91it/s]Device set to use cuda:0\n",
      " 34%|███▎      | 101/300 [00:10<00:22,  8.94it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 34%|███▍      | 103/300 [00:10<00:20,  9.47it/s]Device set to use cuda:0\n",
      " 35%|███▍      | 104/300 [00:10<00:20,  9.58it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 35%|███▌      | 106/300 [00:11<00:19,  9.99it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 36%|███▌      | 108/300 [00:11<00:18, 10.17it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 37%|███▋      | 110/300 [00:11<00:18, 10.35it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 37%|███▋      | 112/300 [00:11<00:18, 10.02it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 38%|███▊      | 114/300 [00:11<00:20,  9.09it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 39%|███▊      | 116/300 [00:12<00:19,  9.26it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 39%|███▉      | 118/300 [00:12<00:19,  9.57it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 40%|████      | 120/300 [00:12<00:18,  9.78it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 41%|████      | 122/300 [00:12<00:17, 10.01it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 41%|████▏     | 124/300 [00:12<00:17, 10.15it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 42%|████▏     | 126/300 [00:13<00:16, 10.24it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 43%|████▎     | 128/300 [00:13<00:16, 10.32it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 43%|████▎     | 130/300 [00:13<00:16, 10.30it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 44%|████▍     | 132/300 [00:13<00:16, 10.27it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 45%|████▍     | 134/300 [00:13<00:16, 10.36it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 45%|████▌     | 136/300 [00:13<00:15, 10.47it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 46%|████▌     | 138/300 [00:14<00:15, 10.51it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 47%|████▋     | 140/300 [00:14<00:17,  8.92it/s]Device set to use cuda:0\n",
      " 47%|████▋     | 141/300 [00:14<00:17,  9.08it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 48%|████▊     | 143/300 [00:14<00:16,  9.46it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 48%|████▊     | 145/300 [00:14<00:15,  9.77it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 49%|████▉     | 147/300 [00:15<00:15, 10.04it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 50%|████▉     | 149/300 [00:15<00:14, 10.17it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 50%|█████     | 151/300 [00:15<00:14, 10.27it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 51%|█████     | 153/300 [00:15<00:14, 10.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 52%|█████▏    | 155/300 [00:15<00:14, 10.23it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 52%|█████▏    | 157/300 [00:16<00:14, 10.18it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 53%|█████▎    | 159/300 [00:16<00:13, 10.11it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 54%|█████▎    | 161/300 [00:16<00:13, 10.29it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 54%|█████▍    | 163/300 [00:16<00:13, 10.16it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 55%|█████▌    | 165/300 [00:16<00:13, 10.18it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 56%|█████▌    | 167/300 [00:17<00:13, 10.19it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 56%|█████▋    | 169/300 [00:17<00:12, 10.29it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 57%|█████▋    | 171/300 [00:17<00:12, 10.35it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 58%|█████▊    | 173/300 [00:17<00:12, 10.39it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 58%|█████▊    | 175/300 [00:17<00:13,  9.42it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 59%|█████▉    | 177/300 [00:18<00:13,  9.20it/s]Device set to use cuda:0\n",
      " 59%|█████▉    | 178/300 [00:18<00:13,  8.76it/s]Device set to use cuda:0\n",
      " 60%|█████▉    | 179/300 [00:18<00:14,  8.58it/s]Device set to use cuda:0\n",
      " 60%|██████    | 180/300 [00:18<00:14,  8.54it/s]Device set to use cuda:0\n",
      " 60%|██████    | 181/300 [00:18<00:14,  8.23it/s]Device set to use cuda:0\n",
      " 61%|██████    | 182/300 [00:18<00:16,  7.28it/s]Device set to use cuda:0\n",
      " 61%|██████    | 183/300 [00:19<00:16,  7.04it/s]Device set to use cuda:0\n",
      " 61%|██████▏   | 184/300 [00:19<00:16,  6.98it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 185/300 [00:19<00:16,  6.93it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 186/300 [00:19<00:16,  6.81it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 187/300 [00:19<00:16,  6.90it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 188/300 [00:19<00:15,  7.07it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 189/300 [00:19<00:14,  7.71it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 190/300 [00:19<00:13,  8.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 64%|██████▍   | 192/300 [00:20<00:11,  9.14it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 65%|██████▍   | 194/300 [00:20<00:11,  9.49it/s]Device set to use cuda:0\n",
      " 65%|██████▌   | 195/300 [00:20<00:11,  9.43it/s]Device set to use cuda:0\n",
      " 65%|██████▌   | 196/300 [00:20<00:11,  9.08it/s]Device set to use cuda:0\n",
      " 66%|██████▌   | 197/300 [00:20<00:11,  9.26it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 66%|██████▋   | 199/300 [00:20<00:10,  9.63it/s]Device set to use cuda:0\n",
      " 67%|██████▋   | 200/300 [00:20<00:10,  9.62it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 67%|██████▋   | 202/300 [00:21<00:09,  9.87it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 68%|██████▊   | 204/300 [00:21<00:09, 10.07it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 69%|██████▊   | 206/300 [00:21<00:09, 10.09it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 69%|██████▉   | 208/300 [00:21<00:09, 10.12it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 70%|███████   | 210/300 [00:21<00:08, 10.11it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 71%|███████   | 212/300 [00:22<00:08, 10.27it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 71%|███████▏  | 214/300 [00:22<00:08, 10.36it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 72%|███████▏  | 216/300 [00:22<00:08, 10.36it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 73%|███████▎  | 218/300 [00:22<00:07, 10.36it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 73%|███████▎  | 220/300 [00:22<00:07, 10.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 74%|███████▍  | 222/300 [00:23<00:07, 10.13it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 75%|███████▍  | 224/300 [00:23<00:07, 10.15it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 75%|███████▌  | 226/300 [00:23<00:07, 10.11it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 76%|███████▌  | 228/300 [00:23<00:07, 10.08it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 77%|███████▋  | 230/300 [00:23<00:06, 10.17it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 77%|███████▋  | 232/300 [00:24<00:06, 10.26it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 78%|███████▊  | 234/300 [00:24<00:06, 10.30it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 79%|███████▊  | 236/300 [00:24<00:06, 10.39it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 79%|███████▉  | 238/300 [00:24<00:05, 10.35it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 80%|████████  | 240/300 [00:24<00:05, 10.25it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 81%|████████  | 242/300 [00:25<00:05, 10.30it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 81%|████████▏ | 244/300 [00:25<00:05, 10.29it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 82%|████████▏ | 246/300 [00:25<00:05, 10.26it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 83%|████████▎ | 248/300 [00:25<00:05, 10.20it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 83%|████████▎ | 250/300 [00:25<00:04, 10.15it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 84%|████████▍ | 252/300 [00:26<00:04, 10.02it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 85%|████████▍ | 254/300 [00:26<00:04,  9.92it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 85%|████████▌ | 256/300 [00:26<00:04, 10.05it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 86%|████████▌ | 258/300 [00:26<00:04, 10.11it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 87%|████████▋ | 260/300 [00:26<00:03, 10.09it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 87%|████████▋ | 262/300 [00:27<00:03, 10.02it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 88%|████████▊ | 264/300 [00:27<00:03,  9.67it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 89%|████████▊ | 266/300 [00:27<00:03,  9.78it/s]Device set to use cuda:0\n",
      " 89%|████████▉ | 267/300 [00:27<00:03,  9.50it/s]Device set to use cuda:0\n",
      " 89%|████████▉ | 268/300 [00:27<00:03,  9.46it/s]Device set to use cuda:0\n",
      " 90%|████████▉ | 269/300 [00:27<00:03,  9.53it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 90%|█████████ | 271/300 [00:28<00:02,  9.74it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 91%|█████████ | 273/300 [00:28<00:02,  9.78it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 92%|█████████▏| 275/300 [00:28<00:02,  9.92it/s]Device set to use cuda:0\n",
      " 92%|█████████▏| 276/300 [00:28<00:02,  9.81it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 93%|█████████▎| 278/300 [00:28<00:02,  9.98it/s]Device set to use cuda:0\n",
      " 93%|█████████▎| 279/300 [00:28<00:02,  9.76it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 94%|█████████▎| 281/300 [00:29<00:01,  9.98it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 94%|█████████▍| 283/300 [00:29<00:01, 10.09it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 95%|█████████▌| 285/300 [00:29<00:01, 10.08it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 96%|█████████▌| 287/300 [00:29<00:01,  9.85it/s]Device set to use cuda:0\n",
      " 96%|█████████▌| 288/300 [00:29<00:01,  9.76it/s]Device set to use cuda:0\n",
      " 96%|█████████▋| 289/300 [00:30<00:01,  6.65it/s]Device set to use cuda:0\n",
      " 97%|█████████▋| 290/300 [00:30<00:01,  7.21it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 97%|█████████▋| 292/300 [00:30<00:00,  8.26it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 98%|█████████▊| 294/300 [00:30<00:00,  9.00it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 99%|█████████▊| 296/300 [00:30<00:00,  9.46it/s]Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      " 99%|█████████▉| 298/300 [00:30<00:00,  9.75it/s]Device set to use cuda:0\n",
      "100%|█████████▉| 299/300 [00:31<00:00,  9.67it/s]Device set to use cuda:0\n",
      "100%|██████████| 300/300 [00:31<00:00,  9.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_code_translation(test_df, model, tokenizer):\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(len(test_df))):\n",
    "        cobol_code = test_df.iloc[i][\"src\"]\n",
    "        prompt = f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "        pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        result = pipe(prompt)\n",
    "        generated_code = result[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "        predictions.append(generated_code)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "y_pred = predict_code_translation(X_test, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:52.891217Z",
     "iopub.status.busy": "2025-06-07T13:06:52.890910Z",
     "iopub.status.idle": "2025-06-07T13:06:52.913394Z",
     "shell.execute_reply": "2025-06-07T13:06:52.912587Z",
     "shell.execute_reply.started": "2025-06-07T13:06:52.891193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.427\n",
      "Accuracy for label Normal: 0.720\n",
      "Accuracy for label Depression: 0.148\n",
      "Accuracy for label Anxiety: 0.000\n",
      "Accuracy for label Bipolar: 0.533\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.61      0.72      0.66       143\n",
      "  Depression       0.40      0.15      0.22       115\n",
      "     Anxiety       0.00      0.00      0.00        27\n",
      "     Bipolar       0.11      0.53      0.19        15\n",
      "\n",
      "   micro avg       0.45      0.43      0.44       300\n",
      "   macro avg       0.28      0.35      0.26       300\n",
      "weighted avg       0.45      0.43      0.41       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[103  21   0   6]\n",
      " [ 44  17   1  51]\n",
      " [ 17   4   0   6]\n",
      " [  6   1   0   8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_code_translation(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred), \"Mismatch in number of samples\"\n",
    "    \n",
    "    def code_similarity(a, b):\n",
    "        return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n",
    "    \n",
    "    similarities = [code_similarity(gt, pred) for gt, pred in zip(y_true, y_pred)]\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    \n",
    "    exact_matches = sum(1 for gt, pred in zip(y_true, y_pred) if gt.strip() == pred.strip())\n",
    "    accuracy = exact_matches / len(y_true)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Average Similarity Score: {avg_similarity:.3f}\")\n",
    "    \n",
    "    # Optionally, show some examples\n",
    "    for i in range(min(3, len(y_true))):\n",
    "        print(\"\\n--- Sample\", i+1)\n",
    "        print(\"True Output:\\n\", y_true[i])\n",
    "        print(\"Predicted Output:\\n\", y_pred[i])\n",
    "        print(\"Similarity Score:\", code_similarity(y_true[i], y_pred[i]))\n",
    "\n",
    "# Example usage:\n",
    "evaluate_code_translation(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:06:52.914506Z",
     "iopub.status.busy": "2025-06-07T13:06:52.914259Z",
     "iopub.status.idle": "2025-06-07T13:06:52.922301Z",
     "shell.execute_reply": "2025-06-07T13:06:52.921615Z",
     "shell.execute_reply.started": "2025-06-07T13:06:52.914490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v_proj', 'gate_proj', 'k_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:11:21.491231Z",
     "iopub.status.busy": "2025-06-07T13:11:21.490525Z",
     "iopub.status.idle": "2025-06-07T13:11:25.008564Z",
     "shell.execute_reply": "2025-06-07T13:11:25.007798Z",
     "shell.execute_reply.started": "2025-06-07T13:11:21.491205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51c27c740f04f6588e85cde9d182488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad497c59ce5e4cc3b2b170610dc816e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce58070d6caf49a6a7be78b2a443ec1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3013fe1d4f4fd08ad1d7a4e70daf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b00100f294b39abb9ae966db85bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58644071942740cdac118a8172339132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c2f6e328ee4c9898449a038e79f1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d436b2435b14b95be2135ab3608457e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Avoid CUDA memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "output_dir = \"CodeLlama-7B-Instruct-fine-tuned-model\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules  # You must define `modules`, e.g., [\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,     # Path to save checkpoints\n",
    "    per_device_train_batch_size=2,             # Small batch for large models\n",
    "    gradient_accumulation_steps=4,             # Effective batch size = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    bf16=True,                                 # If available, use bf16\n",
    "    fp16=False,                                # Disable fp16 to avoid conflict with bf16\n",
    "\n",
    "    gradient_checkpointing=True,               # Save memory\n",
    "    optim=\"paged_adamw_32bit\",                 # Efficient optimizer\n",
    "    max_grad_norm=0.3,                         # Gradient clipping\n",
    "\n",
    "    lr_scheduler_type=\"cosine\",                # Cosine annealing\n",
    "    warmup_ratio=0.03,                         # Warmup steps\n",
    "\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "\n",
    "    report_to=\"none\",                          # Avoid wandb/logging integrations\n",
    "    disable_tqdm=False                         # Show progress bars\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=128,       # smaller = less memory\n",
    "    # tokenizer=tokenizer,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # tokenizer=tokenizer,\n",
    "    # max_seq_length=128,\n",
    "    # packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:11:25.010176Z",
     "iopub.status.busy": "2025-06-07T13:11:25.009866Z",
     "iopub.status.idle": "2025-06-07T13:23:57.817614Z",
     "shell.execute_reply": "2025-06-07T13:23:57.817044Z",
     "shell.execute_reply.started": "2025-06-07T13:11:25.010149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 12:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.829200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38, training_loss=1.291626883180518, metrics={'train_runtime': 752.1467, 'train_samples_per_second': 3.191, 'train_steps_per_second': 0.051, 'total_flos': 3200230769049600.0, 'train_loss': 1.291626883180518})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To start the fine-tuning process:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:23:57.818544Z",
     "iopub.status.busy": "2025-06-07T13:23:57.818344Z",
     "iopub.status.idle": "2025-06-07T13:23:58.538022Z",
     "shell.execute_reply": "2025-06-07T13:23:58.537181Z",
     "shell.execute_reply.started": "2025-06-07T13:23:57.818528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-3.2-fine-tuned-model/tokenizer_config.json',\n",
       " 'llama-3.2-fine-tuned-model/special_tokens_map.json',\n",
       " 'llama-3.2-fine-tuned-model/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:23:58.540434Z",
     "iopub.status.busy": "2025-06-07T13:23:58.540159Z",
     "iopub.status.idle": "2025-06-07T13:24:50.602399Z",
     "shell.execute_reply": "2025-06-07T13:24:50.601558Z",
     "shell.execute_reply.started": "2025-06-07T13:23:58.540415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]Device set to use cuda:0\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "  0%|          | 1/300 [00:00<01:12,  4.12it/s]Device set to use cuda:0\n",
      "  1%|          | 2/300 [00:00<00:58,  5.12it/s]Device set to use cuda:0\n",
      "  1%|          | 3/300 [00:00<00:53,  5.55it/s]Device set to use cuda:0\n",
      "  1%|▏         | 4/300 [00:00<00:50,  5.83it/s]Device set to use cuda:0\n",
      "  2%|▏         | 5/300 [00:00<00:49,  6.01it/s]Device set to use cuda:0\n",
      "  2%|▏         | 6/300 [00:01<00:48,  6.06it/s]Device set to use cuda:0\n",
      "  2%|▏         | 7/300 [00:01<00:47,  6.11it/s]Device set to use cuda:0\n",
      "  3%|▎         | 8/300 [00:01<00:47,  6.12it/s]Device set to use cuda:0\n",
      "  3%|▎         | 9/300 [00:01<00:47,  6.14it/s]Device set to use cuda:0\n",
      "  3%|▎         | 10/300 [00:01<00:48,  6.01it/s]Device set to use cuda:0\n",
      "  4%|▎         | 11/300 [00:01<00:47,  6.05it/s]Device set to use cuda:0\n",
      "  4%|▍         | 12/300 [00:02<00:47,  6.09it/s]Device set to use cuda:0\n",
      "  4%|▍         | 13/300 [00:02<00:46,  6.12it/s]Device set to use cuda:0\n",
      "  5%|▍         | 14/300 [00:02<00:46,  6.11it/s]Device set to use cuda:0\n",
      "  5%|▌         | 15/300 [00:02<00:46,  6.10it/s]Device set to use cuda:0\n",
      "  5%|▌         | 16/300 [00:02<00:48,  5.90it/s]Device set to use cuda:0\n",
      "  6%|▌         | 17/300 [00:02<00:49,  5.74it/s]Device set to use cuda:0\n",
      "  6%|▌         | 18/300 [00:03<01:02,  4.54it/s]Device set to use cuda:0\n",
      "  6%|▋         | 19/300 [00:03<00:56,  4.96it/s]Device set to use cuda:0\n",
      "  7%|▋         | 20/300 [00:03<00:52,  5.29it/s]Device set to use cuda:0\n",
      "  7%|▋         | 21/300 [00:03<00:55,  5.02it/s]Device set to use cuda:0\n",
      "  7%|▋         | 22/300 [00:03<00:52,  5.32it/s]Device set to use cuda:0\n",
      "  8%|▊         | 23/300 [00:04<00:49,  5.60it/s]Device set to use cuda:0\n",
      "  8%|▊         | 24/300 [00:04<00:47,  5.77it/s]Device set to use cuda:0\n",
      "  8%|▊         | 25/300 [00:04<00:46,  5.87it/s]Device set to use cuda:0\n",
      "  9%|▊         | 26/300 [00:04<00:46,  5.95it/s]Device set to use cuda:0\n",
      "  9%|▉         | 27/300 [00:04<00:45,  6.03it/s]Device set to use cuda:0\n",
      "  9%|▉         | 28/300 [00:04<00:44,  6.12it/s]Device set to use cuda:0\n",
      " 10%|▉         | 29/300 [00:05<00:43,  6.17it/s]Device set to use cuda:0\n",
      " 10%|█         | 30/300 [00:05<00:43,  6.21it/s]Device set to use cuda:0\n",
      " 10%|█         | 31/300 [00:05<00:43,  6.22it/s]Device set to use cuda:0\n",
      " 11%|█         | 32/300 [00:05<01:05,  4.10it/s]Device set to use cuda:0\n",
      " 11%|█         | 33/300 [00:05<00:58,  4.56it/s]Device set to use cuda:0\n",
      " 11%|█▏        | 34/300 [00:06<00:53,  4.94it/s]Device set to use cuda:0\n",
      " 12%|█▏        | 35/300 [00:06<00:50,  5.25it/s]Device set to use cuda:0\n",
      " 12%|█▏        | 36/300 [00:06<00:48,  5.47it/s]Device set to use cuda:0\n",
      " 12%|█▏        | 37/300 [00:06<00:46,  5.64it/s]Device set to use cuda:0\n",
      " 13%|█▎        | 38/300 [00:06<00:45,  5.78it/s]Device set to use cuda:0\n",
      " 13%|█▎        | 39/300 [00:06<00:44,  5.91it/s]Device set to use cuda:0\n",
      " 13%|█▎        | 40/300 [00:07<00:46,  5.56it/s]Device set to use cuda:0\n",
      " 14%|█▎        | 41/300 [00:07<00:45,  5.73it/s]Device set to use cuda:0\n",
      " 14%|█▍        | 42/300 [00:07<00:44,  5.85it/s]Device set to use cuda:0\n",
      " 14%|█▍        | 43/300 [00:07<00:43,  5.94it/s]Device set to use cuda:0\n",
      " 15%|█▍        | 44/300 [00:07<00:42,  5.99it/s]Device set to use cuda:0\n",
      " 15%|█▌        | 45/300 [00:07<00:42,  6.06it/s]Device set to use cuda:0\n",
      " 15%|█▌        | 46/300 [00:08<00:41,  6.09it/s]Device set to use cuda:0\n",
      " 16%|█▌        | 47/300 [00:08<00:41,  6.11it/s]Device set to use cuda:0\n",
      " 16%|█▌        | 48/300 [00:08<00:42,  5.96it/s]Device set to use cuda:0\n",
      " 16%|█▋        | 49/300 [00:08<00:41,  6.00it/s]Device set to use cuda:0\n",
      " 17%|█▋        | 50/300 [00:08<00:47,  5.26it/s]Device set to use cuda:0\n",
      " 17%|█▋        | 51/300 [00:09<00:45,  5.52it/s]Device set to use cuda:0\n",
      " 17%|█▋        | 52/300 [00:09<00:44,  5.58it/s]Device set to use cuda:0\n",
      " 18%|█▊        | 53/300 [00:09<00:42,  5.78it/s]Device set to use cuda:0\n",
      " 18%|█▊        | 54/300 [00:09<00:41,  5.87it/s]Device set to use cuda:0\n",
      " 18%|█▊        | 55/300 [00:09<00:40,  5.99it/s]Device set to use cuda:0\n",
      " 19%|█▊        | 56/300 [00:09<00:48,  5.08it/s]Device set to use cuda:0\n",
      " 19%|█▉        | 57/300 [00:10<00:45,  5.36it/s]Device set to use cuda:0\n",
      " 19%|█▉        | 58/300 [00:10<00:43,  5.63it/s]Device set to use cuda:0\n",
      " 20%|█▉        | 59/300 [00:10<00:41,  5.80it/s]Device set to use cuda:0\n",
      " 20%|██        | 60/300 [00:10<00:40,  5.91it/s]Device set to use cuda:0\n",
      " 20%|██        | 61/300 [00:10<00:40,  5.97it/s]Device set to use cuda:0\n",
      " 21%|██        | 62/300 [00:10<00:39,  6.02it/s]Device set to use cuda:0\n",
      " 21%|██        | 63/300 [00:11<00:44,  5.36it/s]Device set to use cuda:0\n",
      " 21%|██▏       | 64/300 [00:11<00:42,  5.57it/s]Device set to use cuda:0\n",
      " 22%|██▏       | 65/300 [00:11<00:41,  5.68it/s]Device set to use cuda:0\n",
      " 22%|██▏       | 66/300 [00:11<00:39,  5.87it/s]Device set to use cuda:0\n",
      " 22%|██▏       | 67/300 [00:11<00:38,  5.97it/s]Device set to use cuda:0\n",
      " 23%|██▎       | 68/300 [00:11<00:38,  6.03it/s]Device set to use cuda:0\n",
      " 23%|██▎       | 69/300 [00:12<00:37,  6.08it/s]Device set to use cuda:0\n",
      " 23%|██▎       | 70/300 [00:12<00:37,  6.10it/s]Device set to use cuda:0\n",
      " 24%|██▎       | 71/300 [00:12<00:37,  6.10it/s]Device set to use cuda:0\n",
      " 24%|██▍       | 72/300 [00:12<00:37,  6.06it/s]Device set to use cuda:0\n",
      " 24%|██▍       | 73/300 [00:12<00:37,  5.98it/s]Device set to use cuda:0\n",
      " 25%|██▍       | 74/300 [00:12<00:38,  5.91it/s]Device set to use cuda:0\n",
      " 25%|██▌       | 75/300 [00:13<00:37,  5.98it/s]Device set to use cuda:0\n",
      " 25%|██▌       | 76/300 [00:13<00:37,  6.05it/s]Device set to use cuda:0\n",
      " 26%|██▌       | 77/300 [00:13<00:36,  6.08it/s]Device set to use cuda:0\n",
      " 26%|██▌       | 78/300 [00:13<00:37,  5.96it/s]Device set to use cuda:0\n",
      " 26%|██▋       | 79/300 [00:13<00:37,  5.89it/s]Device set to use cuda:0\n",
      " 27%|██▋       | 80/300 [00:13<00:37,  5.94it/s]Device set to use cuda:0\n",
      " 27%|██▋       | 81/300 [00:14<00:36,  5.99it/s]Device set to use cuda:0\n",
      " 27%|██▋       | 82/300 [00:14<00:36,  5.96it/s]Device set to use cuda:0\n",
      " 28%|██▊       | 83/300 [00:14<00:36,  6.00it/s]Device set to use cuda:0\n",
      " 28%|██▊       | 84/300 [00:14<00:35,  6.06it/s]Device set to use cuda:0\n",
      " 28%|██▊       | 85/300 [00:14<00:35,  6.11it/s]Device set to use cuda:0\n",
      " 29%|██▊       | 86/300 [00:14<00:34,  6.15it/s]Device set to use cuda:0\n",
      " 29%|██▉       | 87/300 [00:15<00:34,  6.20it/s]Device set to use cuda:0\n",
      " 29%|██▉       | 88/300 [00:15<00:34,  6.21it/s]Device set to use cuda:0\n",
      " 30%|██▉       | 89/300 [00:15<00:51,  4.14it/s]Device set to use cuda:0\n",
      " 30%|███       | 90/300 [00:15<00:45,  4.61it/s]Device set to use cuda:0\n",
      " 30%|███       | 91/300 [00:16<00:41,  5.02it/s]Device set to use cuda:0\n",
      " 31%|███       | 92/300 [00:16<00:39,  5.32it/s]Device set to use cuda:0\n",
      " 31%|███       | 93/300 [00:16<00:37,  5.55it/s]Device set to use cuda:0\n",
      " 31%|███▏      | 94/300 [00:16<00:35,  5.74it/s]Device set to use cuda:0\n",
      " 32%|███▏      | 95/300 [00:16<00:35,  5.78it/s]Device set to use cuda:0\n",
      " 32%|███▏      | 96/300 [00:16<00:34,  5.90it/s]Device set to use cuda:0\n",
      " 32%|███▏      | 97/300 [00:16<00:34,  5.97it/s]Device set to use cuda:0\n",
      " 33%|███▎      | 98/300 [00:17<00:33,  6.02it/s]Device set to use cuda:0\n",
      " 33%|███▎      | 99/300 [00:17<00:33,  6.05it/s]Device set to use cuda:0\n",
      " 33%|███▎      | 100/300 [00:17<00:32,  6.08it/s]Device set to use cuda:0\n",
      " 34%|███▎      | 101/300 [00:17<00:39,  5.05it/s]Device set to use cuda:0\n",
      " 34%|███▍      | 102/300 [00:17<00:36,  5.36it/s]Device set to use cuda:0\n",
      " 34%|███▍      | 103/300 [00:18<00:35,  5.60it/s]Device set to use cuda:0\n",
      " 35%|███▍      | 104/300 [00:18<00:35,  5.55it/s]Device set to use cuda:0\n",
      " 35%|███▌      | 105/300 [00:18<00:34,  5.70it/s]Device set to use cuda:0\n",
      " 35%|███▌      | 106/300 [00:18<00:33,  5.80it/s]Device set to use cuda:0\n",
      " 36%|███▌      | 107/300 [00:18<00:32,  5.88it/s]Device set to use cuda:0\n",
      " 36%|███▌      | 108/300 [00:18<00:32,  5.89it/s]Device set to use cuda:0\n",
      " 36%|███▋      | 109/300 [00:19<00:32,  5.92it/s]Device set to use cuda:0\n",
      " 37%|███▋      | 110/300 [00:19<00:31,  5.95it/s]Device set to use cuda:0\n",
      " 37%|███▋      | 111/300 [00:19<00:34,  5.46it/s]Device set to use cuda:0\n",
      " 37%|███▋      | 112/300 [00:19<00:33,  5.67it/s]Device set to use cuda:0\n",
      " 38%|███▊      | 113/300 [00:19<00:32,  5.68it/s]Device set to use cuda:0\n",
      " 38%|███▊      | 114/300 [00:20<00:39,  4.68it/s]Device set to use cuda:0\n",
      " 38%|███▊      | 115/300 [00:20<00:36,  5.04it/s]Device set to use cuda:0\n",
      " 39%|███▊      | 116/300 [00:20<00:36,  4.98it/s]Device set to use cuda:0\n",
      " 39%|███▉      | 117/300 [00:20<00:34,  5.28it/s]Device set to use cuda:0\n",
      " 39%|███▉      | 118/300 [00:20<00:33,  5.49it/s]Device set to use cuda:0\n",
      " 40%|███▉      | 119/300 [00:20<00:31,  5.67it/s]Device set to use cuda:0\n",
      " 40%|████      | 120/300 [00:21<00:31,  5.70it/s]Device set to use cuda:0\n",
      " 40%|████      | 121/300 [00:21<00:30,  5.86it/s]Device set to use cuda:0\n",
      " 41%|████      | 122/300 [00:21<00:30,  5.92it/s]Device set to use cuda:0\n",
      " 41%|████      | 123/300 [00:21<00:29,  5.98it/s]Device set to use cuda:0\n",
      " 41%|████▏     | 124/300 [00:21<00:29,  6.00it/s]Device set to use cuda:0\n",
      " 42%|████▏     | 125/300 [00:21<00:29,  6.03it/s]Device set to use cuda:0\n",
      " 42%|████▏     | 126/300 [00:22<00:28,  6.05it/s]Device set to use cuda:0\n",
      " 42%|████▏     | 127/300 [00:22<00:28,  6.10it/s]Device set to use cuda:0\n",
      " 43%|████▎     | 128/300 [00:22<00:28,  6.11it/s]Device set to use cuda:0\n",
      " 43%|████▎     | 129/300 [00:22<00:28,  6.08it/s]Device set to use cuda:0\n",
      " 43%|████▎     | 130/300 [00:22<00:28,  6.06it/s]Device set to use cuda:0\n",
      " 44%|████▎     | 131/300 [00:22<00:28,  5.92it/s]Device set to use cuda:0\n",
      " 44%|████▍     | 132/300 [00:23<00:28,  6.00it/s]Device set to use cuda:0\n",
      " 44%|████▍     | 133/300 [00:23<00:27,  6.07it/s]Device set to use cuda:0\n",
      " 45%|████▍     | 134/300 [00:23<00:27,  6.08it/s]Device set to use cuda:0\n",
      " 45%|████▌     | 135/300 [00:23<00:26,  6.13it/s]Device set to use cuda:0\n",
      " 45%|████▌     | 136/300 [00:23<00:26,  6.17it/s]Device set to use cuda:0\n",
      " 46%|████▌     | 137/300 [00:23<00:26,  6.18it/s]Device set to use cuda:0\n",
      " 46%|████▌     | 138/300 [00:24<00:26,  6.21it/s]Device set to use cuda:0\n",
      " 46%|████▋     | 139/300 [00:24<00:37,  4.26it/s]Device set to use cuda:0\n",
      " 47%|████▋     | 140/300 [00:24<00:34,  4.67it/s]Device set to use cuda:0\n",
      " 47%|████▋     | 141/300 [00:24<00:32,  4.94it/s]Device set to use cuda:0\n",
      " 47%|████▋     | 142/300 [00:24<00:29,  5.28it/s]Device set to use cuda:0\n",
      " 48%|████▊     | 143/300 [00:25<00:28,  5.47it/s]Device set to use cuda:0\n",
      " 48%|████▊     | 144/300 [00:25<00:28,  5.46it/s]Device set to use cuda:0\n",
      " 48%|████▊     | 145/300 [00:25<00:27,  5.65it/s]Device set to use cuda:0\n",
      " 49%|████▊     | 146/300 [00:25<00:26,  5.83it/s]Device set to use cuda:0\n",
      " 49%|████▉     | 147/300 [00:25<00:25,  5.96it/s]Device set to use cuda:0\n",
      " 49%|████▉     | 148/300 [00:25<00:25,  6.06it/s]Device set to use cuda:0\n",
      " 50%|████▉     | 149/300 [00:26<00:24,  6.10it/s]Device set to use cuda:0\n",
      " 50%|█████     | 150/300 [00:26<00:24,  6.13it/s]Device set to use cuda:0\n",
      " 50%|█████     | 151/300 [00:26<00:24,  6.10it/s]Device set to use cuda:0\n",
      " 51%|█████     | 152/300 [00:26<00:23,  6.17it/s]Device set to use cuda:0\n",
      " 51%|█████     | 153/300 [00:26<00:23,  6.22it/s]Device set to use cuda:0\n",
      " 51%|█████▏    | 154/300 [00:26<00:23,  6.22it/s]Device set to use cuda:0\n",
      " 52%|█████▏    | 155/300 [00:27<00:23,  6.22it/s]Device set to use cuda:0\n",
      " 52%|█████▏    | 156/300 [00:27<00:23,  6.22it/s]Device set to use cuda:0\n",
      " 52%|█████▏    | 157/300 [00:27<00:23,  6.20it/s]Device set to use cuda:0\n",
      " 53%|█████▎    | 158/300 [00:27<00:22,  6.21it/s]Device set to use cuda:0\n",
      " 53%|█████▎    | 159/300 [00:27<00:22,  6.19it/s]Device set to use cuda:0\n",
      " 53%|█████▎    | 160/300 [00:27<00:22,  6.21it/s]Device set to use cuda:0\n",
      " 54%|█████▎    | 161/300 [00:28<00:22,  6.22it/s]Device set to use cuda:0\n",
      " 54%|█████▍    | 162/300 [00:28<00:22,  6.20it/s]Device set to use cuda:0\n",
      " 54%|█████▍    | 163/300 [00:28<00:22,  6.13it/s]Device set to use cuda:0\n",
      " 55%|█████▍    | 164/300 [00:28<00:22,  6.10it/s]Device set to use cuda:0\n",
      " 55%|█████▌    | 165/300 [00:28<00:21,  6.16it/s]Device set to use cuda:0\n",
      " 55%|█████▌    | 166/300 [00:28<00:22,  6.01it/s]Device set to use cuda:0\n",
      " 56%|█████▌    | 167/300 [00:29<00:22,  5.98it/s]Device set to use cuda:0\n",
      " 56%|█████▌    | 168/300 [00:29<00:22,  5.92it/s]Device set to use cuda:0\n",
      " 56%|█████▋    | 169/300 [00:29<00:22,  5.85it/s]Device set to use cuda:0\n",
      " 57%|█████▋    | 170/300 [00:29<00:22,  5.90it/s]Device set to use cuda:0\n",
      " 57%|█████▋    | 171/300 [00:29<00:21,  5.97it/s]Device set to use cuda:0\n",
      " 57%|█████▋    | 172/300 [00:29<00:21,  6.05it/s]Device set to use cuda:0\n",
      " 58%|█████▊    | 173/300 [00:30<00:22,  5.72it/s]Device set to use cuda:0\n",
      " 58%|█████▊    | 174/300 [00:30<00:21,  5.84it/s]Device set to use cuda:0\n",
      " 58%|█████▊    | 175/300 [00:30<00:26,  4.72it/s]Device set to use cuda:0\n",
      " 59%|█████▊    | 176/300 [00:30<00:24,  5.00it/s]Device set to use cuda:0\n",
      " 59%|█████▉    | 177/300 [00:30<00:23,  5.29it/s]Device set to use cuda:0\n",
      " 59%|█████▉    | 178/300 [00:31<00:22,  5.54it/s]Device set to use cuda:0\n",
      " 60%|█████▉    | 179/300 [00:31<00:21,  5.62it/s]Device set to use cuda:0\n",
      " 60%|██████    | 180/300 [00:31<00:20,  5.78it/s]Device set to use cuda:0\n",
      " 60%|██████    | 181/300 [00:31<00:22,  5.41it/s]Device set to use cuda:0\n",
      " 61%|██████    | 182/300 [00:31<00:25,  4.58it/s]Device set to use cuda:0\n",
      " 61%|██████    | 183/300 [00:32<00:25,  4.56it/s]Device set to use cuda:0\n",
      " 61%|██████▏   | 184/300 [00:32<00:23,  4.91it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 185/300 [00:32<00:22,  5.22it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 186/300 [00:32<00:20,  5.45it/s]Device set to use cuda:0\n",
      " 62%|██████▏   | 187/300 [00:32<00:20,  5.53it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 188/300 [00:32<00:19,  5.61it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 189/300 [00:33<00:19,  5.76it/s]Device set to use cuda:0\n",
      " 63%|██████▎   | 190/300 [00:33<00:18,  5.87it/s]Device set to use cuda:0\n",
      " 64%|██████▎   | 191/300 [00:33<00:18,  5.94it/s]Device set to use cuda:0\n",
      " 64%|██████▍   | 192/300 [00:33<00:17,  6.04it/s]Device set to use cuda:0\n",
      " 64%|██████▍   | 193/300 [00:33<00:17,  6.14it/s]Device set to use cuda:0\n",
      " 65%|██████▍   | 194/300 [00:33<00:17,  6.18it/s]Device set to use cuda:0\n",
      " 65%|██████▌   | 195/300 [00:34<00:16,  6.22it/s]Device set to use cuda:0\n",
      " 65%|██████▌   | 196/300 [00:34<00:18,  5.75it/s]Device set to use cuda:0\n",
      " 66%|██████▌   | 197/300 [00:34<00:17,  5.88it/s]Device set to use cuda:0\n",
      " 66%|██████▌   | 198/300 [00:34<00:17,  5.97it/s]Device set to use cuda:0\n",
      " 66%|██████▋   | 199/300 [00:34<00:16,  6.02it/s]Device set to use cuda:0\n",
      " 67%|██████▋   | 200/300 [00:34<00:16,  6.05it/s]Device set to use cuda:0\n",
      " 67%|██████▋   | 201/300 [00:35<00:16,  6.06it/s]Device set to use cuda:0\n",
      " 67%|██████▋   | 202/300 [00:35<00:16,  6.04it/s]Device set to use cuda:0\n",
      " 68%|██████▊   | 203/300 [00:35<00:15,  6.08it/s]Device set to use cuda:0\n",
      " 68%|██████▊   | 204/300 [00:35<00:15,  6.15it/s]Device set to use cuda:0\n",
      " 68%|██████▊   | 205/300 [00:35<00:15,  6.18it/s]Device set to use cuda:0\n",
      " 69%|██████▊   | 206/300 [00:35<00:15,  5.94it/s]Device set to use cuda:0\n",
      " 69%|██████▉   | 207/300 [00:36<00:15,  5.97it/s]Device set to use cuda:0\n",
      " 69%|██████▉   | 208/300 [00:36<00:15,  5.88it/s]Device set to use cuda:0\n",
      " 70%|██████▉   | 209/300 [00:36<00:15,  5.95it/s]Device set to use cuda:0\n",
      " 70%|███████   | 210/300 [00:36<00:15,  5.98it/s]Device set to use cuda:0\n",
      " 70%|███████   | 211/300 [00:36<00:14,  6.06it/s]Device set to use cuda:0\n",
      " 71%|███████   | 212/300 [00:36<00:14,  6.12it/s]Device set to use cuda:0\n",
      " 71%|███████   | 213/300 [00:37<00:14,  6.12it/s]Device set to use cuda:0\n",
      " 71%|███████▏  | 214/300 [00:37<00:14,  6.14it/s]Device set to use cuda:0\n",
      " 72%|███████▏  | 215/300 [00:37<00:13,  6.17it/s]Device set to use cuda:0\n",
      " 72%|███████▏  | 216/300 [00:37<00:13,  6.17it/s]Device set to use cuda:0\n",
      " 72%|███████▏  | 217/300 [00:37<00:13,  6.20it/s]Device set to use cuda:0\n",
      " 73%|███████▎  | 218/300 [00:37<00:13,  6.24it/s]Device set to use cuda:0\n",
      " 73%|███████▎  | 219/300 [00:38<00:12,  6.24it/s]Device set to use cuda:0\n",
      " 73%|███████▎  | 220/300 [00:38<00:12,  6.26it/s]Device set to use cuda:0\n",
      " 74%|███████▎  | 221/300 [00:38<00:12,  6.25it/s]Device set to use cuda:0\n",
      " 74%|███████▍  | 222/300 [00:38<00:12,  6.22it/s]Device set to use cuda:0\n",
      " 74%|███████▍  | 223/300 [00:38<00:12,  6.24it/s]Device set to use cuda:0\n",
      " 75%|███████▍  | 224/300 [00:38<00:12,  6.25it/s]Device set to use cuda:0\n",
      " 75%|███████▌  | 225/300 [00:39<00:12,  5.98it/s]Device set to use cuda:0\n",
      " 75%|███████▌  | 226/300 [00:39<00:12,  5.98it/s]Device set to use cuda:0\n",
      " 76%|███████▌  | 227/300 [00:39<00:12,  5.81it/s]Device set to use cuda:0\n",
      " 76%|███████▌  | 228/300 [00:39<00:12,  5.89it/s]Device set to use cuda:0\n",
      " 76%|███████▋  | 229/300 [00:39<00:11,  5.92it/s]Device set to use cuda:0\n",
      " 77%|███████▋  | 230/300 [00:39<00:11,  5.93it/s]Device set to use cuda:0\n",
      " 77%|███████▋  | 231/300 [00:40<00:11,  5.96it/s]Device set to use cuda:0\n",
      " 77%|███████▋  | 232/300 [00:40<00:11,  6.07it/s]Device set to use cuda:0\n",
      " 78%|███████▊  | 233/300 [00:40<00:10,  6.12it/s]Device set to use cuda:0\n",
      " 78%|███████▊  | 234/300 [00:40<00:10,  6.11it/s]Device set to use cuda:0\n",
      " 78%|███████▊  | 235/300 [00:40<00:10,  6.15it/s]Device set to use cuda:0\n",
      " 79%|███████▊  | 236/300 [00:40<00:10,  6.19it/s]Device set to use cuda:0\n",
      " 79%|███████▉  | 237/300 [00:41<00:10,  6.22it/s]Device set to use cuda:0\n",
      " 79%|███████▉  | 238/300 [00:41<00:09,  6.27it/s]Device set to use cuda:0\n",
      " 80%|███████▉  | 239/300 [00:41<00:10,  6.08it/s]Device set to use cuda:0\n",
      " 80%|████████  | 240/300 [00:41<00:09,  6.09it/s]Device set to use cuda:0\n",
      " 80%|████████  | 241/300 [00:41<00:09,  5.92it/s]Device set to use cuda:0\n",
      " 81%|████████  | 242/300 [00:41<00:09,  6.00it/s]Device set to use cuda:0\n",
      " 81%|████████  | 243/300 [00:42<00:09,  6.07it/s]Device set to use cuda:0\n",
      " 81%|████████▏ | 244/300 [00:42<00:09,  6.10it/s]Device set to use cuda:0\n",
      " 82%|████████▏ | 245/300 [00:42<00:08,  6.14it/s]Device set to use cuda:0\n",
      " 82%|████████▏ | 246/300 [00:42<00:08,  6.11it/s]Device set to use cuda:0\n",
      " 82%|████████▏ | 247/300 [00:42<00:09,  5.86it/s]Device set to use cuda:0\n",
      " 83%|████████▎ | 248/300 [00:42<00:08,  5.78it/s]Device set to use cuda:0\n",
      " 83%|████████▎ | 249/300 [00:43<00:08,  5.91it/s]Device set to use cuda:0\n",
      " 83%|████████▎ | 250/300 [00:43<00:08,  6.03it/s]Device set to use cuda:0\n",
      " 84%|████████▎ | 251/300 [00:43<00:08,  6.09it/s]Device set to use cuda:0\n",
      " 84%|████████▍ | 252/300 [00:43<00:07,  6.06it/s]Device set to use cuda:0\n",
      " 84%|████████▍ | 253/300 [00:43<00:07,  5.93it/s]Device set to use cuda:0\n",
      " 85%|████████▍ | 254/300 [00:43<00:07,  5.87it/s]Device set to use cuda:0\n",
      " 85%|████████▌ | 255/300 [00:44<00:07,  5.87it/s]Device set to use cuda:0\n",
      " 85%|████████▌ | 256/300 [00:44<00:07,  5.93it/s]Device set to use cuda:0\n",
      " 86%|████████▌ | 257/300 [00:44<00:07,  5.87it/s]Device set to use cuda:0\n",
      " 86%|████████▌ | 258/300 [00:44<00:07,  5.94it/s]Device set to use cuda:0\n",
      " 86%|████████▋ | 259/300 [00:44<00:06,  6.04it/s]Device set to use cuda:0\n",
      " 87%|████████▋ | 260/300 [00:44<00:06,  5.94it/s]Device set to use cuda:0\n",
      " 87%|████████▋ | 261/300 [00:45<00:06,  6.03it/s]Device set to use cuda:0\n",
      " 87%|████████▋ | 262/300 [00:45<00:06,  6.07it/s]Device set to use cuda:0\n",
      " 88%|████████▊ | 263/300 [00:45<00:06,  5.37it/s]Device set to use cuda:0\n",
      " 88%|████████▊ | 264/300 [00:45<00:06,  5.59it/s]Device set to use cuda:0\n",
      " 88%|████████▊ | 265/300 [00:45<00:06,  5.78it/s]Device set to use cuda:0\n",
      " 89%|████████▊ | 266/300 [00:45<00:05,  5.75it/s]Device set to use cuda:0\n",
      " 89%|████████▉ | 267/300 [00:46<00:06,  5.43it/s]Device set to use cuda:0\n",
      " 89%|████████▉ | 268/300 [00:46<00:05,  5.65it/s]Device set to use cuda:0\n",
      " 90%|████████▉ | 269/300 [00:46<00:05,  5.78it/s]Device set to use cuda:0\n",
      " 90%|█████████ | 270/300 [00:46<00:05,  5.93it/s]Device set to use cuda:0\n",
      " 90%|█████████ | 271/300 [00:46<00:04,  6.06it/s]Device set to use cuda:0\n",
      " 91%|█████████ | 272/300 [00:46<00:04,  6.16it/s]Device set to use cuda:0\n",
      " 91%|█████████ | 273/300 [00:47<00:04,  6.21it/s]Device set to use cuda:0\n",
      " 91%|█████████▏| 274/300 [00:47<00:04,  6.27it/s]Device set to use cuda:0\n",
      " 92%|█████████▏| 275/300 [00:47<00:03,  6.25it/s]Device set to use cuda:0\n",
      " 92%|█████████▏| 276/300 [00:47<00:03,  6.06it/s]Device set to use cuda:0\n",
      " 92%|█████████▏| 277/300 [00:47<00:03,  5.97it/s]Device set to use cuda:0\n",
      " 93%|█████████▎| 278/300 [00:47<00:03,  6.08it/s]Device set to use cuda:0\n",
      " 93%|█████████▎| 279/300 [00:48<00:03,  5.58it/s]Device set to use cuda:0\n",
      " 93%|█████████▎| 280/300 [00:48<00:03,  5.77it/s]Device set to use cuda:0\n",
      " 94%|█████████▎| 281/300 [00:48<00:03,  5.90it/s]Device set to use cuda:0\n",
      " 94%|█████████▍| 282/300 [00:48<00:02,  6.00it/s]Device set to use cuda:0\n",
      " 94%|█████████▍| 283/300 [00:48<00:02,  6.06it/s]Device set to use cuda:0\n",
      " 95%|█████████▍| 284/300 [00:48<00:02,  6.08it/s]Device set to use cuda:0\n",
      " 95%|█████████▌| 285/300 [00:49<00:02,  6.15it/s]Device set to use cuda:0\n",
      " 95%|█████████▌| 286/300 [00:49<00:02,  6.16it/s]Device set to use cuda:0\n",
      " 96%|█████████▌| 287/300 [00:49<00:02,  6.19it/s]Device set to use cuda:0\n",
      " 96%|█████████▌| 288/300 [00:49<00:01,  6.15it/s]Device set to use cuda:0\n",
      " 96%|█████████▋| 289/300 [00:50<00:03,  3.31it/s]Device set to use cuda:0\n",
      " 97%|█████████▋| 290/300 [00:50<00:02,  3.85it/s]Device set to use cuda:0\n",
      " 97%|█████████▋| 291/300 [00:50<00:02,  4.33it/s]Device set to use cuda:0\n",
      " 97%|█████████▋| 292/300 [00:50<00:01,  4.80it/s]Device set to use cuda:0\n",
      " 98%|█████████▊| 293/300 [00:50<00:01,  5.17it/s]Device set to use cuda:0\n",
      " 98%|█████████▊| 294/300 [00:51<00:01,  5.47it/s]Device set to use cuda:0\n",
      " 98%|█████████▊| 295/300 [00:51<00:00,  5.71it/s]Device set to use cuda:0\n",
      " 99%|█████████▊| 296/300 [00:51<00:00,  5.90it/s]Device set to use cuda:0\n",
      " 99%|█████████▉| 297/300 [00:51<00:00,  5.96it/s]Device set to use cuda:0\n",
      " 99%|█████████▉| 298/300 [00:51<00:00,  6.02it/s]Device set to use cuda:0\n",
      "100%|█████████▉| 299/300 [00:51<00:00,  5.80it/s]Device set to use cuda:0\n",
      "100%|██████████| 300/300 [00:52<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.810\n",
      "Accuracy for label Normal: 0.937\n",
      "Accuracy for label Depression: 0.870\n",
      "Accuracy for label Anxiety: 0.222\n",
      "Accuracy for label Bipolar: 0.200\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.86      0.94      0.90       143\n",
      "  Depression       0.76      0.87      0.81       115\n",
      "     Anxiety       0.75      0.22      0.34        27\n",
      "     Bipolar       1.00      0.20      0.33        15\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       300\n",
      "   macro avg       0.84      0.56      0.60       300\n",
      "weighted avg       0.82      0.81      0.79       300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[134   8   0   0]\n",
      " [ 13 100   2   0]\n",
      " [  7  14   6   0]\n",
      " [  2  10   0   3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:32:31.829272Z",
     "iopub.status.busy": "2025-06-07T13:32:31.828459Z",
     "iopub.status.idle": "2025-06-07T13:32:32.025450Z",
     "shell.execute_reply": "2025-06-07T13:32:32.024685Z",
     "shell.execute_reply.started": "2025-06-07T13:32:31.829244Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load your Hugging Face token from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:32:32.027181Z",
     "iopub.status.busy": "2025-06-07T13:32:32.026479Z",
     "iopub.status.idle": "2025-06-07T13:32:32.030147Z",
     "shell.execute_reply": "2025-06-07T13:32:32.029402Z",
     "shell.execute_reply.started": "2025-06-07T13:32:32.027161Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = base_model_name\n",
    "fine_tuned_model = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:32:32.031087Z",
     "iopub.status.busy": "2025-06-07T13:32:32.030878Z",
     "iopub.status.idle": "2025-06-07T13:32:33.538145Z",
     "shell.execute_reply": "2025-06-07T13:32:33.537274Z",
     "shell.execute_reply.started": "2025-06-07T13:32:32.031072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:32:33.540700Z",
     "iopub.status.busy": "2025-06-07T13:32:33.539948Z",
     "iopub.status.idle": "2025-06-07T13:32:34.287699Z",
     "shell.execute_reply": "2025-06-07T13:32:34.287169Z",
     "shell.execute_reply.started": "2025-06-07T13:32:33.540678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:32:34.288737Z",
     "iopub.status.busy": "2025-06-07T13:32:34.288467Z",
     "iopub.status.idle": "2025-06-07T13:32:34.436777Z",
     "shell.execute_reply": "2025-06-07T13:32:34.436056Z",
     "shell.execute_reply.started": "2025-06-07T13:32:34.288713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression\n"
     ]
    }
   ],
   "source": [
    "cobol_code = \"\"\"\n",
    "       IDENTIFICATION DIVISION.\n",
    "       PROGRAM-ID. HELLO.\n",
    "       PROCEDURE DIVISION.\n",
    "           DISPLAY 'HELLO, WORLD'.\n",
    "           STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:39:44.857873Z",
     "iopub.status.busy": "2025-06-07T13:39:44.857269Z",
     "iopub.status.idle": "2025-06-07T13:39:44.861032Z",
     "shell.execute_reply": "2025-06-07T13:39:44.860266Z",
     "shell.execute_reply.started": "2025-06-07T13:39:44.857851Z"
    }
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# folder_path = \"/kaggle/working/llama-3.2-fine-tuned-model\"\n",
    "\n",
    "# if os.path.exists(folder_path):\n",
    "#     shutil.rmtree(folder_path)\n",
    "#     print(\"Folder removed.\")\n",
    "# else:\n",
    "#     print(\"Folder does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:39:57.124686Z",
     "iopub.status.busy": "2025-06-07T13:39:57.124140Z",
     "iopub.status.idle": "2025-06-07T13:40:03.689884Z",
     "shell.execute_reply": "2025-06-07T13:40:03.689231Z",
     "shell.execute_reply.started": "2025-06-07T13:39:57.124668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-1B-Instruct-Mental-Health-Classification/tokenizer_config.json',\n",
       " 'Llama-3.1-1B-Instruct-Mental-Health-Classification/special_tokens_map.json',\n",
       " 'Llama-3.1-1B-Instruct-Mental-Health-Classification/tokenizer.json')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:40:06.910035Z",
     "iopub.status.busy": "2025-06-07T13:40:06.909330Z",
     "iopub.status.idle": "2025-06-07T13:40:06.913784Z",
     "shell.execute_reply": "2025-06-07T13:40:06.912925Z",
     "shell.execute_reply.started": "2025-06-07T13:40:06.909998Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import CommitInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:40:12.291814Z",
     "iopub.status.busy": "2025-06-07T13:40:12.291521Z",
     "iopub.status.idle": "2025-06-07T13:41:13.096699Z",
     "shell.execute_reply": "2025-06-07T13:41:13.095935Z",
     "shell.execute_reply.started": "2025-06-07T13:40:12.291792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776c0476e72e43c583aa6c4e35a073f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ded6c4aadc14d92b303952598f55c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7be558653f4e529f21bd1528b3145c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dhirajpatra/Llama-3.1-1B-Instruct-Mental-Health-Classification/commit/d6402d8e3ac2ed34b90c951ed59d9eddc5c7135d', commit_message='Upload tokenizer', commit_description='', oid='d6402d8e3ac2ed34b90c951ed59d9eddc5c7135d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dhirajpatra/Llama-3.1-1B-Instruct-Mental-Health-Classification', endpoint='https://huggingface.co', repo_type='model', repo_id='dhirajpatra/Llama-3.1-1B-Instruct-Mental-Health-Classification'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(model_dir, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(model_dir, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:42:38.413309Z",
     "iopub.status.busy": "2025-06-07T13:42:38.412590Z",
     "iopub.status.idle": "2025-06-07T13:43:08.470372Z",
     "shell.execute_reply": "2025-06-07T13:43:08.469552Z",
     "shell.execute_reply.started": "2025-06-07T13:42:38.413282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057bf0bf84b740869d20adbed39bb40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c564deae27643cca161c90dafff520b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1408cff317a4f569279a5fb1ea74785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a81b50d946849f7ac402847dddd5df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d2528f55ec46e8a4959513b3e94747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26caf7e5a7594cd692873d81b6d13cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dhirajpatra/\" + model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dhirajpatra/\" + model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:45:15.487190Z",
     "iopub.status.busy": "2025-06-07T13:45:15.486670Z",
     "iopub.status.idle": "2025-06-07T13:45:20.082693Z",
     "shell.execute_reply": "2025-06-07T13:45:20.081918Z",
     "shell.execute_reply.started": "2025-06-07T13:45:15.487166Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"dhirajpatra/\" + model_dir\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T13:46:40.125525Z",
     "iopub.status.busy": "2025-06-07T13:46:40.124967Z",
     "iopub.status.idle": "2025-06-07T13:46:41.845947Z",
     "shell.execute_reply": "2025-06-07T13:46:41.845305Z",
     "shell.execute_reply.started": "2025-06-07T13:46:40.125506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression\n"
     ]
    }
   ],
   "source": [
    "cobol_code = \"\"\"\n",
    "       IDENTIFICATION DIVISION.\n",
    "       PROGRAM-ID. HELLO.\n",
    "       PROCEDURE DIVISION.\n",
    "           DISPLAY 'HELLO, WORLD'.\n",
    "           STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5338273,
     "sourceId": 8870083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
