{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version first\n",
    "!nvcc --version\n",
    "\n",
    "# For CUDA 12.1+ (more likely on your server)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core libraries\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "pip install peft\n",
    "pip install trl\n",
    "pip install bitsandbytes\n",
    "pip install accelerate\n",
    "\n",
    "# Flash attention (improved installation)\n",
    "pip install packaging wheel\n",
    "pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Tokenization and data handling\n",
    "pip install sentencepiece\n",
    "pip install protobuf\n",
    "\n",
    "# Evaluation metrics\n",
    "pip install evaluate\n",
    "pip install rouge-score\n",
    "pip install sacrebleu\n",
    "\n",
    "# Monitoring options\n",
    "pip install wandb\n",
    "pip install tensorboard\n",
    "pip install pynvml\n",
    "pip install psutil\n",
    "\n",
    "# TensorRT optimization\n",
    "pip install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Ensure GPU is available\n",
    "assert torch.cuda.is_available(), \"CUDA GPU not available.\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Print GPU info for verification\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# GPU optimization settings for RTX 4090\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Optimize for RTX 4090\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster training\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Clear cache at start\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory fraction to avoid OOM (optional - adjust based on your needs)\n",
    "# torch.cuda.set_per_process_memory_fraction(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Fine-tuning Pipeline for CodeLlama COBOL to Python Conversion\n",
    "Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n",
    "\"\"\"\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    CodeLlamaTokenizer,  # Specific for CodeLlama\n",
    "    LlamaForCausalLM     # Specific for CodeLlama\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Monitoring and optimization\n",
    "import wandb\n",
    "import tensorrt as trt\n",
    "import pynvml\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA optimizations for RTX 4090\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,roundup_power2_divisions:16\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Async kernel launches for speed\n",
    "\n",
    "# RTX 4090 specific optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Ada Lovelace TF32 support\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"codellama/CodeLlama-7B-Instruct-hf\"\n",
    "output_model = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "\n",
    "# Training configuration\n",
    "max_seq_length = 2048  # Adjust based on your COBOL/Python code lengths\n",
    "batch_size = 4         # Start with 4, can increase with 24GB VRAM\n",
    "gradient_accumulation_steps = 4  # Effective batch size = 16\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(f\"Output will be saved as: {output_model}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Core packages for fine-tuning\n",
    "%pip install -U -q transformers datasets peft accelerate huggingface_hub trl\n",
    "\n",
    "# Quantization and optimization\n",
    "%pip install -U -q bitsandbytes optimum auto-gptq\n",
    "\n",
    "# Memory and attention optimization\n",
    "%pip install -U -q xformers --no-deps\n",
    "\n",
    "# Flash attention for better performance (if not already installed)\n",
    "%pip install -U -q flash-attn --no-build-isolation\n",
    "\n",
    "# Evaluation metrics for code conversion\n",
    "%pip install -U -q evaluate rouge-score sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCodeLlamaFineTuner:\n",
    "    \"\"\"\n",
    "    A comprehensive class designed for efficient fine-tuning of CodeLlama models,\n",
    "    specifically optimized for tasks like COBOL to Python code translation.\n",
    "\n",
    "    This class leverages cutting-edge techniques such as Parameter-Efficient Fine-Tuning\n",
    "    (PEFT) using LoRA (Low-Rank Adaptation) and 4-bit quantization with bitsandbytes\n",
    "    to enable training large language models on consumer-grade GPUs or environments\n",
    "    with limited memory.\n",
    "\n",
    "    Key Features:\n",
    "    -   **Memory-Efficient Model Loading**: Supports loading CodeLlama models in 4-bit\n",
    "        quantization (`bnb.nn.Linear4bit`) with configurable double quantization\n",
    "        and compute data types (e.g., bfloat16 for RTX 40 series GPUs).\n",
    "    -   **LoRA Integration**: Seamlessly configures and applies LoRA adapters to\n",
    "        target specific linear layers (e.g., query, key, value projections) for\n",
    "        efficient fine-tuning without modifying the full model weights.\n",
    "    -   **Automated Target Module Detection**: Can automatically identify all\n",
    "        4-bit linear layers for LoRA application if not explicitly specified.\n",
    "    -   **Dataset Handling**: Facilitates loading and preprocessing of custom datasets,\n",
    "        supporting instruction-tuning formats (e.g., \"### Instruction:\\n...### Response:\\n\").\n",
    "    -   **Optimized Training Arguments**: Sets up `transformers.TrainingArguments`\n",
    "        with best practices for memory efficiency (gradient accumulation, gradient\n",
    "        checkpointing, paged optimizers, bfloat16/fp16 precision, mixed-precision training).\n",
    "    -   **Supervised Fine-Tuning (SFT)**: Utilizes `trl.SFTTrainer` for streamlined\n",
    "        supervised fine-tuning, handling data collators, tokenization, and training loops.\n",
    "    -   **Flexible Evaluation**: Provides methods for generating predictions from the\n",
    "        fine-tuned model and calculating key evaluation metrics like Exact Match Accuracy\n",
    "        and Average Similarity Score (using SequenceMatcher).\n",
    "    -   **Model Management**: Supports saving LoRA adapters, merging them with the\n",
    "        base model for a standalone full model, and pushing the final model/tokenizer\n",
    "        to the Hugging Face Hub for easy sharing and deployment.\n",
    "    -   **CUDA Memory Optimization**: Includes environment variable settings to help\n",
    "        avoid CUDA memory fragmentation, improving stability during training.\n",
    "\n",
    "    Usage Workflow:\n",
    "    1.  **Initialization**: Instantiate `OptimizedCodeLlamaFineTuner` with a comprehensive\n",
    "        configuration dictionary (`config_dict`) specifying model, LoRA, and training parameters.\n",
    "    2.  **Load Model & Tokenizer**: Call `load_model_and_tokenizer()` to prepare the base model.\n",
    "    3.  **Setup LoRA**: Invoke `setup_lora()` to apply PEFT adapters to the model.\n",
    "    4.  **Load Datasets**: Use `load_your_specific_datasets()` to prepare your training and\n",
    "        evaluation data.\n",
    "    5.  **Create Trainer**: Call `create_and_run_trainer()` to get an initialized\n",
    "        `SFTTrainer` instance.\n",
    "    6.  **Train**: Execute `trainer.train()` to start the fine-tuning process.\n",
    "    7.  **Evaluate**: Use `evaluate_model()` to generate predictions and\n",
    "        `calculate_and_print_metrics()` to assess performance.\n",
    "    8.  **Merge & Save**: Merge the LoRA adapters into the base model using `PeftModel.from_pretrained`\n",
    "        and `merge_and_unload()`, then save the final model with `save_pretrained()`\n",
    "        or push to the Hugging Face Hub with `push_to_hub()`.\n",
    "\n",
    "    Parameters:\n",
    "        config (dict): A dictionary containing all necessary configuration parameters\n",
    "                       for model loading, LoRA setup, and training. Example parameters\n",
    "                       include `model_name`, `load_in_4bit`, `lora_r`, `lora_alpha`,\n",
    "                       `num_train_epochs`, `per_device_train_batch_size`, `output_dir`, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_nvidia_environment()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.datasets = []  # Support for multiple datasets\n",
    "        self.combined_dataset = None\n",
    "        \n",
    "    def setup_nvidia_environment(self):\n",
    "        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available!\")\n",
    "            \n",
    "        # Initialize NVML for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        \n",
    "        print(f\"NVIDIA Setup:\")\n",
    "        print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   - Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode()\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n",
    "            \n",
    "        # Enable TensorFloat-32 for RTX 4090 (Ada Lovelace)\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"   - TensorFloat-32 enabled for RTX 4090\")\n",
    "            \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load CodeLlama 7B with optimized quantization\"\"\"\n",
    "        \n",
    "        # BitsAndBytesConfig for 4-bit quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        print(\"Loading CodeLlama tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True,  # Important for code generation\n",
    "            add_bos_token=True\n",
    "        )\n",
    "        \n",
    "        # Set special tokens for code tasks\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "            \n",
    "        print(\"Loading CodeLlama model with 4-bit quantization...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\" if self.config.get(\"use_flash_attention\", True) else \"eager\"\n",
    "        )\n",
    "        \n",
    "        # Prepare model for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        print(f\"CodeLlama model loaded on: {self.model.device}\")\n",
    "        \n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA configuration for CodeLlama fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get(\"lora_r\", 16),\n",
    "            lora_alpha=self.config.get(\"lora_alpha\", 32),\n",
    "            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n",
    "            # CodeLlama specific target modules\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"LoRA Configuration:\")\n",
    "        print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   - Total parameters: {total_params:,}\")\n",
    "        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "        \n",
    "    def load_your_specific_datasets(self):\n",
    "        \"\"\"Load and prepare your specific datasets for COBOL to Python conversion\"\"\"\n",
    "        print(\"Loading your specific datasets...\")\n",
    "        \n",
    "        # Load your datasets you can change the %\n",
    "        print(\"Loading MainframeBench COBOL dataset...\")\n",
    "        mainframe = load_dataset(\"Fsoft-AIC/MainframeBench\", \"COBOL_code_summarization\", split=\"train\")\n",
    "\n",
    "        # the-stack dataset more than 3 TB of data\n",
    "        print(\"Loading COBOL from The Stack...\")\n",
    "        stack_cobol = load_dataset(\"bigcode/the-stack\", data_dir=\"data/cobol\", split=\"train[:20%]\")\n",
    "        \n",
    "        print(\"Loading Python datasets...\")\n",
    "        stack_python = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", split=\"train[:20%]\")\n",
    "        python_set = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "        \n",
    "        # Combine Python datasets\n",
    "        python_combined = concatenate_datasets([stack_python, python_set])\n",
    "        \n",
    "        # Prepare datasets for training\n",
    "        all_datasets = []\n",
    "        \n",
    "        # 1. MainframeBench - COBOL code summarization (convert to instruction format)\n",
    "        mainframe_formatted = self.format_mainframe_dataset(mainframe)\n",
    "        all_datasets.append(mainframe_formatted)\n",
    "        \n",
    "        # 2. COBOL understanding dataset (from The Stack)\n",
    "        cobol_formatted = self.format_cobol_understanding_dataset(stack_cobol)\n",
    "        all_datasets.append(cobol_formatted)\n",
    "        \n",
    "        # 3. Python generation dataset (teach Python syntax)\n",
    "        python_formatted = self.format_python_teaching_dataset(python_combined)\n",
    "        all_datasets.append(python_formatted)\n",
    "        \n",
    "        # Combine all datasets\n",
    "        self.combined_dataset = concatenate_datasets(all_datasets)\n",
    "        \n",
    "        # Filter by length and sample if needed\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        self.combined_dataset = self.combined_dataset.filter(\n",
    "            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"max_samples\"):\n",
    "            self.combined_dataset = self.combined_dataset.select(\n",
    "                range(min(len(self.combined_dataset), self.config[\"max_samples\"]))\n",
    "            )\n",
    "            \n",
    "        print(f\"Combined dataset prepared: {len(self.combined_dataset)} samples\")\n",
    "        \n",
    "    def format_mainframe_dataset(self, dataset):\n",
    "        \"\"\"Format MainframeBench dataset for COBOL understanding\"\"\"\n",
    "        def format_mainframe(examples):\n",
    "            texts = []\n",
    "            for code, summary in zip(examples['code'], examples['summary']):\n",
    "                prompt = f\"\"\"### Task: Analyze and explain the following COBOL code\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{code.strip()}\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "{summary.strip()}\n",
    "\n",
    "### Convert to Python equivalent:\n",
    "```python\n",
    "# Python equivalent would be:\n",
    "# This COBOL code performs: {summary.strip()}\n",
    "```\"\"\"\n",
    "                texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        return dataset.map(format_mainframe, batched=True, remove_columns=dataset.column_names)\n",
    "    \n",
    "    def format_cobol_understanding_dataset(self, dataset):\n",
    "        \"\"\"Format COBOL dataset from The Stack for understanding\"\"\"\n",
    "        def format_cobol(examples):\n",
    "            texts = []\n",
    "            for content in examples['content']:\n",
    "                # Skip very short or very long files\n",
    "                if len(content.strip()) < 100 or len(content.strip()) > 5000:\n",
    "                    continue\n",
    "                \n",
    "                prompt = f\"\"\"### Task: Understand this COBOL code and suggest Python equivalent structure\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{content.strip()}\n",
    "```\n",
    "\n",
    "### Analysis:\n",
    "This COBOL program demonstrates typical mainframe programming patterns. \n",
    "\n",
    "### Python Structure:\n",
    "```python\n",
    "# Python equivalent structure would involve:\n",
    "# - Converting COBOL divisions to Python modules/classes\n",
    "# - Replacing COBOL data structures with Python equivalents\n",
    "# - Converting COBOL procedures to Python functions\n",
    "```\"\"\"\n",
    "                texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        formatted = dataset.map(format_cobol, batched=True, remove_columns=dataset.column_names)\n",
    "        # Take a subset to avoid overwhelming the model\n",
    "        return formatted.select(range(min(10000, len(formatted))))\n",
    "    \n",
    "    def format_python_teaching_dataset(self, dataset):\n",
    "        \"\"\"Format Python dataset to teach Python syntax and patterns\"\"\"\n",
    "        def format_python(examples):\n",
    "            texts = []\n",
    "            for content in examples['content']:\n",
    "                # Skip very short or very long files\n",
    "                if len(content.strip()) < 50 or len(content.strip()) > 3000:\n",
    "                    continue\n",
    "                \n",
    "                # Focus on clean, well-structured Python code\n",
    "                if any(keyword in content.lower() for keyword in ['class ', 'def ', 'import ', 'for ', 'if ']):\n",
    "                    prompt = f\"\"\"### Task: Learn Python programming patterns\n",
    "\n",
    "### Python Code:\n",
    "```python\n",
    "{content.strip()}\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "This Python code demonstrates modern programming practices that can be used when converting from COBOL.\"\"\"\n",
    "                    texts.append(prompt)\n",
    "            return {\"text\": texts}\n",
    "            \n",
    "        formatted = dataset.map(format_python, batched=True, remove_columns=dataset.column_names)\n",
    "        # Take a subset focused on quality code\n",
    "        return formatted.select(range(min(15000, len(formatted))))\n",
    "        \n",
    "    def setup_training_arguments(self):\n",
    "        \"\"\"Setup optimized training arguments for RTX 4090\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.get(\"output_dir\", \"./\" + output_model),\n",
    "            \n",
    "            # Training hyperparameters optimized for code generation\n",
    "            num_train_epochs=self.config.get(\"num_epochs\", 3),\n",
    "            per_device_train_batch_size=self.config.get(\"batch_size\", 2),  # Reduced for code tasks\n",
    "            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 8),  # Increased\n",
    "            learning_rate=self.config.get(\"learning_rate\", 1e-4),  # Lower for code tasks\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.05,\n",
    "            weight_decay=0.01,\n",
    "            \n",
    "            # NVIDIA RTX 4090 optimizations\n",
    "            bf16=True,  # Use bfloat16 for Ada Lovelace\n",
    "            tf32=True,  # Enable TensorFloat-32\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=4,\n",
    "            \n",
    "            # Memory optimizations for long code sequences\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",  # Fused optimizer for NVIDIA GPUs\n",
    "            max_grad_norm=1.0,\n",
    "            \n",
    "            # Logging and saving\n",
    "            logging_steps=25,\n",
    "            save_steps=500,\n",
    "            save_total_limit=2,\n",
    "            evaluation_strategy=\"no\",  # Disable eval to save memory\n",
    "            \n",
    "            # Additional optimizations\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n",
    "            run_name=f\"codellama-cobol-python-{self.config.get('experiment_name', 'default')}\",\n",
    "            \n",
    "            # Prevent OOM with long sequences\n",
    "            dataloader_drop_last=True,\n",
    "            \n",
    "            # DDP settings (single GPU for now)\n",
    "            ddp_find_unused_parameters=False,\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Execute the fine-tuning process\"\"\"\n",
    "        print(\"üöÄ Starting CodeLlama COBOL‚ÜíPython fine-tuning...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get(\"use_wandb\", False):\n",
    "            wandb.init(\n",
    "                project=self.config.get(\"wandb_project\", \"codellama-cobol-python\"),\n",
    "                name=f\"codellama-{self.config.get('experiment_name', 'default')}\"\n",
    "            )\n",
    "            \n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_arguments()\n",
    "        \n",
    "        # Data collator for code generation\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    "        )\n",
    "        \n",
    "        # Initialize SFTTrainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.combined_dataset,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.config.get(\"max_length\", 2048),\n",
    "            packing=False,  # Don't pack for code conversion tasks\n",
    "        )\n",
    "        \n",
    "        # Clear cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Monitor initial GPU usage\n",
    "        self.monitor_gpu_usage()\n",
    "        \n",
    "        # Start training\n",
    "        print(\"üéØ Training started...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"üíæ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(\"‚úÖ CodeLlama COBOL‚ÜíPython fine-tuning completed!\")\n",
    "        \n",
    "    def evaluate_model(self, test_samples=None):\n",
    "        \"\"\"Evaluate the fine-tuned model on COBOL to Python conversion\"\"\"\n",
    "        if test_samples is None:\n",
    "            # Use a small subset for quick evaluation\n",
    "            test_samples = self.combined_dataset.select(range(min(10, len(self.combined_dataset))))\n",
    "            \n",
    "        print(\"üîç Evaluating model performance...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sample in test_samples:\n",
    "                # Extract COBOL code from the sample\n",
    "                text = sample['text']\n",
    "                cobol_start = text.find('```cobol') + 8\n",
    "                cobol_end = text.find('```', cobol_start)\n",
    "                cobol_code = text[cobol_start:cobol_end].strip()\n",
    "                \n",
    "                # Generate Python code\n",
    "                prompt = f\"\"\"### Task: Convert the following COBOL code to Python\n",
    "\n",
    "### COBOL Code:\n",
    "```cobol\n",
    "{cobol_code}\n",
    "```\n",
    "\n",
    "### Python Code:\n",
    "```python\"\"\"\n",
    "                \n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                python_code = generated.split('```python')[-1].split('```')[0].strip()\n",
    "                \n",
    "                results.append({\n",
    "                    'cobol_code': cobol_code,\n",
    "                    'generated_python': python_code\n",
    "                })\n",
    "                \n",
    "        print(f\"‚úÖ Evaluated {len(results)} samples\")\n",
    "        return results\n",
    "        \n",
    "    def monitor_gpu_usage(self):\n",
    "        \"\"\"Monitor GPU usage during training\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved, {total_memory:.2f}GB total\")\n",
    "\n",
    "    def _extract_true_python_code(self, sample_text):\n",
    "        \"\"\"\n",
    "        Helper to extract the true Python code from the 'text' field of a dataset sample.\n",
    "        This needs to be robust to the different formatting styles you used\n",
    "        across the combined datasets.\n",
    "        \"\"\"\n",
    "        # This is the same helper function as in Option 1\n",
    "        python_code_start_marker_1 = \"### Python Code:\\n```python\"\n",
    "        python_code_start_marker_2 = \"### Python Structure:\\n```python\"\n",
    "        python_code_start_marker_3 = \"### Convert to Python equivalent:\\n```python\"\n",
    "        python_end_marker = \"```\"\n",
    "\n",
    "        start_index = -1\n",
    "        if python_code_start_marker_1 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_1) + len(python_code_start_marker_1)\n",
    "        elif python_code_start_marker_2 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_2) + len(python_code_start_marker_2)\n",
    "        elif python_code_start_marker_3 in sample_text:\n",
    "            start_index = sample_text.find(python_code_start_marker_3) + len(python_code_start_marker_3)\n",
    "\n",
    "        if start_index != -1:\n",
    "            end_index = sample_text.find(python_end_marker, start_index)\n",
    "            if end_index != -1:\n",
    "                return sample_text[start_index:end_index].strip()\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def calculate_and_print_metrics(self, test_dataset, predictions, num_examples_to_show=3):\n",
    "        \"\"\"\n",
    "        Calculates and prints evaluation metrics for code translation.\n",
    "\n",
    "        Args:\n",
    "            test_dataset (Dataset): The Hugging Face Dataset used for testing.\n",
    "            predictions (list): A list of generated Python code strings (y_pred).\n",
    "        \"\"\"\n",
    "        if not isinstance(test_dataset, Dataset):\n",
    "            raise TypeError(\"test_dataset must be a Hugging Face Dataset object.\")\n",
    "\n",
    "        if len(test_dataset) != len(predictions):\n",
    "            raise ValueError(\"Mismatch in number of samples between test_dataset and predictions.\")\n",
    "\n",
    "        # Extract ground truth (y_true)\n",
    "        y_true = [self._extract_true_python_code(sample[\"text\"]) for sample in test_dataset]\n",
    "\n",
    "        # Filter out samples where true_code could not be extracted\n",
    "        valid_pairs = [(gt, pred) for gt, pred in zip(y_true, predictions) if gt]\n",
    "\n",
    "        if not valid_pairs:\n",
    "            print(\"‚ùó No valid ground truth Python code found in the test dataset for evaluation metrics.\")\n",
    "            return {}\n",
    "\n",
    "        y_true_filtered = [pair[0] for pair in valid_pairs]\n",
    "        y_pred_filtered = [pair[1] for pair in valid_pairs]\n",
    "\n",
    "        print(f\"Evaluating metrics on {len(y_true_filtered)} valid samples.\")\n",
    "\n",
    "        def code_similarity(a, b):\n",
    "            return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n",
    "\n",
    "        similarities = [code_similarity(gt, pred) for gt, pred in zip(y_true_filtered, y_pred_filtered)]\n",
    "        avg_similarity = np.mean(similarities)\n",
    "\n",
    "        exact_matches = sum(1 for gt, pred in zip(y_true_filtered, y_pred_filtered) if gt.strip() == pred.strip())\n",
    "        accuracy = exact_matches / len(y_true_filtered)\n",
    "\n",
    "        print(f\"\\n--- Evaluation Metrics ---\")\n",
    "        print(f\"Exact Match Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Average Similarity Score: {avg_similarity:.3f}\")\n",
    "\n",
    "        print(f\"\\n--- Sample Outputs ({min(num_examples_to_show, len(y_true_filtered))} examples) ---\")\n",
    "        for i in range(min(num_examples_to_show, len(y_true_filtered))):\n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            print(\"True Output:\\n\", y_true_filtered[i])\n",
    "            print(\"Predicted Output:\\n\", y_pred_filtered[i])\n",
    "            print(\"Similarity Score:\", code_similarity(y_true_filtered[i], y_pred_filtered[i]))\n",
    "\n",
    "        return {\n",
    "            \"exact_match_accuracy\": accuracy,\n",
    "            \"average_similarity\": avg_similarity\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    pipeline, \n",
    "    logging,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    # CodeLlama specific\n",
    "    CodeLlamaTokenizer,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "\n",
    "# Dataset handling\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fine-tuning libraries\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# Monitoring and optimization\n",
    "import wandb\n",
    "import pynvml\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T05:36:08.970729Z",
     "iopub.status.busy": "2025-06-13T05:36:08.970188Z",
     "iopub.status.idle": "2025-06-13T05:36:09.214670Z",
     "shell.execute_reply": "2025-06-13T05:36:09.214092Z",
     "shell.execute_reply.started": "2025-06-13T05:36:08.970709Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import CommitInfo\n",
    "\n",
    "# Set environment variables in your shell or .bashrc/.zshrc\n",
    "# export HUGGING_FACE_WRITE_API_KEY=\"hf_your_write_token_here\"\n",
    "# export HUGGINGFACE_TOKEN=\"hf_your_read_token_here\"\n",
    "\n",
    "# In your Python code\n",
    "secret_value_0 = os.getenv(\"HUGGING_FACE_WRITE_API_KEY\")\n",
    "secret_value_1 = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not secret_value_0 or not secret_value_1:\n",
    "    raise ValueError(\"Please set HUGGING_FACE_WRITE_API_KEY and HUGGINGFACE_TOKEN environment variables\")\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load a fresh pipeline, specify the model and tokenizer:\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Ensure model config also has pad_token_id\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "prompt = \"How to convert COBOL code into Python?\"\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Combine dataset \n",
    "| Dataset Name                    | HF ID                                            | Contains                  | Use                                |\n",
    "| ------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------- |\n",
    "| **MainframeBench**              | `Fsoft-AIC/MainframeBench`                       | COBOL code + descriptions | Base COBOL understanding           |\n",
    "| **The Stack**                   | `bigcode/the-stack`                              | COBOL + other languages   | Language variety + COBOL samples   |\n",
    "| **Python Code Dataset**         | `jtatman/python-code-dataset-500k`               | Python code               | Target code corpus                 |\n",
    "| **SantaCoder Fine-tuned COBOL** | `muhtasham/santacoder-finetuned-the-stack-cobol` | Pretrained model          | Base model for COBOL understanding |\n",
    "| **General Code**                | `codeparrot/github-code`                         | Multi-language            | Extra fine-tuning                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split 80% train, 10% eval, 10% test\n",
    "# Use the combined_dataset from your fine_tuner instance\n",
    "splits = fine_tuner.combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "eval_test = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"eval\": eval_test[\"train\"],\n",
    "    \"test\": eval_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.039Z"
    }
   },
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=fine_tuner.model, # Use the model with LoRA already applied by fine_tuner.setup_lora()\n",
    "    args=fine_tuner.setup_training_arguments(), # Get TrainingArguments from your class\n",
    "    train_dataset=dataset_dict[\"train\"], # Use the train split from dataset_dict\n",
    "    eval_dataset=dataset_dict[\"eval\"],   # Use the eval split from dataset_dict (optional but good for validation)\n",
    "    tokenizer=fine_tuner.tokenizer, # Use the tokenizer from your class\n",
    "    dataset_text_field=fine_tuner.config.get(\"dataset_text_field\", \"text\"), # Get from config\n",
    "    max_seq_length=fine_tuner.config.get(\"max_length\", 2048), # Get from config\n",
    "    packing=fine_tuner.config.get(\"packing\", False), # Get from config\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer successfully configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-06-13T05:49:33.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# To start the fine-tuning process:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "\n",
    "# Get the output directory path directly from the trainer's arguments\n",
    "# This ensures consistency with where the trainer has been saving checkpoints\n",
    "final_model_output_path = trainer.args.output_dir\n",
    "\n",
    "trainer.save_model(final_model_output_path)\n",
    "fine_tuner.tokenizer.save_pretrained(final_model_output_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {final_model_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n",
    "# and 'dataset_dict' contains your \"test\" split.\n",
    "\n",
    "print(\"Starting model evaluation...\")\n",
    "\n",
    "# 1. Generate predictions using the evaluate_model method\n",
    "# This method generates predictions and extracts the true labels\n",
    "# The method will return a list of dictionaries, where each dict might contain\n",
    "# 'cobol_code', 'generated_python' (prediction), and potentially 'true_python_code' (ground truth).\n",
    "evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n",
    "\n",
    "# From the evaluation_results, extract just the generated Python code (y_pred)\n",
    "# and the true Python code (y_true) for passing to the metrics calculator.\n",
    "# Note: The 'evaluate_model' method should already be structured to return these.\n",
    "# If 'evaluate_model' returns predictions directly, then you extract y_pred from there.\n",
    "# And y_true is extracted by calculate_and_print_metrics internally using test_dataset.\n",
    "\n",
    "# Example: If evaluation_results is a list of dictionaries with a 'generated_python' key\n",
    "y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n",
    "\n",
    "# 2. Calculate and print metrics using the calculate_and_print_metrics method\n",
    "# This method takes the test_dataset (to re-extract y_true internally) and the predictions.\n",
    "fine_tuner.calculate_and_print_metrics(test_dataset=dataset_dict[\"test\"], predictions=y_pred_for_metrics)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fine_tuner' is your OptimizedCodeLlamaFineTuner instance\n",
    "# and the training has completed and model saved.\n",
    "\n",
    "# The base model name is stored in your fine_tuner's config\n",
    "base_model_name = fine_tuner.config[\"model_name\"]\n",
    "\n",
    "# The output directory for the fine-tuned model is stored in the trainer's args\n",
    "# which came from your fine_tuner's config.\n",
    "fine_tuned_model_path = trainer.args.output_dir\n",
    "\n",
    "print(f\"Base Model: {base_model_name}\")\n",
    "print(f\"Fine-tuned Model Saved At: {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use the 'base_model_name' variable defined in the previous step\n",
    "# (which came from fine_tuner.config[\"model_name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, # Use the defined base_model_name\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # Use torch.bfloat16 for consistency with your config_dict's bnb_4bit_compute_dtype\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model '{base_model_name}' and its tokenizer reloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Merge adapter with base model\n",
    "# 'base_model_reload' is the AutoModelForCausalLM you just reloaded.\n",
    "# 'fine_tuned_model_path' is the path where your LoRA adapters were saved by the trainer.\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path)\n",
    "\n",
    "# This merges the LoRA adapter weights into the base model weights\n",
    "# and removes the adapter layers, leaving you with a single, merged model.\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"LoRA adapters successfully merged into the base model. The merged model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # Ensure pipeline is imported\n",
    "\n",
    "cobol_code = \"\"\"\n",
    "        IDENTIFICATION DIVISION.\n",
    "        PROGRAM-ID. HELLO.\n",
    "        PROCEDURE DIVISION.\n",
    "            DISPLAY 'HELLO, WORLD'.\n",
    "            STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,        # This is your merged model\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency with training and loading\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Generated Python Code:\\n\")\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save your final merged model\n",
    "model_dir = \"CodeLlama-7B-Instruct-COBOL-to-Python\"\n",
    "\n",
    "# Save the merged model (which now contains the LoRA weights)\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"Your final merged model and tokenizer have been saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Assuming base_model_name and fine_tuned_model_path are defined from previous steps:\n",
    "# For example:\n",
    "# base_model_name = fine_tuner.config[\"model_name\"]\n",
    "# fine_tuned_model_path = trainer.args.output_dir # or your chosen output directory for adapters\n",
    "\n",
    "print(f\"Loading tokenizer from: {base_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(f\"Loading base model from: {base_model_name} with torch_dtype=torch.bfloat16\")\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, # Use the correct variable name for the base model identifier\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for consistency\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading fine-tuned adapters from: {fine_tuned_model_path}\")\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model_path) # Use the correct path for adapters\n",
    "\n",
    "print(\"Merging adapters into the base model...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"Model and adapters merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and register to Hugging Face Hub\n",
    "# Ensure you are logged in to Hugging Face locally (huggingface-cli login)\n",
    "\n",
    "# Push the merged model to your Hugging Face repository\n",
    "model.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n",
    "\n",
    "# Push the tokenizer to the same repository\n",
    "tokenizer.push_to_hub(\"dhirajpatra/codellama-cobol-python\", use_temp_dir=False)\n",
    "\n",
    "print(\"\\nModel and tokenizer successfully pushed to Hugging Face Hub!\")\n",
    "print(\"You can find them at: https://huggingface.co/dhirajpatra/codellama-cobol-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Assuming 'model_dir' is the string \"codellama-cobol-python\" you used when saving\n",
    "# Or, even better, just use the full repo ID directly for clarity:\n",
    "hf_repo_id = \"dhirajpatra/codellama-cobol-python\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
    "\n",
    "print(f\"Model and tokenizer loaded successfully from Hugging Face Hub: {hf_repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use the full repository ID directly for clarity and robustness\n",
    "# Assuming model_dir was \"codellama-cobol-python\" from your save step\n",
    "model_id = \"dhirajpatra/codellama-cobol-python\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "print(f\"**Model and tokenizer loaded successfully from Hugging Face Hub:** `{model_id}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # Ensure pipeline is imported\n",
    "import torch # Ensure torch is imported if not already\n",
    "\n",
    "cobol_code = \"\"\"\n",
    "        IDENTIFICATION DIVISION.\n",
    "        PROGRAM-ID. HELLO.\n",
    "        PROCEDURE DIVISION.\n",
    "            DISPLAY 'HELLO, WORLD'.\n",
    "            STOP RUN.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Convert the following COBOL code to Python:\n",
    "\n",
    "{cobol_code}\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16, # Changed to bfloat16 for consistency and performance\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1)\n",
    "generated_code = outputs[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(\"Generated Python Code:\\n\")\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, DatasetDict\n",
    "# Assuming fine_tuner is your OptimizedCodeLlamaFineTuner instance\n",
    "# and dataset_dict contains your \"test\" split.\n",
    "# And assuming you've already run the evaluation_results step:\n",
    "# evaluation_results = fine_tuner.evaluate_model(test_dataset=dataset_dict[\"test\"])\n",
    "# y_pred_for_metrics = [item['generated_python'] for item in evaluation_results]\n",
    "\n",
    "# 1. Load the BLEU metric\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "# 2. Prepare predictions (preds) - This is your generated code\n",
    "# Assuming y_pred_for_metrics holds the list of generated Python code strings\n",
    "preds = y_pred_for_metrics\n",
    "\n",
    "# 3. Prepare references (refs) - This requires extracting the true Python code\n",
    "# The calculate_and_print_metrics method has a helper for this.\n",
    "# We need to call that helper or re-extract here.\n",
    "# Let's re-extract the filtered ground truth for consistency with metrics.\n",
    "y_true = [fine_tuner._extract_true_python_code(sample[\"text\"]) for sample in dataset_dict[\"test\"]]\n",
    "valid_pairs = [(gt, pred) for gt, pred in zip(y_true, preds) if gt]\n",
    "\n",
    "# preds should be from the filtered valid_pairs as well for direct comparison\n",
    "# If you ran fine_tuner.calculate_and_print_metrics, it already filtered.\n",
    "# For BLEU, ensure preds and refs correspond to the same filtered set.\n",
    "preds_filtered_for_bleu = [pair[1] for pair in valid_pairs]\n",
    "refs_filtered_for_bleu = [[pair[0]] for pair in valid_pairs] # BLEU expects a list of lists for references\n",
    "\n",
    "if not preds_filtered_for_bleu:\n",
    "    print(\"‚ùó No valid samples to compute BLEU score on.\")\n",
    "else:\n",
    "    # 4. Compute the BLEU score\n",
    "    results = bleu.compute(predictions=preds_filtered_for_bleu, references=refs_filtered_for_bleu)\n",
    "\n",
    "    print(\"\\n--- BLEU Score ---\")\n",
    "    print(f\"BLEU score: {results['bleu']:.4f}\")\n",
    "    # You can also print other details if available in results, e.g., 'precisions'\n",
    "    # print(results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
