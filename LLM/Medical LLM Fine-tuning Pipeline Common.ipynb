{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448ac4a-e441-4aab-b967-39fced6563e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Medical LLM Fine-tuning Pipeline\n",
    "Complete end-to-end fine-tuning setup for medical/clinical applications\n",
    "Supports CUDA acceleration and Hugging Face datasets\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52e2c9-7c43-4ea1-979f-9820e18a4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalLLMFineTuner:\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Initialize the Medical LLM Fine-tuning pipeline\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary with model, training, and data parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.accelerator = Accelerator()\n",
    "        self.device = self.accelerator.device\n",
    "        \n",
    "        # Set up directories\n",
    "        self.output_dir = config['output_dir']\n",
    "        self.logs_dir = os.path.join(self.output_dir, 'logs')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.logs_dir, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            logger.info(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load the base medical model and tokenizer\"\"\"\n",
    "        model_name = self.config['model_name']\n",
    "        logger.info(f\"Loading model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        # Add pad token if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Load model with quantization for memory efficiency\n",
    "        if self.config.get('use_quantization', True):\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            \n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "        \n",
    "        # Prepare model for training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        logger.info(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.get('lora_r', 16),\n",
    "            lora_alpha=self.config.get('lora_alpha', 32),\n",
    "            target_modules=self.config.get('lora_target_modules', [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]),\n",
    "            lora_dropout=self.config.get('lora_dropout', 0.1),\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        logger.info(\"LoRA configuration applied\")\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load and preprocess the medical dataset\"\"\"\n",
    "        dataset_name = self.config['dataset_name']\n",
    "        logger.info(f\"Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        if dataset_name.startswith('custom:'):\n",
    "            # Load custom dataset from local path\n",
    "            dataset_path = dataset_name.replace('custom:', '')\n",
    "            self.dataset = load_dataset('json', data_files=dataset_path)\n",
    "        else:\n",
    "            # Load from Hugging Face Hub\n",
    "            self.dataset = load_dataset(\n",
    "                dataset_name,\n",
    "                split=self.config.get('dataset_split', 'train')\n",
    "            )\n",
    "        \n",
    "        # If dataset doesn't have train/validation split, create one\n",
    "        if isinstance(self.dataset, Dataset):\n",
    "            split = self.dataset.train_test_split(\n",
    "                test_size=self.config.get('validation_split', 0.1),\n",
    "                seed=42\n",
    "            )\n",
    "            self.dataset = DatasetDict({\n",
    "                'train': split['train'],\n",
    "                'validation': split['test']\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Dataset loaded - Train: {len(self.dataset['train'])}, Val: {len(self.dataset['validation'])}\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the dataset for training\"\"\"\n",
    "        def format_medical_prompt(example):\n",
    "            \"\"\"Format the data into a medical instruction-response format\"\"\"\n",
    "            if 'instruction' in example and 'response' in example:\n",
    "                # Standard instruction-response format\n",
    "                prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "            elif 'question' in example and 'answer' in example:\n",
    "                # Q&A format\n",
    "                prompt = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n",
    "            elif 'input' in example and 'output' in example:\n",
    "                # Input-output format\n",
    "                prompt = f\"### Input:\\n{example['input']}\\n\\n### Output:\\n{example['output']}\"\n",
    "            elif 'text' in example:\n",
    "                # Raw text format\n",
    "                prompt = example['text']\n",
    "            else:\n",
    "                # Try to find the best fields automatically\n",
    "                keys = list(example.keys())\n",
    "                if len(keys) >= 2:\n",
    "                    prompt = f\"### {keys[0].title()}:\\n{example[keys[0]]}\\n\\n### {keys[1].title()}:\\n{example[keys[1]]}\"\n",
    "                else:\n",
    "                    prompt = str(example)\n",
    "            \n",
    "            return {\"text\": prompt + self.tokenizer.eos_token}\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize the text\"\"\"\n",
    "            tokenized = self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=self.config.get('max_length', 2048),\n",
    "                return_overflowing_tokens=False,\n",
    "            )\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            return tokenized\n",
    "\n",
    "        # Apply formatting\n",
    "        self.dataset = self.dataset.map(format_medical_prompt, remove_columns=self.dataset['train'].column_names)\n",
    "        \n",
    "        # Tokenize\n",
    "        self.dataset = self.dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "            desc=\"Tokenizing dataset\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Dataset preprocessing completed\")\n",
    "\n",
    "    def setup_training_arguments(self):\n",
    "        \"\"\"Setup training arguments\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_name = f\"medical_llm_finetune_{timestamp}\"\n",
    "        \n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            run_name=run_name,\n",
    "            \n",
    "            # Training hyperparameters\n",
    "            num_train_epochs=self.config.get('num_epochs', 3),\n",
    "            per_device_train_batch_size=self.config.get('train_batch_size', 4),\n",
    "            per_device_eval_batch_size=self.config.get('eval_batch_size', 4),\n",
    "            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 4),\n",
    "            \n",
    "            # Optimization\n",
    "            learning_rate=self.config.get('learning_rate', 2e-4),\n",
    "            weight_decay=self.config.get('weight_decay', 0.01),\n",
    "            lr_scheduler_type=self.config.get('lr_scheduler_type', \"cosine\"),\n",
    "            warmup_ratio=self.config.get('warmup_ratio', 0.03),\n",
    "            \n",
    "            # Memory optimization\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            fp16=False,\n",
    "            bf16=True,\n",
    "            dataloader_pin_memory=False,\n",
    "            \n",
    "            # Evaluation and saving\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=self.config.get('eval_steps', 500),\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=self.config.get('save_steps', 500),\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            \n",
    "            # Logging\n",
    "            logging_dir=self.logs_dir,\n",
    "            logging_steps=self.config.get('logging_steps', 100),\n",
    "            report_to=\"wandb\" if self.config.get('use_wandb', False) else None,\n",
    "            \n",
    "            # Miscellaneous\n",
    "            remove_unused_columns=False,\n",
    "            push_to_hub=self.config.get('push_to_hub', False),\n",
    "            hub_model_id=self.config.get('hub_model_id', None),\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Training arguments configured\")\n",
    "\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Setup the Trainer\"\"\"\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        callbacks = []\n",
    "        if self.config.get('early_stopping', True):\n",
    "            callbacks.append(EarlyStoppingCallback(\n",
    "                early_stopping_patience=self.config.get('early_stopping_patience', 3)\n",
    "            ))\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.dataset['train'],\n",
    "            eval_dataset=self.dataset['validation'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Trainer configured\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Start the training process\"\"\"\n",
    "        logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get('use_wandb', False):\n",
    "            wandb.init(\n",
    "                project=self.config.get('wandb_project', 'medical-llm-finetuning'),\n",
    "                config=self.config\n",
    "            )\n",
    "        \n",
    "        # Start training\n",
    "        train_result = self.trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        self.trainer.save_model()\n",
    "        self.trainer.save_state()\n",
    "        \n",
    "        # Log training results\n",
    "        logger.info(f\"Training completed!\")\n",
    "        logger.info(f\"Final train loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # Save training metrics\n",
    "        metrics_file = os.path.join(self.output_dir, 'training_metrics.json')\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(train_result.metrics, f, indent=2)\n",
    "        \n",
    "        return train_result\n",
    "\n",
    "    def save_model_for_inference(self):\n",
    "        \"\"\"Save the model in a format ready for inference\"\"\"\n",
    "        inference_dir = os.path.join(self.output_dir, 'inference_model')\n",
    "        \n",
    "        # Save the merged model (LoRA + base model)\n",
    "        self.model.save_pretrained(inference_dir)\n",
    "        self.tokenizer.save_pretrained(inference_dir)\n",
    "        \n",
    "        # Create model card\n",
    "        model_card = f\"\"\"\n",
    "# Medical LLM Fine-tuned Model\n",
    "\n",
    "This model has been fine-tuned for medical/clinical applications.\n",
    "\n",
    "## Model Details\n",
    "- Base Model: {self.config['model_name']}\n",
    "- Dataset: {self.config['dataset_name']}\n",
    "- Fine-tuning Method: LoRA (Low-Rank Adaptation)\n",
    "- Training Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{inference_dir}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{inference_dir}\")\n",
    "\n",
    "# Example usage\n",
    "prompt = \"### Instruction:\\\\nExplain the symptoms of diabetes\\\\n\\\\n### Response:\\\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=512, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Intended Use\n",
    "- Clinical trial support\n",
    "- Medical question answering\n",
    "- RAG applications in healthcare\n",
    "- Multi-agent medical systems\n",
    "\n",
    "## Limitations\n",
    "- This model is for research and educational purposes\n",
    "- Should not be used as a substitute for professional medical advice\n",
    "- Always validate outputs with medical professionals\n",
    "\"\"\"\n",
    "        \n",
    "        with open(os.path.join(inference_dir, 'README.md'), 'w') as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        logger.info(f\"Model saved for inference at: {inference_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082efb95-d7c0-40f7-83ce-c98bcf05c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the fine-tuning pipeline\"\"\"\n",
    "    \n",
    "    # Configuration optimized for 2x RTX 4090 setup (48GB total VRAM)\n",
    "    config = {\n",
    "        # Model configuration - can handle larger models with your setup\n",
    "        'model_name': 'medalpaca/medalpaca-13b',  # 13B model fits well on dual 4090s\n",
    "        'use_quantization': True,  # Still use quantization for memory efficiency\n",
    "        \n",
    "        # Dataset configuration\n",
    "        'dataset_name': 'medalpaca/medical_meadow_medical_flashcards',  # Medical dataset\n",
    "        'dataset_split': 'train',\n",
    "        'validation_split': 0.1,\n",
    "        'max_length': 4096,  # Increased context length for better performance\n",
    "        \n",
    "        # LoRA configuration - more aggressive settings for better results\n",
    "        'lora_r': 64,  # Higher rank for better adaptation\n",
    "        'lora_alpha': 128,  # Scaled accordingly\n",
    "        'lora_dropout': 0.05,  # Lower dropout with more VRAM\n",
    "        'lora_target_modules': [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"  # More target modules\n",
    "        ],\n",
    "        \n",
    "        # Training configuration - optimized for dual RTX 4090\n",
    "        'num_epochs': 5,  # More epochs for better convergence\n",
    "        'train_batch_size': 4,  # Larger batch size with 48GB VRAM\n",
    "        'eval_batch_size': 4,\n",
    "        'gradient_accumulation_steps': 4,  # Reduced due to larger batch size\n",
    "        'learning_rate': 1e-4,  # Slightly lower LR for stability\n",
    "        'weight_decay': 0.01,\n",
    "        'lr_scheduler_type': \"cosine\",\n",
    "        'warmup_ratio': 0.05,\n",
    "        \n",
    "        # Evaluation and saving\n",
    "        'eval_steps': 100,  # More frequent evaluation\n",
    "        'save_steps': 250,  # More frequent saves\n",
    "        'logging_steps': 25,  # More frequent logging\n",
    "        'early_stopping': True,\n",
    "        'early_stopping_patience': 5,\n",
    "        \n",
    "        # Output configuration\n",
    "        'output_dir': './medical_llm_finetuned',\n",
    "        \n",
    "        # Weights & Biases (recommended for this setup)\n",
    "        'use_wandb': True,  # Enable for experiment tracking\n",
    "        'wandb_project': 'medical-llm-dual-4090',\n",
    "        \n",
    "        # Hugging Face Hub (optional)\n",
    "        'push_to_hub': False,  # Set to True if you want to push to hub\n",
    "        'hub_model_id': 'your-username/medical-llm-13b-clinical',\n",
    "    }\n",
    "    \n",
    "    # Initialize and run the fine-tuning pipeline\n",
    "    finetuner = MedicalLLMFineTuner(config)\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        finetuner.load_model_and_tokenizer()\n",
    "        \n",
    "        # Setup LoRA\n",
    "        finetuner.setup_lora()\n",
    "        \n",
    "        # Load and preprocess dataset\n",
    "        finetuner.load_dataset()\n",
    "        finetuner.preprocess_data()\n",
    "        \n",
    "        # Setup training\n",
    "        finetuner.setup_training_arguments()\n",
    "        finetuner.setup_trainer()\n",
    "        \n",
    "        # Train the model\n",
    "        train_result = finetuner.train()\n",
    "        \n",
    "        # Save model for inference\n",
    "        finetuner.save_model_for_inference()\n",
    "        \n",
    "        logger.info(\"Fine-tuning pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during fine-tuning: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778359a-723d-4bbc-bd92-26fdaca749ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
