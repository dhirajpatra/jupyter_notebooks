{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462d430-8e6c-42a2-a9bc-1a1dc00e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install transformers and related libraries\n",
    "pip install transformers>=4.36.0\n",
    "pip install datasets\n",
    "pip install peft>=0.7.0\n",
    "pip install trl\n",
    "pip install bitsandbytes\n",
    "pip install accelerate\n",
    "pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Optional: For monitoring\n",
    "pip install wandb\n",
    "pip install pynvml\n",
    "\n",
    "# For TensorRT optimization (optional)\n",
    "pip install tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5c33e-806c-4e9a-90b9-e246f72c9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Fine-tuning Pipeline for Meta Llama 3.2 3B\n",
    "Using NVIDIA AI Software Stack (CUDA, cuDNN, TensorRT, PyTorch)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import tensorrt as trt\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d57822-9727-41b8-bdaa-907319a7c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA optimizations\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257667e-6fb0-4c73-b95a-fd674112d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedLlamaFineTuner:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_nvidia_environment()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        \n",
    "    def setup_nvidia_environment(self):\n",
    "        \"\"\"Setup NVIDIA environment and check GPU capabilities\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available!\")\n",
    "            \n",
    "        # Initialize NVML for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        \n",
    "        print(f\"ðŸš€ NVIDIA Setup:\")\n",
    "        print(f\"   - CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   - cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   - Available GPUs: {gpu_count}\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode()\n",
    "            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            print(f\"   - GPU {i}: {name} ({memory.total // 1024**3} GB)\")\n",
    "            \n",
    "        # Enable TensorFloat-32 for A100/RTX 30xx series\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"   - TensorFloat-32 enabled for Ampere+ GPUs\")\n",
    "            \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load Llama 3.2 3B with optimized quantization\"\"\"\n",
    "        model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        \n",
    "        # BitsAndBytesConfig for 4-bit quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“¥ Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(\"ðŸ“¥ Loading model with 4-bit quantization...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\" if self.config.get(\"use_flash_attention\", True) else \"eager\"\n",
    "        )\n",
    "        \n",
    "        # Prepare model for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        print(f\"âœ… Model loaded on: {self.model.device}\")\n",
    "        \n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA configuration for efficient fine-tuning\"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.config.get(\"lora_r\", 16),\n",
    "            lora_alpha=self.config.get(\"lora_alpha\", 32),\n",
    "            lora_dropout=self.config.get(\"lora_dropout\", 0.1),\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"ðŸŽ¯ LoRA Configuration:\")\n",
    "        print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   - Total parameters: {total_params:,}\")\n",
    "        print(f\"   - Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "        \n",
    "    def load_and_prepare_dataset(self):\n",
    "        \"\"\"Load and prepare dataset for training\"\"\"\n",
    "        dataset_name = self.config.get(\"dataset_name\", \"tatsu-lab/alpaca\")\n",
    "        \n",
    "        print(f\"ðŸ“Š Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        if isinstance(dataset_name, str):\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        else:\n",
    "            # Handle custom dataset loading\n",
    "            dataset = dataset_name\n",
    "            \n",
    "        # Prepare dataset based on format\n",
    "        if \"instruction\" in dataset.column_names and \"output\" in dataset.column_names:\n",
    "            # Alpaca format\n",
    "            def format_alpaca(examples):\n",
    "                texts = []\n",
    "                for instruction, input_text, output in zip(\n",
    "                    examples[\"instruction\"], \n",
    "                    examples.get(\"input\", [\"\"] * len(examples[\"instruction\"])), \n",
    "                    examples[\"output\"]\n",
    "                ):\n",
    "                    if input_text:\n",
    "                        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "                    else:\n",
    "                        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "                    texts.append(prompt)\n",
    "                return {\"text\": texts}\n",
    "                \n",
    "            dataset = dataset.map(format_alpaca, batched=True, remove_columns=dataset.column_names)\n",
    "            \n",
    "        elif \"text\" not in dataset.column_names:\n",
    "            raise ValueError(\"Dataset must have 'text' column or Alpaca format (instruction/output)\")\n",
    "            \n",
    "        # Filter by length to avoid OOM\n",
    "        max_length = self.config.get(\"max_length\", 2048)\n",
    "        dataset = dataset.filter(lambda x: len(self.tokenizer.encode(x[\"text\"])) <= max_length)\n",
    "        \n",
    "        # Take subset if specified\n",
    "        if self.config.get(\"max_samples\"):\n",
    "            dataset = dataset.select(range(min(len(dataset), self.config[\"max_samples\"])))\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        print(f\"âœ… Dataset prepared: {len(dataset)} samples\")\n",
    "        \n",
    "    def setup_training_arguments(self):\n",
    "        \"\"\"Setup optimized training arguments\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.get(\"output_dir\", \"./llama-3.2-3b-finetuned\"),\n",
    "            \n",
    "            # Training hyperparameters\n",
    "            num_train_epochs=self.config.get(\"num_epochs\", 3),\n",
    "            per_device_train_batch_size=self.config.get(\"batch_size\", 4),\n",
    "            gradient_accumulation_steps=self.config.get(\"gradient_accumulation_steps\", 4),\n",
    "            learning_rate=self.config.get(\"learning_rate\", 2e-4),\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            \n",
    "            # NVIDIA optimizations\n",
    "            bf16=True,  # Use bfloat16 for Ampere+ GPUs\n",
    "            tf32=True,  # Enable TensorFloat-32\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=4,\n",
    "            \n",
    "            # Memory optimizations\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch_fused\",  # Fused optimizer for NVIDIA GPUs\n",
    "            \n",
    "            # Logging and saving\n",
    "            logging_steps=10,\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            evaluation_strategy=\"no\",  # Disable eval to save memory\n",
    "            \n",
    "            # Additional optimizations\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\" if self.config.get(\"use_wandb\", False) else None,\n",
    "            run_name=f\"llama-3.2-3b-finetune-{self.config.get('experiment_name', 'default')}\",\n",
    "            \n",
    "            # DDP settings (if using multiple GPUs)\n",
    "            ddp_find_unused_parameters=False,\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Execute the fine-tuning process\"\"\"\n",
    "        print(\"ðŸš€ Starting fine-tuning process...\")\n",
    "        \n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.get(\"use_wandb\", False):\n",
    "            wandb.init(\n",
    "                project=self.config.get(\"wandb_project\", \"llama-finetune\"),\n",
    "                name=f\"llama-3.2-3b-{self.config.get('experiment_name', 'default')}\"\n",
    "            )\n",
    "            \n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_arguments()\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize SFTTrainer (optimized for instruction tuning)\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset,\n",
    "            data_collator=data_collator,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.config.get(\"max_length\", 2048),\n",
    "            packing=False,  # Disable packing to avoid issues with instruction format\n",
    "        )\n",
    "        \n",
    "        # Clear cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Start training\n",
    "        print(\"ðŸŽ¯ Training started...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        print(\"ðŸ’¾ Saving model...\")\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        print(\"âœ… Fine-tuning completed!\")\n",
    "        \n",
    "    def optimize_for_inference(self, model_path):\n",
    "        \"\"\"Optimize model for inference using TensorRT (optional)\"\"\"\n",
    "        print(\"âš¡ Optimizing model for inference...\")\n",
    "        \n",
    "        # Load the fine-tuned model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "        model.eval()\n",
    "        \n",
    "        # Convert to TorchScript\n",
    "        traced_model = torch.jit.trace(model, example_inputs=(torch.randint(0, 1000, (1, 512)),))\n",
    "        \n",
    "        # Save optimized model\n",
    "        optimized_path = f\"{model_path}_optimized\"\n",
    "        traced_model.save(f\"{optimized_path}/traced_model.pt\")\n",
    "        \n",
    "        print(f\"âœ… Optimized model saved to: {optimized_path}\")\n",
    "        \n",
    "    def monitor_gpu_usage(self):\n",
    "        \"\"\"Monitor GPU usage during training\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"GPU {i}: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750d613-dc5b-40e4-9f5f-1cfbaff0415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        # Model and dataset\n",
    "        \"dataset_name\": \"tatsu-lab/alpaca\",  # Change to your dataset\n",
    "        \"max_samples\": None,  # Set to limit dataset size for testing\n",
    "        \"max_length\": 2048,\n",
    "        \n",
    "        # LoRA configuration\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 4,  # Adjust based on your GPU memory\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \n",
    "        # Optimizations\n",
    "        \"use_flash_attention\": True,\n",
    "        \n",
    "        # Paths and logging\n",
    "        \"output_dir\": \"./llama-3.2-3b-finetuned\",\n",
    "        \"experiment_name\": \"alpaca_finetune\",\n",
    "        \"use_wandb\": False,  # Set to True if you want to use Weights & Biases\n",
    "        \"wandb_project\": \"llama-finetune\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc72f57-d1f0-4e6c-bf8c-5a87b25182a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fine-tuner\n",
    "fine_tuner = OptimizedLlamaFineTuner(config)\n",
    "\n",
    "try:\n",
    "    # Load model and tokenizer\n",
    "    fine_tuner.load_model_and_tokenizer()\n",
    "    \n",
    "    # Setup LoRA\n",
    "    fine_tuner.setup_lora()\n",
    "    \n",
    "    # Load and prepare dataset\n",
    "    fine_tuner.load_and_prepare_dataset()\n",
    "    \n",
    "    # Start training\n",
    "    fine_tuner.train()\n",
    "    \n",
    "    # Optional: Optimize for inference\n",
    "    # fine_tuner.optimize_for_inference(config[\"output_dir\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during fine-tuning: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921dd784-4d04-4f30-a4ac-636da73616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
