{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Intel AI Kit and XGBoost\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Utilize XGBoost with Intel's AI KIt\n",
    "* Take advantage of Intel extensions to SciKit Learn by enabling them with XGBoost\n",
    "* Use Cross Validation technique to find better XGBoost Hyperparameters\n",
    "* Use a learning curve to estimate the ideal number of trees\n",
    "* Improve performance by implementing early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this example, we will use a dataset with particle features and functions of those features **to distinguish between a signal process which produces Higgs bosons (1) and a background process which does not (0)**. The Higgs boson is a basic particle in the standard model produced by the quantum excitation of the Higgs field, named after physicist Peter Higgs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "![image](3D_view_energy_of_8_TeV.png)\n",
    "[Images Source](https://commons.wikimedia.org/wiki/File:3D_view_of_an_event_recorded_with_the_CMS_detector_in_2012_at_a_proton-proton_centre_of_mass_energy_of_8_TeV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install xgboost\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex, Int16Dtype  # if you don't import in this order you will get a pandas.Int64Index fix for FutureWarning error.\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This line is a Jupyter Notebook magic command that allows plots to be displayed inline\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"XGB Version          : \", xgb.__version__)\n",
    "print(\"Scikit-Learn Version : \", sklearn.__version__)\n",
    "print(\"Pandas Version       : \", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Import the Data:\n",
    "\n",
    "* The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features):\n",
    "\n",
    "* The dataset has 1.1 million rows, adjust the __nrows__ value to something manageable by the sytem you happen to be using.  100K is easy for a modern laptop; however, once you start optimizing much more than that can take some time. \n",
    "\n",
    "[Data Source](https://archive.ics.uci.edu/ml/datasets/HIGGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### To get the data using the Intel DevCloud execute the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O higgs.zip https://archive.ics.uci.edu/static/public/280/higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cp /data/oneapi_workshop/big_datasets/xgboost/HIGGS.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xzf HIGGS.tar.gz\n",
    "! unzip higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Specify the input and output file names\n",
    "input_file = \"HIGGS.csv.gz\"\n",
    "output_file = \"HIGGS.csv\"\n",
    "\n",
    "# Decompress the gzip file\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### __Do not__ run this if on the Intel DevCloud.  To fetch the data for your local install execute the below two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# if not os.path.isfile(\"./HIGGS.csv.gz\"):\n",
    "#         print(\"Fetching data set from Internet...~2.8GB\")\n",
    "#         url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"\n",
    "#         myfile = requests.get(url)\n",
    "#         with open('./HIGGS.csv.gz', 'wb') as f:\n",
    "#             f.write(myfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gunzip HIGGS.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Set the number of rows to use via nrows= variable.  100K is manageable on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'HIGGS.csv'\n",
    "names =  ['class_label', 'lepton pT', 'lepton eta', 'lepton phi', 'missing energy magnitude', 'missing energy phi', 'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag', 'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag', 'jet 3 pt', 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag', 'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag', 'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
    "#data = pd.read_csv(filename, names=names, delimiter=\",\", nrows=100000)\n",
    "data = pd.read_csv(filename, names=names, delimiter=\",\", nrows=1100000)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time p_df = pd.read_csv(\"HIGGS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Examine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "* What kind of data is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "* Examine the distribution of the Higgs Boson class_label.  Depending on how many rows you load this could change how you choose to split the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.class_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "* In this scenario loading 100000 rows the balance isn't too skewed, the next cell is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.class_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Create your train/test split. \n",
    "\n",
    "* Remember the first column is 0 = no signal 1 = signal, so we want to leave out the labels and predict column 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:, 1:],data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "* These next two cell are optional, just a sanity check of the split data actually representing our intentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check split of data.  This is the x variable.\n",
    "print(data.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the y target vector -- the ones we want to predict.\n",
    "print(data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### We are using the scikit-learn methodology to create the train test/split.  Feel free to play with the split and random state, just make sure you use the same random state throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "* Another sanity check, make sure nothing odd happened after splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Get a baseline using the XGBoost defaults.  \n",
    "\n",
    "Now that we have our data split into train and test datasets let's use the default XGBoost parameters to see default results.  If you are familiar with these parameters feel free to add them to the parameters cell below and feel free to modify these.  We will explore how to find better results later in the notebook.\n",
    "\n",
    "* __learning_rate:__ step size shrinkage used to prevent overfitting. Range is 0 to 1 but a lower rate is usually better.\n",
    "* __max_depth:__ determines how deeply each tree is allowed to grow during any boosting round.\n",
    "* __subsample:__ percentage of samples used per tree. Low value can lead to underfitting.\n",
    "* __colsample_bytree:__ percentage of features used per tree. High value can lead to overfitting.\n",
    "* __n_estimators:__ number of trees built\n",
    "* __objective:__ determines the loss function type: \n",
    "    * reg:linear for # regression problems.\n",
    "    * reg:logistic for classification problems with only decision.\n",
    "    * binary:logistic for classification problems with probability.\n",
    "    \n",
    "    [There are many more parameters, here is the reference.](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters)\n",
    "    \n",
    "* For a default we are selecting three parameters:  binary:logistic, using the cpu_predictor and due to a recent change in XGBoosts behaviour setting the error metric to error rather than logistic error for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective':                    'binary:logistic',\n",
    "    'predictor':                    'cpu_predictor',\n",
    "    'disable_default_eval_metric':  'true',\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "t1_start = perf_counter()  # Time fit function\n",
    "model_xgb= xgb.XGBClassifier(**xgb_params)\n",
    "model_xgb.fit(X_train,y_train)\n",
    "t1_stop = perf_counter()\n",
    "print (\"It took\", t1_stop-t1_start,\" to fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_predict_xgb_test = model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model accuracy\n",
    "acc = np.mean(y_test == result_predict_xgb_test)\n",
    "print(\"Model accuracy =\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "\n",
    "* 100000 rows using defaults achieved ~72% accuracy.  Not bad, but let us see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the settings of the default XGBoost implementation.\n",
    "model_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Tune Parameters with GridSearchCV\n",
    "\n",
    "* As you can see above there are many parameters that can be modified and tuned and that would take a lot of time to profile each parameter.  In this exercise we will focus on some of the most frequently chosen parameters to tune. GridSearchCV is an exhaustive search over a set of parameters fitting seperate models to each combination.  It is important to consider how many cores you have and how much memory you have. \n",
    "\n",
    "#### Parameters for Tree Booster\n",
    "\n",
    "__eta__ [default=0.3, alias: learning_rate]  range: [0,1]\n",
    "\n",
    "* Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "\n",
    "\n",
    "\n",
    "__gamma__ [default=0, alias: min_split_loss]  range: [0,∞]\n",
    "\n",
    "* Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "\n",
    "\n",
    "__max_depth__ [default=6]  range: [0,∞]\n",
    "\n",
    "Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree. exact tree method requires non-zero value.\n",
    "\n",
    "\n",
    "__subsample__ [default=1] range: [0,1]\n",
    "\n",
    "* Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "\n",
    "__colsample_bytree__ [default=1] range: [0,1]\n",
    "\n",
    "* colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "\n",
    "__Lambda__ [default=1, alias: reg_lambda]\n",
    "\n",
    "* L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "__scale_pos_weight__ [default=1]\n",
    "\n",
    "* Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "\n",
    "[These descriptions are straight from the docs, which you can view all parameters explanation here.](https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "\n",
    "Feel free to change these values, these are a good starting point for round one.  Word of caution this takes ~ 1 to 3 hours on an Intel® Xeon® 6128 running in the Intel DevCloud.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],\n",
    "    \"gamma\": [0, 0.25, 1],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"subsample\": [0.5, 1],\n",
    "    \"colsample_bytree\": [0.7, 1],\n",
    "    \"colsample_bynode\": [0.7, 1],\n",
    "    \"reg_lambda\": [0, 1, 10],\n",
    "    \"scale_pos_weight\": [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "xgb_params2 = {\n",
    "    'objective':                    'binary:logistic',\n",
    "    'predictor':                    'cpu_predictor',\n",
    "    'disable_default_eval_metric':  'true',\n",
    "    'tree_method':                  'hist', \n",
    "}\n",
    "# Necessary for now to supress multi-threaded Future errors with respect to pandas and XGBoost\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS']='ignore::FutureWarning'\n",
    "\n",
    "# Train the model\n",
    "model_xgb= xgb.XGBClassifier(**xgb_params2, use_label_encoder=False)\n",
    "\n",
    "# Setup grid search n_jobs=-1 uses all cores, reducing cv from 5 to 3 for speed, scoring is done using area under curve.\n",
    "grid_cv = GridSearchCV(model_xgb, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "# This fit function takes a while--hours, make sure you are ready.\n",
    "_ = grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "    grid_cv.best_score_ = 0.80 grid cv.best_params\n",
    "\n",
    "    {'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 8, 'reg_lambda': 10, 'scale_pos_weight': 1, 'subsample': 1}\n",
    "\n",
    "As you can see the results came back at 80% which is a great improvement, over the default settings.  \n",
    "\n",
    "    \"max_depth\": [2, 4, 6, 8],\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5],\n",
    "    \"gamma\": [0, 0.25, 1],\n",
    "    \"reg_lambda\": [0, 1, 10],\n",
    "    \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [1],\n",
    "    \"colsample_bytree\": [1],\n",
    "  \n",
    "The best results from the experiment were as follows formatted for easy pasting into the above xgb_params kwargs:\n",
    "\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0,\n",
    "    'reg_lambda': 10,\n",
    "    'scale_pos_weight': 1,\n",
    "    \n",
    "From this result it would be worth exploring a higher tree depth as it was at the edge of our parameters as well as the reg_lambda parameter.  This is an exercise you can run setting the best values and adding some additional ranges.  For example max_depth: [8, 10, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Implement an XGBoost classifier using the above results:  \n",
    "\n",
    "* hint you could use the **grid_cv.best_params like xgb_params above.\n",
    "\n",
    "        xgb_model.XGBClassifier(\n",
    "            **grid_cv.best_params_,\n",
    "            objective=\"binary:logistic\",\n",
    "            colsample_bytree=1,\n",
    "            subsample=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Further Tuning\n",
    "\n",
    "Another frequent parameter that is tuned for is:\n",
    "\n",
    "     n_estimators:, default=100\n",
    "\n",
    "n_estimaters represents the number of trees in the forest.  A good way to see how many trees might be useful is to plot the learning curve.  Since this is a classification problem we will use log loss as our measurement where lower values are better.  \n",
    "\n",
    "Our orignal fit function needs to be modified to include eval_metric with the type set to logloss.  In addition we need to define the evaluation data set so that the results are evaluated after each round in order to plot them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets used for evaluation after each round\n",
    "evalset = [(X_train, y_train), (X_test,y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_xgb.fit(X_train, y_train, eval_metric='logloss', eval_set=evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check model accuracy\n",
    "result_predict_xgb_test = model_xgb.predict(X_test)\n",
    "acc = np.mean(y_test == result_predict_xgb_test)\n",
    "print(\"Model accuracy =\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve performance metrics\n",
    "results = model_xgb.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['validation_0']['logloss'], label='train')\n",
    "plt.plot(results['validation_1']['logloss'], label='test')\n",
    "# display legend\n",
    "plt.legend()\n",
    "# render\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Put it all together:\n",
    "\n",
    "* Use the results from the gridsearch and add an additional parameter n_estimators\n",
    "        \n",
    "        n_estimators are the number of trees in the forest. default=100    \n",
    "\n",
    "*   From the curves above you can see that they are still at a slope when n_estimators is at 100.  In this case it would seem that increasing the number of trees might yield a better result. We know that we achieve 72% when using the default 100 trees and our best results discovered via the grid search is approximately 80%.  Can we do better?\n",
    "\n",
    "* Start by setting up a new parameters section and use the values discovered earlier.  In addition set n_estimators to 1000 and see if a better result is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective':                    'binary:logistic',\n",
    "    'predictor':                    'cpu_predictor',\n",
    "    'disable_default_eval_metric':  'true',\n",
    "    'max_depth':                     8,\n",
    "    'learning_rate':                 0.1,\n",
    "    'subsample':                     1,\n",
    "    'gamma':                         0,\n",
    "    'reg_lambda':                    10,\n",
    "    'scale_pos_weight':              1,\n",
    "    'tree_method':                  'hist', \n",
    "    'n_estimators':                  1000,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "t1_start = perf_counter()  # Time fit function\n",
    "model_xgb= xgb.XGBClassifier(**xgb_params)\n",
    "model_xgb.fit(X_train,y_train, eval_metric='logloss', eval_set=evalset, verbose=True)\n",
    "t1_stop = perf_counter()\n",
    "print (\"It took\", t1_stop-t1_start,\"seconds to fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check model accuracy\n",
    "result_predict_xgb_test = model_xgb.predict(X_test)\n",
    "acc = np.mean(y_test == result_predict_xgb_test)\n",
    "print(\"Model accuracy =\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve performance metrics\n",
    "results = model_xgb.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['validation_0']['logloss'], label='train')\n",
    "plt.plot(results['validation_1']['logloss'], label='test')\n",
    "# display legend\n",
    "plt.legend()\n",
    "# render\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## So how many trees do we need really?\n",
    "\n",
    "* It takes awhile to watch 1000 trees get evaluated, a great performance improvement is to use the XGBoost early stopping capability.\n",
    "\n",
    "* Modify the fit function to stop the training after 10 to 15 rounds of no improvement.  \n",
    "        \n",
    "        model_xgb.fit(X_train,y_train, early_stopping_rounds=10, eval_metric='logloss', eval_set=evalset, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "* Depending on how large a dataset you used this will vary.  There are numerous other optimizations that one can undertake, hopefully this gets you started.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "* We covered how to set parameters for XGBoost.\n",
    "* How to enable Intel's SciKit-Learn features\n",
    "* How to use CV to identify better hyperparameter options\n",
    "* How to use a learning curve to estimate the number of trees\n",
    "* How to use early stopping to optimize training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
