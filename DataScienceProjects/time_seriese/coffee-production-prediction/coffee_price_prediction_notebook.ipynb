{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Coffee Price Prediction — Notebook\n",
    "\n",
    "**Goal:** Build reproducible pipelines and ML models to forecast global coffee futures prices (e.g., ICE Arabica / Yahoo ticker `KC=F`) using market, macro and weather features.\n",
    "\n",
    "**Data sources (examples to download inside the notebook):**\n",
    "- ICO (International Coffee Organization) historical indicators and market reports. \n",
    "- Yahoo Finance futures ticker `KC=F` (download via `yfinance` or Yahoo historical CSV).\n",
    "- Kaggle public coffee price datasets as ready CSVs for quick experimentation.\n",
    "\n",
    "This notebook is structured for production-ready experimentation: data acquisition → EDA → features → models → evaluation → deployment artifacts.\n",
    "\n",
    "--\n",
    "\n",
    "## Usage notes\n",
    "1. Run cells in order. Internet access is required for the 'data download' cells which use `yfinance`, `requests` or Kaggle APIs. \n",
    "2. If you run on a cloud notebook, enable internet and set Kaggle credentials (if using Kaggle datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Data sources (suggested)\n",
    "\n",
    "- ICO historical data / indicators (Composite indicator prices, group prices): https://ico.org/historical-data-on-the-global-coffee-trade/  \n",
    "- Yahoo Finance futures (KC=F) historical: https://finance.yahoo.com/quote/KC%3DF/history/  \n",
    "- Kaggle coffee price datasets (examples): https://www.kaggle.com/datasets/timmofeyy/coffee-prices-historical-data  \n",
    "\n",
    "(Use these links to download CSVs or call APIs directly from the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup (run once)\n",
    "# !pip install yfinance xgboost nbformat pandas scikit-learn statsmodels tensorflow matplotlib seaborn kaggle optuna\n",
    "\n",
    "# Note: on many managed notebooks yfinance and xgboost are preinstalled.\n",
    "print('Install the required packages if not present: yfinance, xgboost, statsmodels, tensorflow, optuna, kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion examples\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# 1) Download coffee futures using yfinance (KC=F). Requires internet.\n",
    "def fetch_coffee_yahoo(start='1990-01-01', end=None):\n",
    "    import yfinance as yf\n",
    "    if end is None:\n",
    "        end = dt.date.today().isoformat()\n",
    "    ticker = 'KC=F'\n",
    "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
    "    df = df.rename(columns={'Close':'close','Open':'open','High':'high','Low':'low','Volume':'volume'})\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "# 2) Load ICO CSV (if downloaded locally)\n",
    "def load_ico_csv(path):\n",
    "    return pd.read_excel(path, sheet_name=None)  # ICO provides multiple sheets sometimes\n",
    "\n",
    "# 3) Load Kaggle CSV (if downloaded)\n",
    "def load_kaggle_csv(path):\n",
    "    return pd.read_csv(path, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Example usage (uncomment to run with internet):\n",
    "# coffee = fetch_coffee_yahoo('1980-01-01')\n",
    "# print(coffee.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "- Visualize price series, seasonality, rolling statistics.\n",
    "- Inspect missing data and contract roll effects (futures continuous contracts need stitching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA snippets (requires coffee DataFrame from ingestion cell)\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_series(df, col='close', title='Coffee price'):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df[col])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price (USD/lb)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot_series(coffee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "- Lag features (t-1 ... t-30)\n",
    "- Rolling mean/std (7, 30, 90 days)\n",
    "- Technical indicators (momentum, RSI, ATR)\n",
    "- Macro features: USD index, Oil price, S&P500, Shipping costs\n",
    "- Weather/agronomic indices for origin countries (rainfall anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering examples\n",
    "import numpy as np\n",
    "\n",
    "def make_features(df, lags=[1,3,7,14,30]):\n",
    "    data = df.copy()\n",
    "    data['log_close'] = np.log(data['close'])\n",
    "    for l in lags:\n",
    "        data[f'lag_{l}'] = data['log_close'].shift(l)\n",
    "    # rolling\n",
    "    data['roll7_mean'] = data['log_close'].rolling(7).mean()\n",
    "    data['roll30_mean'] = data['log_close'].rolling(30).mean()\n",
    "    data['roll7_std'] = data['log_close'].rolling(7).std()\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# feats = make_features(coffee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Modeling — Baseline (ARIMA / SARIMAX)\n",
    "Use statsmodels SARIMAX as a strong statistical baseline. Fit on log prices; include exogenous regressors if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA baseline (example)\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def arima_backtest(train_series, order=(1,1,1), seasonal_order=(0,0,0,0)):\n",
    "    model = SARIMAX(train_series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    res = model.fit(disp=False)\n",
    "    return res\n",
    "\n",
    "# Example usage:\n",
    "# res = arima_backtest(np.log(coffee['close']).dropna())\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Modeling — Tree-based (XGBoost / LightGBM)\n",
    "Create supervised dataset with sliding-window forecasting (predict next-day or next-month price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost example (supervised forecasting)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_xgb(X, y):\n",
    "    params = {'objective':'reg:squarederror', 'n_estimators':200, 'learning_rate':0.05, 'max_depth':4}\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Example usage (after building X,y):\n",
    "# model = train_xgb(X_train, y_train)\n",
    "# preds = model.predict(X_test)\n",
    "# print('RMSE', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Modeling — Deep Learning (LSTM)\n",
    "Use a scaled sequence dataset and a simple LSTM model for comparison. Useful for capturing non-linear temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM example (Keras)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def build_lstm(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(n_timesteps, n_features), return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Note: prepare 3D arrays for LSTM (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Evaluation & Backtesting\n",
    "- Use rolling-origin evaluation (walk-forward)\n",
    "- Report RMSE, MAE, MAPE for holdout windows\n",
    "- Check economic metrics: P&L of simple hedging strategy using predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward backtest skeleton\n",
    "def walk_forward_forecast(X, y, model_fn, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    metrics = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = model_fn(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "    return metrics\n",
    "\n",
    "# Example usage:\n",
    "# metrics = walk_forward_forecast(X, y, lambda Xt, yt: train_xgb(Xt, yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Deployment & Next steps\n",
    "- Package pipeline with `mlflow` or `kedro`.\n",
    "- Export model artifacts and scaler.\n",
    "- Build a lightweight dashboard (Streamlit / Dash) to show real-time predictions and sell/hold recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### Appendix\n",
    "- The notebook includes commented instructions to download ICO, Yahoo and Kaggle datasets. Replace file paths or uncomment download cells when running on a machine with internet access."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
