{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 0.004897,
     "end_time": "2025-09-28T02:42:14.536023",
     "exception": false,
     "start_time": "2025-09-28T02:42:14.531126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I've created a comprehensive Jupyter notebook for crowd disaster management and stampede prevention! This system is designed to predict dangerous situations before they occur using real-time data from traffic cameras, sensors, and other monitoring systems.\n",
    "Key Features of the System:\n",
    "\n",
    "🔍 Core Components:\n",
    "\n",
    "Real-time Data Processing - Simulates integration with traffic cameras, IoT sensors, weather APIs\n",
    "AI-Powered Risk Prediction - Uses Random Forest ML model to predict disaster probability\n",
    "Anomaly Detection - Identifies unusual crowd patterns using Isolation Forest\n",
    "Multi-zone Monitoring - Tracks different areas simultaneously\n",
    "Automated Alert System - Generates actionable alerts with specific recommendations\n",
    "\n",
    "📊 Risk Indicators Monitored:\n",
    "\n",
    "Crowd Density - People per square meter vs historical averages\n",
    "Flow Rate - How quickly crowds are moving through spaces\n",
    "Traffic Congestion - Vehicle flow that might affect crowd movement\n",
    "Bottleneck Detection - High density + low flow = danger zone\n",
    "Environmental Factors - Weather, temperature, special events\n",
    "\n",
    "🚨 Alert Levels:\n",
    "\n",
    "CRITICAL - High disaster probability (>70%), immediate action required\n",
    "WARNING - Anomalous patterns or concerning density ratios\n",
    "MEDIUM - Elevated risk factors requiring monitoring\n",
    "LOW - Normal operations\n",
    "\n",
    "🎯 Predictive Capabilities:\n",
    "The system can predict dangerous situations by analyzing:\n",
    "\n",
    "Current crowd density compared to historical norms (2.5x = danger threshold)\n",
    "Crowd movement velocity (slower = more dangerous)\n",
    "Traffic flow impacts on pedestrian areas\n",
    "Special events that increase crowd gathering\n",
    "Weather conditions affecting crowd behavior\n",
    "\n",
    "📈 Real-world Applications:\n",
    "\n",
    "Shopping malls during sales events\n",
    "Transportation hubs during peak hours\n",
    "Sports venues and concert halls\n",
    "Religious gatherings and festivals\n",
    "Emergency evacuations\n",
    "\n",
    "The notebook includes a complete simulation showing how the system would detect a high-risk scenario (high density, slow movement, special event) and automatically generate alerts with specific action recommendations like deploying crowd control personnel and opening alternative exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 27.730351,
     "end_time": "2025-09-28T02:42:42.270381",
     "exception": false,
     "start_time": "2025-09-28T02:42:14.540030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Some advanced features may not work.\")\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.012129,
     "end_time": "2025-09-28T02:42:42.287426",
     "exception": false,
     "start_time": "2025-09-28T02:42:42.275297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crowd Disaster Management and Stampede Prevention System\n",
    "# Real-time Analysis and Prediction Notebook\n",
    "\n",
    "print(\"🚨 CROWD DISASTER MANAGEMENT PREDICTION SYSTEM 🚨\")\n",
    "print(\"=\"*60)\n",
    "print(\"Initializing real-time crowd analysis and stampede prevention system...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.019123,
     "end_time": "2025-09-28T02:42:42.311157",
     "exception": false,
     "start_time": "2025-09-28T02:42:42.292034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1. DATA SIMULATION AND COLLECTION FRAMEWORK\n",
    "\n",
    "def generate_synthetic_crowd_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Generate synthetic crowd and traffic data for training and testing\n",
    "    This simulates real-world data from cameras, sensors, and traffic systems\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Time-based features\n",
    "    timestamps = pd.date_range(start='2024-01-01', periods=n_samples, freq='5min')\n",
    "    hours = [t.hour for t in timestamps]\n",
    "    days_of_week = [t.dayofweek for t in timestamps]\n",
    "    is_weekend = [1 if d >= 5 else 0 for d in days_of_week]\n",
    "    \n",
    "    # Location features (simulating different areas/zones)\n",
    "    zones = np.random.choice(['Zone_A', 'Zone_B', 'Zone_C', 'Zone_D', 'Zone_E'], n_samples)\n",
    "    \n",
    "    # Crowd density features (people per square meter)\n",
    "    base_density = np.random.normal(2.5, 1.0, n_samples)\n",
    "    base_density = np.clip(base_density, 0, 10)\n",
    "    \n",
    "    # Traffic flow features (vehicles per minute)\n",
    "    traffic_flow = np.random.normal(15, 5, n_samples)\n",
    "    traffic_flow = np.clip(traffic_flow, 0, 50)\n",
    "    \n",
    "    # Environmental factors\n",
    "    weather_conditions = np.random.choice(['Clear', 'Rainy', 'Foggy'], n_samples, p=[0.7, 0.2, 0.1])\n",
    "    temperature = np.random.normal(25, 10, n_samples)\n",
    "    \n",
    "    # Event-based factors\n",
    "    special_events = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])  # 10% chance of special event\n",
    "    \n",
    "    # Calculate derived features\n",
    "    crowd_velocity = np.random.normal(1.2, 0.3, n_samples)  # m/s\n",
    "    crowd_velocity = np.clip(crowd_velocity, 0, 3)\n",
    "    \n",
    "    # Crowd flow rate (people crossing per minute)\n",
    "    flow_rate = base_density * crowd_velocity * 10 + np.random.normal(0, 2, n_samples)\n",
    "    flow_rate = np.clip(flow_rate, 0, 100)\n",
    "    \n",
    "    # Historical average for comparison\n",
    "    historical_avg_density = np.random.normal(2.0, 0.5, n_samples)\n",
    "    historical_avg_traffic = np.random.normal(12, 3, n_samples)\n",
    "    \n",
    "    # Density ratio (current vs historical)\n",
    "    density_ratio = base_density / (historical_avg_density + 0.1)\n",
    "    traffic_ratio = traffic_flow / (historical_avg_traffic + 0.1)\n",
    "    \n",
    "    # Risk indicators\n",
    "    congestion_level = np.where(\n",
    "        (density_ratio > 2.5) & (flow_rate < 20), 1,  # High density, low flow\n",
    "        np.where((density_ratio > 2.0) & (traffic_ratio > 1.5), 1, 0)  # High density and traffic\n",
    "    )\n",
    "    \n",
    "    # Create target variable (disaster risk)\n",
    "    # Risk factors: high density ratio, low velocity, special events, congestion\n",
    "    risk_score = (\n",
    "        (density_ratio - 1) * 0.3 +\n",
    "        (1 / (crowd_velocity + 0.1)) * 0.2 +\n",
    "        special_events * 0.3 +\n",
    "        congestion_level * 0.2\n",
    "    )\n",
    "    \n",
    "    # Add some noise and create binary classification\n",
    "    risk_score += np.random.normal(0, 0.1, n_samples)\n",
    "    disaster_risk = np.where(risk_score > 1.2, 1, 0)  # Binary classification\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'hour': hours,\n",
    "        'day_of_week': days_of_week,\n",
    "        'is_weekend': is_weekend,\n",
    "        'zone': zones,\n",
    "        'crowd_density': base_density,\n",
    "        'traffic_flow': traffic_flow,\n",
    "        'weather': weather_conditions,\n",
    "        'temperature': temperature,\n",
    "        'special_event': special_events,\n",
    "        'crowd_velocity': crowd_velocity,\n",
    "        'flow_rate': flow_rate,\n",
    "        'historical_avg_density': historical_avg_density,\n",
    "        'historical_avg_traffic': historical_avg_traffic,\n",
    "        'density_ratio': density_ratio,\n",
    "        'traffic_ratio': traffic_ratio,\n",
    "        'congestion_level': congestion_level,\n",
    "        'risk_score': risk_score,\n",
    "        'disaster_risk': disaster_risk\n",
    "    })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.106691,
     "end_time": "2025-09-28T02:42:42.422423",
     "exception": false,
     "start_time": "2025-09-28T02:42:42.315732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "print(\"📊 Generating synthetic crowd and traffic data...\")\n",
    "crowd_data = generate_synthetic_crowd_data(10000)\n",
    "print(f\"✅ Generated {len(crowd_data)} data points\")\n",
    "print(f\"🚨 Disaster risk cases: {crowd_data['disaster_risk'].sum()} ({crowd_data['disaster_risk'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 1.865082,
     "end_time": "2025-09-28T02:42:44.292068",
     "exception": false,
     "start_time": "2025-09-28T02:42:42.426986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "def analyze_crowd_patterns(data):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of crowd patterns and risk factors\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 CROWD PATTERN ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"Total records: {len(data):,}\")\n",
    "    print(f\"Date range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
    "    print(f\"High-risk incidents: {data['disaster_risk'].sum():,} ({data['disaster_risk'].mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Crowd Disaster Risk Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Crowd density distribution by risk level\n",
    "    axes[0,0].hist(data[data['disaster_risk']==0]['crowd_density'], alpha=0.7, label='Low Risk', bins=30, color='green')\n",
    "    axes[0,0].hist(data[data['disaster_risk']==1]['crowd_density'], alpha=0.7, label='High Risk', bins=30, color='red')\n",
    "    axes[0,0].set_xlabel('Crowd Density (people/m²)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].set_title('Crowd Density Distribution by Risk Level')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Risk by hour of day\n",
    "    hourly_risk = data.groupby('hour')['disaster_risk'].agg(['mean', 'count']).reset_index()\n",
    "    axes[0,1].bar(hourly_risk['hour'], hourly_risk['mean'], color='orange', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Hour of Day')\n",
    "    axes[0,1].set_ylabel('Risk Probability')\n",
    "    axes[0,1].set_title('Disaster Risk Probability by Hour')\n",
    "    axes[0,1].set_xticks(range(0, 24, 2))\n",
    "    \n",
    "    # 3. Density ratio vs Flow rate (risk identification)\n",
    "    scatter_colors = ['red' if risk else 'blue' for risk in data['disaster_risk']]\n",
    "    axes[0,2].scatter(data['density_ratio'], data['flow_rate'], c=scatter_colors, alpha=0.6, s=20)\n",
    "    axes[0,2].set_xlabel('Density Ratio (Current/Historical)')\n",
    "    axes[0,2].set_ylabel('Flow Rate (people/min)')\n",
    "    axes[0,2].set_title('Risk Pattern: Density vs Flow Rate')\n",
    "    axes[0,2].axvline(x=2.0, color='red', linestyle='--', alpha=0.7, label='High Density Threshold')\n",
    "    axes[0,2].axhline(y=30, color='red', linestyle='--', alpha=0.7, label='Low Flow Threshold')\n",
    "    axes[0,2].legend()\n",
    "    \n",
    "    # 4. Zone-wise risk distribution\n",
    "    zone_risk = data.groupby('zone')['disaster_risk'].agg(['mean', 'count']).reset_index()\n",
    "    axes[1,0].bar(zone_risk['zone'], zone_risk['mean'], color='purple', alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Zone')\n",
    "    axes[1,0].set_ylabel('Risk Probability')\n",
    "    axes[1,0].set_title('Risk Probability by Zone')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Traffic flow impact on risk\n",
    "    axes[1,1].boxplot([data[data['disaster_risk']==0]['traffic_flow'], \n",
    "                      data[data['disaster_risk']==1]['traffic_flow']], \n",
    "                     labels=['Low Risk', 'High Risk'])\n",
    "    axes[1,1].set_ylabel('Traffic Flow (vehicles/min)')\n",
    "    axes[1,1].set_title('Traffic Flow Distribution by Risk Level')\n",
    "    \n",
    "    # 6. Time series of risk incidents\n",
    "    daily_risk = data.set_index('timestamp').resample('D')['disaster_risk'].sum()\n",
    "    axes[1,2].plot(daily_risk.index, daily_risk.values, color='darkred', linewidth=2)\n",
    "    axes[1,2].set_xlabel('Date')\n",
    "    axes[1,2].set_ylabel('Daily Risk Incidents')\n",
    "    axes[1,2].set_title('Risk Incidents Over Time')\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return hourly_risk, zone_risk\n",
    "\n",
    "# Perform analysis\n",
    "hourly_patterns, zone_patterns = analyze_crowd_patterns(crowd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.018096,
     "end_time": "2025-09-28T02:42:44.319954",
     "exception": false,
     "start_time": "2025-09-28T02:42:44.301858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3. REAL-TIME ANOMALY DETECTION SYSTEM\n",
    "\n",
    "class CrowdAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Real-time anomaly detection for unusual crowd patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Train the anomaly detection model\n",
    "        \"\"\"\n",
    "        features = ['crowd_density', 'traffic_flow', 'flow_rate', 'density_ratio', 'traffic_ratio']\n",
    "        X = data[features].fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit isolation forest\n",
    "        self.isolation_forest.fit(X_scaled)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(\"✅ Anomaly detection model trained successfully\")\n",
    "        \n",
    "    def detect_anomalies(self, data):\n",
    "        \"\"\"\n",
    "        Detect anomalies in real-time data\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "            \n",
    "        features = ['crowd_density', 'traffic_flow', 'flow_rate', 'density_ratio', 'traffic_ratio']\n",
    "        X = data[features].fillna(0)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "        anomaly_predictions = self.isolation_forest.predict(X_scaled)\n",
    "        anomaly_scores = self.isolation_forest.decision_function(X_scaled)\n",
    "        \n",
    "        return anomaly_predictions, anomaly_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 1.002177,
     "end_time": "2025-09-28T02:42:45.331918",
     "exception": false,
     "start_time": "2025-09-28T02:42:44.329741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and train anomaly detector\n",
    "print(\"\\n🔍 Training Anomaly Detection System...\")\n",
    "anomaly_detector = CrowdAnomalyDetector()\n",
    "anomaly_detector.fit(crowd_data)\n",
    "\n",
    "# Detect anomalies in the dataset\n",
    "anomaly_preds, anomaly_scores = anomaly_detector.detect_anomalies(crowd_data)\n",
    "crowd_data['anomaly'] = anomaly_preds\n",
    "crowd_data['anomaly_score'] = anomaly_scores\n",
    "\n",
    "print(f\"🚨 Detected {(anomaly_preds == -1).sum()} anomalous patterns out of {len(crowd_data)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.027654,
     "end_time": "2025-09-28T02:42:45.369735",
     "exception": false,
     "start_time": "2025-09-28T02:42:45.342081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 4. MACHINE LEARNING RISK PREDICTION MODEL\n",
    "\n",
    "class DisasterRiskPredictor:\n",
    "    \"\"\"\n",
    "    Advanced machine learning model for disaster risk prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def prepare_features(self, data, fit_encoders=True):\n",
    "        \"\"\"\n",
    "        Prepare features for machine learning\n",
    "        \"\"\"\n",
    "        # Select features\n",
    "        numerical_features = [\n",
    "            'hour', 'day_of_week', 'is_weekend', 'crowd_density', 'traffic_flow',\n",
    "            'temperature', 'special_event', 'crowd_velocity', 'flow_rate',\n",
    "            'density_ratio', 'traffic_ratio', 'congestion_level'\n",
    "        ]\n",
    "        \n",
    "        categorical_features = ['zone', 'weather']\n",
    "        \n",
    "        # Prepare numerical features\n",
    "        X_num = data[numerical_features].fillna(0)\n",
    "        \n",
    "        # Prepare categorical features\n",
    "        X_cat = data[categorical_features].copy()\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            if fit_encoders:\n",
    "                if col not in self.label_encoders:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                X_cat[col] = self.label_encoders[col].fit_transform(X_cat[col].fillna('Unknown'))\n",
    "            else:\n",
    "                if col in self.label_encoders:\n",
    "                    # Handle unknown categories\n",
    "                    known_categories = set(self.label_encoders[col].classes_)\n",
    "                    X_cat[col] = X_cat[col].fillna('Unknown')\n",
    "                    X_cat[col] = X_cat[col].apply(lambda x: x if x in known_categories else 'Unknown')\n",
    "                    X_cat[col] = self.label_encoders[col].transform(X_cat[col])\n",
    "                else:\n",
    "                    X_cat[col] = 0\n",
    "        \n",
    "        # Combine features\n",
    "        X = pd.concat([X_num, X_cat], axis=1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        Train the disaster risk prediction model\n",
    "        \"\"\"\n",
    "        print(\"\\n🤖 Training Disaster Risk Prediction Model...\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = self.prepare_features(data, fit_encoders=True)\n",
    "        y = data['disaster_risk']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        self.rf_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_score = self.rf_model.score(X_train_scaled, y_train)\n",
    "        test_score = self.rf_model.score(X_test_scaled, y_test)\n",
    "        \n",
    "        print(f\"✅ Model trained successfully\")\n",
    "        print(f\"📈 Training accuracy: {train_score:.3f}\")\n",
    "        print(f\"📈 Testing accuracy: {test_score:.3f}\")\n",
    "        \n",
    "        # Get feature importance\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Predictions for evaluation\n",
    "        y_pred = self.rf_model.predict(X_test_scaled)\n",
    "        \n",
    "        print(\"\\n📊 Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, \n",
    "                                  target_names=['Low Risk', 'High Risk']))\n",
    "        \n",
    "        return X_test, y_test, y_pred\n",
    "    \n",
    "    def predict_risk(self, data):\n",
    "        \"\"\"\n",
    "        Predict disaster risk for new data\n",
    "        \"\"\"\n",
    "        X = self.prepare_features(data, fit_encoders=False)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        predictions = self.rf_model.predict(X_scaled)\n",
    "        probabilities = self.rf_model.predict_proba(X_scaled)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Plot feature importance\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = self.feature_importance.head(10)\n",
    "        \n",
    "        plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 10 Most Important Features for Risk Prediction')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        for i, v in enumerate(top_features['importance']):\n",
    "            plt.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 1.105661,
     "end_time": "2025-09-28T02:42:46.485247",
     "exception": false,
     "start_time": "2025-09-28T02:42:45.379586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the risk prediction model\n",
    "risk_predictor = DisasterRiskPredictor()\n",
    "X_test, y_test, y_pred = risk_predictor.train(crowd_data)\n",
    "\n",
    "# Plot feature importance\n",
    "risk_predictor.plot_feature_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.028586,
     "end_time": "2025-09-28T02:42:46.526818",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.498232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 5. REAL-TIME MONITORING AND ALERT SYSTEM\n",
    "\n",
    "class RealTimeMonitor:\n",
    "    \"\"\"\n",
    "    Real-time monitoring system for crowd disaster prevention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, risk_predictor, anomaly_detector):\n",
    "        self.risk_predictor = risk_predictor\n",
    "        self.anomaly_detector = anomaly_detector\n",
    "        self.alert_thresholds = {\n",
    "            'HIGH_RISK': 0.7,      # High disaster risk probability\n",
    "            'ANOMALY': -0.3,       # Anomaly score threshold\n",
    "            'DENSITY_RATIO': 2.5,   # Density compared to historical\n",
    "            'LOW_FLOW': 20          # Low crowd flow rate\n",
    "        }\n",
    "        \n",
    "    def analyze_current_situation(self, current_data):\n",
    "        \"\"\"\n",
    "        Analyze current crowd situation and generate alerts\n",
    "        \"\"\"\n",
    "        # Predict risk\n",
    "        risk_pred, risk_prob = self.risk_predictor.predict_risk(current_data)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        anomaly_pred, anomaly_score = self.anomaly_detector.detect_anomalies(current_data)\n",
    "        \n",
    "        # Generate comprehensive assessment\n",
    "        assessment = {\n",
    "            'timestamp': current_data['timestamp'].iloc[0] if 'timestamp' in current_data else datetime.now(),\n",
    "            'zone': current_data['zone'].iloc[0] if 'zone' in current_data else 'Unknown',\n",
    "            'risk_probability': risk_prob[0][1] if len(risk_prob) > 0 else 0,\n",
    "            'is_high_risk': risk_pred[0] == 1 if len(risk_pred) > 0 else False,\n",
    "            'is_anomaly': anomaly_pred[0] == -1 if len(anomaly_pred) > 0 else False,\n",
    "            'anomaly_score': anomaly_score[0] if len(anomaly_score) > 0 else 0,\n",
    "            'crowd_density': current_data['crowd_density'].iloc[0],\n",
    "            'density_ratio': current_data['density_ratio'].iloc[0],\n",
    "            'flow_rate': current_data['flow_rate'].iloc[0],\n",
    "            'traffic_flow': current_data['traffic_flow'].iloc[0],\n",
    "        }\n",
    "        \n",
    "        # Generate alerts\n",
    "        alerts = self.generate_alerts(assessment)\n",
    "        assessment['alerts'] = alerts\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def generate_alerts(self, assessment):\n",
    "        \"\"\"\n",
    "        Generate specific alerts based on assessment\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # High risk alert\n",
    "        if assessment['risk_probability'] > self.alert_thresholds['HIGH_RISK']:\n",
    "            alerts.append({\n",
    "                'level': 'CRITICAL',\n",
    "                'type': 'HIGH_DISASTER_RISK',\n",
    "                'message': f\"🚨 CRITICAL: High disaster risk detected! Probability: {assessment['risk_probability']:.1%}\",\n",
    "                'action': 'Implement crowd control measures immediately'\n",
    "            })\n",
    "        \n",
    "        # Anomaly alert\n",
    "        if assessment['anomaly_score'] < self.alert_thresholds['ANOMALY']:\n",
    "            alerts.append({\n",
    "                'level': 'WARNING',\n",
    "                'type': 'ANOMALY_DETECTED',\n",
    "                'message': f\"⚠️  WARNING: Unusual crowd pattern detected (Score: {assessment['anomaly_score']:.2f})\",\n",
    "                'action': 'Monitor situation closely'\n",
    "            })\n",
    "        \n",
    "        # High density alert\n",
    "        if assessment['density_ratio'] > self.alert_thresholds['DENSITY_RATIO']:\n",
    "            alerts.append({\n",
    "                'level': 'WARNING',\n",
    "                'type': 'HIGH_DENSITY',\n",
    "                'message': f\"⚠️  WARNING: Crowd density {assessment['density_ratio']:.1f}x higher than normal\",\n",
    "                'action': 'Consider crowd dispersal measures'\n",
    "            })\n",
    "        \n",
    "        # Low flow alert (potential bottleneck)\n",
    "        if (assessment['crowd_density'] > 3.0 and \n",
    "            assessment['flow_rate'] < self.alert_thresholds['LOW_FLOW']):\n",
    "            alerts.append({\n",
    "                'level': 'WARNING',\n",
    "                'type': 'LOW_FLOW_BOTTLENECK',\n",
    "                'message': f\"⚠️  WARNING: Potential bottleneck - High density ({assessment['crowd_density']:.1f}) with low flow ({assessment['flow_rate']:.1f})\",\n",
    "                'action': 'Check for obstructions and improve flow'\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def print_assessment(self, assessment):\n",
    "        \"\"\"\n",
    "        Print formatted assessment report\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🔍 REAL-TIME CROWD SITUATION ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"📍 Zone: {assessment['zone']}\")\n",
    "        print(f\"⏰ Time: {assessment['timestamp']}\")\n",
    "        print(f\"🎯 Risk Probability: {assessment['risk_probability']:.1%}\")\n",
    "        print(f\"👥 Crowd Density: {assessment['crowd_density']:.1f} people/m²\")\n",
    "        print(f\"📊 Density Ratio: {assessment['density_ratio']:.1f}x historical\")\n",
    "        print(f\"🌊 Flow Rate: {assessment['flow_rate']:.1f} people/min\")\n",
    "        print(f\"🚗 Traffic Flow: {assessment['traffic_flow']:.1f} vehicles/min\")\n",
    "        \n",
    "        if assessment['alerts']:\n",
    "            print(f\"\\n🚨 ACTIVE ALERTS ({len(assessment['alerts'])})\")\n",
    "            print(\"-\" * 40)\n",
    "            for alert in assessment['alerts']:\n",
    "                print(f\"[{alert['level']}] {alert['message']}\")\n",
    "                print(f\"   → Action: {alert['action']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"\\n✅ No alerts - Situation appears normal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 0.019293,
     "end_time": "2025-09-28T02:42:46.557760",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.538467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize real-time monitor\n",
    "print(\"\\n📡 Initializing Real-Time Monitoring System...\")\n",
    "real_time_monitor = RealTimeMonitor(risk_predictor, anomaly_detector)\n",
    "print(\"✅ Real-time monitoring system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.020292,
     "end_time": "2025-09-28T02:42:46.589682",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.569390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 6. SIMULATION OF REAL-TIME MONITORING\n",
    "\n",
    "def simulate_real_time_monitoring(data, monitor, n_samples=5):\n",
    "    \"\"\"\n",
    "    Simulate real-time monitoring with sample data points\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎬 SIMULATING REAL-TIME MONITORING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample some high-risk and normal situations\n",
    "    high_risk_samples = data[data['disaster_risk'] == 1].sample(min(3, len(data[data['disaster_risk'] == 1])))\n",
    "    normal_samples = data[data['disaster_risk'] == 0].sample(min(2, len(data[data['disaster_risk'] == 0])))\n",
    "    \n",
    "    sample_data = pd.concat([high_risk_samples, normal_samples]).sample(n_samples)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(sample_data.iterrows(), 1):\n",
    "        print(f\"\\n📊 MONITORING SAMPLE {idx}/{n_samples}\")\n",
    "        \n",
    "        # Convert row to DataFrame for processing\n",
    "        current_data = pd.DataFrame([row])\n",
    "        \n",
    "        # Analyze situation\n",
    "        assessment = monitor.analyze_current_situation(current_data)\n",
    "        \n",
    "        # Print assessment\n",
    "        monitor.print_assessment(assessment)\n",
    "        \n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": 0.272162,
     "end_time": "2025-09-28T02:42:46.873288",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.601126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "simulate_real_time_monitoring(crowd_data, real_time_monitor, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.019479,
     "end_time": "2025-09-28T02:42:46.904422",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.884943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. RECOMMENDATIONS AND ACTION PLANS\n",
    "\n",
    "def generate_prevention_recommendations():\n",
    "    \"\"\"\n",
    "    Generate comprehensive disaster prevention recommendations\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'immediate_actions': [\n",
    "            \"Deploy additional security personnel in high-risk zones\",\n",
    "            \"Set up crowd flow monitoring cameras at bottleneck points\",\n",
    "            \"Establish emergency evacuation routes and mark them clearly\",\n",
    "            \"Install crowd density sensors in critical areas\",\n",
    "            \"Create real-time communication system with field teams\"\n",
    "        ],\n",
    "        \n",
    "        'crowd_management': [\n",
    "            \"Implement one-way flow systems in narrow passages\",\n",
    "            \"Use barriers to guide crowd movement and prevent clustering\",\n",
    "            \"Deploy crowd control personnel at strategic chokepoints\",\n",
    "            \"Create multiple entry/exit points to distribute flow\",\n",
    "            \"Use public announcement systems for crowd direction\"\n",
    "        ],\n",
    "        \n",
    "        'technology_integration': [\n",
    "            \"Install AI-powered video analytics for automatic crowd counting\",\n",
    "            \"Deploy IoT sensors for real-time density measurement\",\n",
    "            \"Use mobile apps to provide crowd updates to visitors\",\n",
    "            \"Implement predictive modeling for event planning\",\n",
    "            \"Set up automated alert systems for emergency response\"\n",
    "        ],\n",
    "        \n",
    "        'emergency_response': [\n",
    "            \"Train staff on stampede prevention and response protocols\",\n",
    "            \"Establish rapid response teams at strategic locations\",\n",
    "            \"Create communication protocols with local emergency services\",\n",
    "            \"Develop evacuation procedures for different scenarios\",\n",
    "            \"Conduct regular emergency drills and simulations\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 COMPREHENSIVE PREVENTION RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for category, actions in recommendations.items():\n",
    "        print(f\"\\n🎯 {category.upper().replace('_', ' ')}\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, action in enumerate(actions, 1):\n",
    "            print(f\"{i}. {action}\")\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": 0.017631,
     "end_time": "2025-09-28T02:42:46.933162",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.915531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "prevention_recommendations = generate_prevention_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.023789,
     "end_time": "2025-09-28T02:42:46.968366",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.944577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. MODEL PERFORMANCE DASHBOARD\n",
    "\n",
    "def create_performance_dashboard(data, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create comprehensive model performance dashboard\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 MODEL PERFORMANCE DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create confusion matrix visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Crowd Disaster Prediction System - Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
    "                xticklabels=['Low Risk', 'High Risk'],\n",
    "                yticklabels=['Low Risk', 'High Risk'])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    \n",
    "    # 2. Risk Distribution\n",
    "    risk_dist = data['disaster_risk'].value_counts()\n",
    "    axes[0,1].pie(risk_dist.values, labels=['Low Risk', 'High Risk'], \n",
    "                  autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "    axes[0,1].set_title('Risk Distribution in Dataset')\n",
    "    \n",
    "    # 3. Anomaly Detection Results\n",
    "    anomaly_dist = pd.Series(data['anomaly']).value_counts()\n",
    "    axes[1,0].bar(['Normal', 'Anomaly'], [anomaly_dist[1], anomaly_dist[-1]], \n",
    "                  color=['blue', 'red'], alpha=0.7)\n",
    "    axes[1,0].set_title('Anomaly Detection Results')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    \n",
    "    # 4. Risk Score Distribution\n",
    "    axes[1,1].hist(data[data['disaster_risk']==0]['risk_score'], alpha=0.7, \n",
    "                   label='Low Risk', bins=30, color='green')\n",
    "    axes[1,1].hist(data[data['disaster_risk']==1]['risk_score'], alpha=0.7, \n",
    "                   label='High Risk', bins=30, color='red')\n",
    "    axes[1,1].set_xlabel('Risk Score')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].set_title('Risk Score Distribution')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "papermill": {
     "duration": 0.029425,
     "end_time": "2025-09-28T02:42:47.009184",
     "exception": false,
     "start_time": "2025-09-28T02:42:46.979759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Print key metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n📈 MODEL PERFORMANCE METRICS:\")\n",
    "print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall:    {recall:.3f}\")\n",
    "print(f\"   F1-Score:  {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.357122,
   "end_time": "2025-09-28T02:42:50.203722",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-28T02:42:07.846600",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
