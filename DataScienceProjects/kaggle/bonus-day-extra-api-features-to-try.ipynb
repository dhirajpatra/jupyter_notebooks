{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2024 Google LLC.","metadata":{}},{"cell_type":"code","source":"# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bonus Day - So long and farewell!\n\nCongrats on finishing the 5-day Generative AI Intensive course from Kaggle and Google!\n\nThis notebook is a \"bonus episode\" that highlights a few more things you can do with the Gemini API that weren't covered during the course. This material doesn't pair with the whitepapers or podcast, but covers some extra features that you might find useful when building Gemini API powered apps.","metadata":{}},{"cell_type":"markdown","source":"## Get set up\n\nInstall the SDK and other tools for this notebook, then import the package and set up a retry policy so you don't have to manually retry when you hit a quota limit.","metadata":{}},{"cell_type":"code","source":"%pip install -q google-generativeai pydub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:33:29.047308Z","iopub.execute_input":"2024-11-15T04:33:29.047769Z","iopub.status.idle":"2024-11-15T04:33:44.840005Z","shell.execute_reply.started":"2024-11-15T04:33:29.047727Z","shell.execute_reply":"2024-11-15T04:33:44.838518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You do not have to restart the kernel.","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nfrom google.api_core import retry\n\nretry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T06:49:46.027424Z","iopub.execute_input":"2024-11-15T06:49:46.027859Z","iopub.status.idle":"2024-11-15T06:49:47.451617Z","shell.execute_reply.started":"2024-11-15T06:49:46.027815Z","shell.execute_reply":"2024-11-15T06:49:47.450191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T06:49:48.791827Z","iopub.execute_input":"2024-11-15T06:49:48.792668Z","iopub.status.idle":"2024-11-15T06:49:49.092566Z","shell.execute_reply.started":"2024-11-15T06:49:48.792622Z","shell.execute_reply":"2024-11-15T06:49:49.091318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{}},{"cell_type":"markdown","source":"## Multi-modal prompting\n\nAs you may have noticed in AI Studio, the Gemini models support more than just text as input. You can provide pictures, videos, audio and more.\n\n\n### Images\n\nStart by downloading an image.","metadata":{}},{"cell_type":"code","source":"import PIL\nfrom IPython.display import Image\n\n!wget -nv https://storage.googleapis.com/generativeai-downloads/images/cake.jpg\nImage('cake.jpg', width=500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:18:53.948753Z","iopub.execute_input":"2024-11-15T04:18:53.9492Z","iopub.status.idle":"2024-11-15T04:18:56.426775Z","shell.execute_reply.started":"2024-11-15T04:18:53.949161Z","shell.execute_reply":"2024-11-15T04:18:56.42565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Python SDK can take a list as the prompt input. This list represents a sequence of prompt parts, and while each part needs to be a single mode (such as text or image), you can combine them together to form a multi-modal prompt.","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nprompt = [\n  \"What is this? Please describe it in detail.\",\n  PIL.Image.open(\"cake.jpg\"),\n]\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:18:58.164918Z","iopub.execute_input":"2024-11-15T04:18:58.165924Z","iopub.status.idle":"2024-11-15T04:19:00.822539Z","shell.execute_reply.started":"2024-11-15T04:18:58.165876Z","shell.execute_reply":"2024-11-15T04:19:00.82149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Image understanding in the Gemini models can be quite powerful. Check out [this guide on object detection](https://github.com/google-gemini/cookbook/blob/main/examples/Object_detection.ipynb), where the Gemini API identifies and highlights objects in an image based on a prompt.\n\nMore input modes are supported, but first take a look at how to handle large files.","metadata":{}},{"cell_type":"markdown","source":"## Use and upload files\n\nThe Gemini models have very large context windows, up to 2 million input tokens are supported for the 1.5 Pro model. This translates to up to 2 hours of video or up to 19 hours of audio.\n\nAs files of this length are typically too large to send in HTTP requests, the Gemini API provides a File API to that you can use to send large files in requests. It also means you can reuse the same files across different requests without having to re-upload the same content each time, improving your request latency.\n\nNote that some file limits exist, including how long they are kept. See [the note in the docs](https://ai.google.dev/gemini-api/docs/vision?hl=en&lang=python#upload-image) for more info.","metadata":{}},{"cell_type":"markdown","source":"### Audio\n\nThe Gemini API supports audio as an input medium. If you are the kind of person that takes audio notes with the Recorder or Voice Memo apps, this can be an efficient way to interact with your recordings ([check out this example](https://github.com/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb)), but you are not limited to personal notes.\n\nThis MP3 audio recording is a State of the Union addess from US president Kennedy. Running the following code should give you a playable audio controller so you can listen to it.","metadata":{}},{"cell_type":"code","source":"from pydub import AudioSegment\nfrom IPython.display import Audio\n\n\n!wget -nv https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3 -O speech.mp3\n\n# This audio file is over 40mb, so trim the file before sending it to your browser.\nfull_speech = AudioSegment.from_mp3(\"speech.mp3\")\n\n# Preview the first 30 seconds.\nfirst_30s_speech = full_speech[:30000]\nfirst_30s_speech\n\n# If you want to download and listen to the whole file, uncomment this.\n# Audio(\"speech.mp3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:36:05.116081Z","iopub.execute_input":"2024-11-15T04:36:05.116502Z","iopub.status.idle":"2024-11-15T04:36:12.73931Z","shell.execute_reply.started":"2024-11-15T04:36:05.116458Z","shell.execute_reply":"2024-11-15T04:36:12.738218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now upload the full file so it can be used in a prompt.","metadata":{}},{"cell_type":"code","source":"uploaded_speech = genai.upload_file(path='speech.mp3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:39:39.443736Z","iopub.execute_input":"2024-11-15T04:39:39.444673Z","iopub.status.idle":"2024-11-15T04:39:42.851172Z","shell.execute_reply.started":"2024-11-15T04:39:39.44463Z","shell.execute_reply":"2024-11-15T04:39:42.850218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"Who made the following speech? What were they positive about?\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content([prompt, uploaded_speech], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:40:23.698834Z","iopub.execute_input":"2024-11-15T04:40:23.699764Z","iopub.status.idle":"2024-11-15T04:40:32.76011Z","shell.execute_reply.started":"2024-11-15T04:40:23.699705Z","shell.execute_reply":"2024-11-15T04:40:32.758982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Video\n\n","metadata":{}},{"cell_type":"markdown","source":"Now try out video understanding. In this example you will upload the [\"Big Buck Bunny\"](https://peach.blender.org/) short film and use the Gemini API to ask questions.\n\n> \"Big Buck Bunny\" is (c) copyright 2008, Blender Foundation / www.bigbuckbunny.org and [licensed](https://peach.blender.org/about/) under the [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License.\n\nStart by downloading the video to this notebook and then uploading to the File API.","metadata":{}},{"cell_type":"code","source":"!wget https://download.blender.org/peach/bigbuckbunny_movies/BigBuckBunny_320x180.mp4\n\nprint(\"Uploading to the File API...\")\nvideo_file = genai.upload_file(path=\"BigBuckBunny_320x180.mp4\")\nprint(\"Upload complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:58:55.206672Z","iopub.execute_input":"2024-11-15T04:58:55.207547Z","iopub.status.idle":"2024-11-15T04:59:03.499535Z","shell.execute_reply.started":"2024-11-15T04:58:55.207502Z","shell.execute_reply":"2024-11-15T04:59:03.498176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Larger files can take some time to process when they upload. Ensure that the file is ready to use.","metadata":{}},{"cell_type":"code","source":"import time\n\nwhile video_file.state.name == \"PROCESSING\":\n    print('Waiting for video to be processed.')\n    time.sleep(10)\n    video_file = genai.get_file(video_file.name)\n\nif video_file.state.name == \"FAILED\":\n  raise ValueError(video_file.state.name)\n\nprint(f'Video processing complete: ' + video_file.uri)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:59:55.18633Z","iopub.execute_input":"2024-11-15T04:59:55.186782Z","iopub.status.idle":"2024-11-15T05:00:05.990912Z","shell.execute_reply.started":"2024-11-15T04:59:55.186736Z","shell.execute_reply":"2024-11-15T05:00:05.989883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that it is ready, use it in a prompt. Note that using large files in requests typically takes more time than a small text request, so increase the timeout and be aware that you may have to wait for this response.","metadata":{}},{"cell_type":"code","source":"prompt = \"What characters are in this movie?\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content([prompt, video_file],\n                                  request_options=retry_policy | {\"timeout\": 600})\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T05:08:50.724986Z","iopub.execute_input":"2024-11-15T05:08:50.72599Z","iopub.status.idle":"2024-11-15T05:08:59.965989Z","shell.execute_reply.started":"2024-11-15T05:08:50.725943Z","shell.execute_reply":"2024-11-15T05:08:59.96475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Streaming\n\nSo far, you have been making transactional requests with the Gemini API - send the request, receive a full response. The API also supports response streaming.\n\nPass `stream=True` to `generate_content` to trigger streaming mode. Note that it may render quickly - uncomment the final `print` to see each streamed chunk on its own.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"Write an essay defending why dogs are the best animals.\nTreat the essay as serious and include proper essay structure.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(prompt, stream=True, request_options=retry_policy)\nfor chunk in response:\n    print(chunk.text, end='')\n    # Uncomment this to see the individual tokens in separate sections.\n    # print(\"\\n----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:38:41.434237Z","iopub.execute_input":"2024-11-15T07:38:41.434719Z","iopub.status.idle":"2024-11-15T07:38:44.303579Z","shell.execute_reply.started":"2024-11-15T07:38:41.434679Z","shell.execute_reply":"2024-11-15T07:38:44.302161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Context caching\n\nContext caching is a technique that allows you to cache part of a request, such that it does not need to be re-processed by the model each time you use it. This is useful, for example, for asking new questions of the same documents.\n\nNote that context caching typically charges per million tokens per hour of caching. If you are using a paid API key, be sure to set your cache expiry or delete the cached tokens after use. See the [billing page](https://ai.google.dev/pricing) for more info. The Flash 1.5 model also supports caching on the free tier.\n\nTo ensure that the cache remains valid, caches are created by specifying versioned model names, so `gemini-1.5-flash-001`, where `-001` signifies the model version.","metadata":{}},{"cell_type":"code","source":"from google.generativeai import caching\n\n# Download the transcript\n!wget -O apollo11.txt https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n\n# Upload to the File API\ntranscript_file = genai.upload_file('apollo11.txt')\n\n# Create a cache\napollo_cache = caching.CachedContent.create(\n    model='gemini-1.5-flash-001',\n    system_instruction=\"You are a space history buff that enjoys discussing and explaining historical space events.\",\n    contents=[transcript_file],\n)\n\napollo_cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:26:38.502849Z","iopub.execute_input":"2024-11-15T07:26:38.503341Z","iopub.status.idle":"2024-11-15T07:26:42.670662Z","shell.execute_reply.started":"2024-11-15T07:26:38.50329Z","shell.execute_reply":"2024-11-15T07:26:42.669327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now you can create a new model that uses this cache.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown\n\napollo_model = genai.GenerativeModel.from_cached_content(cached_content=apollo_cache)\n\nresponse = apollo_model.generate_content(\"Find a nice moment from this transcript\")\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:34:03.963646Z","iopub.execute_input":"2024-11-15T07:34:03.964091Z","iopub.status.idle":"2024-11-15T07:34:07.919284Z","shell.execute_reply.started":"2024-11-15T07:34:03.964048Z","shell.execute_reply":"2024-11-15T07:34:07.917906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The response object includes information about the number of tokens that were cached and otherwise used in the prompt.","metadata":{}},{"cell_type":"code","source":"response.usage_metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:33:58.763806Z","iopub.execute_input":"2024-11-15T07:33:58.764256Z","iopub.status.idle":"2024-11-15T07:33:58.772138Z","shell.execute_reply.started":"2024-11-15T07:33:58.764192Z","shell.execute_reply":"2024-11-15T07:33:58.77058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And you can calculate how many non-cached tokens were used as input.","metadata":{}},{"cell_type":"code","source":"response.usage_metadata.total_token_count - response.usage_metadata.cached_content_token_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:32:22.695359Z","iopub.execute_input":"2024-11-15T07:32:22.696767Z","iopub.status.idle":"2024-11-15T07:32:22.706493Z","shell.execute_reply.started":"2024-11-15T07:32:22.69669Z","shell.execute_reply":"2024-11-15T07:32:22.705017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Delete the cache\n\nTo ensure you are not charged for any cached tokens you are not using, delete the cache. If you are on the free tier, you won't be charged, but it's good practice to clean up when you're done.","metadata":{}},{"cell_type":"code","source":"print(apollo_cache.name)\napollo_cache.delete()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T07:37:26.42085Z","iopub.execute_input":"2024-11-15T07:37:26.421504Z","iopub.status.idle":"2024-11-15T07:37:26.820292Z","shell.execute_reply.started":"2024-11-15T07:37:26.421438Z","shell.execute_reply":"2024-11-15T07:37:26.819072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Further reading\n\nTake a look through the [Gemini API cookbook](https://github.com/google-gemini/cookbook) for more feature-based quickstarts and complex examples.\n\nIf you enabled billing on your API key and are finished with the key, you can [turn it off](https://ai.google.dev/gemini-api/docs/billing) unless you plan on using it again.\n\nAnd thank you for coming with us on this 5-day learning journey!\n\n\\- [Mark McD](https://twitter.com/m4rkmc)","metadata":{}}]}