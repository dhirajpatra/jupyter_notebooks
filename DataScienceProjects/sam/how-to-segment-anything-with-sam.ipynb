{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0Wut2G5kz2Z"
   },
   "source": [
    "# Segment Anything Model (SAM)\n",
    "\n",
    "---\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)\n",
    "\n",
    "Segment Anything Model (SAM): a new AI model from Meta AI that can \"cut out\" any object, in any image, with a single click. SAM is a promptable segmentation system with zero-shot generalization to unfamiliar objects and images, without the need for additional training. This notebook is an extension of the [official notebook](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb) prepared by Meta AI.\n",
    "\n",
    "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-paper.png)\n",
    "\n",
    "## Complementary Materials\n",
    "\n",
    "---\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/D-D6ZmadzPE) [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-segment-anything-model-sam)\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model.\n",
    "\n",
    "![segment anything model blogpost](https://media.roboflow.com/notebooks/examples/segment-anything-model-blogpost.png)\n",
    "\n",
    "## Pro Tip: Use GPU Acceleration\n",
    "\n",
    "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
    "\n",
    "## Steps in this Tutorial\n",
    "\n",
    "In this tutorial, we are going to cover:\n",
    "\n",
    "- **Before you start** - Make sure you have access to the GPU\n",
    "- Install Segment Anything Model (SAM)\n",
    "- Download Example Data\n",
    "- Load Model\n",
    "- Automated Mask Generation\n",
    "- Generate Segmentation with Bounding Box\n",
    "- Segment Anything in Roboflow Universe Dataset\n",
    "\n",
    "## Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2s3TnW4nhjC"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frcrk09FhJeV"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e61jI_Go9xEg"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re, sys, datetime, json, random, cv2, os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p854H-PY9yvM"
   },
   "outputs": [],
   "source": [
    "## If you are using the data by mounting the google drive, use the following :\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4nULecX92L0"
   },
   "outputs": [],
   "source": [
    "!ls \"/content/gdrive/MyDrive/structure_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iov3yhoRnxG2"
   },
   "source": [
    "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgS8jFPMnj5h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3DPGZSn57p"
   },
   "source": [
    "## Install Segment Anything Model (SAM) and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1H9YruJen0Q8"
   },
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3CtzYroC2Lb"
   },
   "outputs": [],
   "source": [
    "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VeYIWh1iDWW"
   },
   "source": [
    "### Download SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aszw1OxBwowI"
   },
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "!mkdir {HOME}/weights\n",
    "%cd {HOME}/weights\n",
    "\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxoFmhsHw_fG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlhbd_f4xfiJ"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6_9PSZupghA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n41g6y-Zx-9x"
   },
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi3C4uDWo10h"
   },
   "source": [
    "## Automated Mask Generation\n",
    "\n",
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtymFaiKyQ57"
   },
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0Pm0RYArgm9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGE_NAME = \"7.png\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"gdrive/MyDrive/structure_images\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgL88fUuelk"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u34UjLT8o7iC"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUNhAvdPjZ-Y"
   },
   "source": [
    "### Output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxO265XOymA2"
   },
   "source": [
    "`SamAutomaticMaskGenerator` returns a `list` of masks, where each mask is a `dict` containing various information about the mask:\n",
    "\n",
    "* `segmentation` - `[np.ndarray]` - the mask with `(W, H)` shape, and `bool` type\n",
    "* `area` - `[int]` - the area of the mask in pixels\n",
    "* `bbox` - `[List[int]]` - the boundary box of the mask in `xywh` format\n",
    "* `predicted_iou` - `[float]` - the model's own prediction for the quality of the mask\n",
    "* `point_coords` - `[List[List[float]]]` - the sampled input point that generated this mask\n",
    "* `stability_score` - `[float]` - an additional measure of mask quality\n",
    "* `crop_box` - `List[int]` - the crop of the image used to generate this mask in `xywh` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRiwoYNEzbBN"
   },
   "outputs": [],
   "source": [
    "print(sam_result[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkNDZqBEj5Cr"
   },
   "source": [
    "### Results visualisation with Supervision\n",
    "\n",
    "As of version `0.5.0` Supervision has native support for SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdT3xT2AkS4g"
   },
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator()\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsdFDDQnjhkP"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS_WhY60XMNL"
   },
   "outputs": [],
   "source": [
    "masks = [\n",
    "    mask['segmentation']\n",
    "    for mask\n",
    "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(8, int(len(masks) / 8)),\n",
    "    size=(16, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKPiidy9nwH"
   },
   "source": [
    "## Generate Segmentation with Bounding Box\n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU1SN8_WCLny"
   },
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS27Xlnb7MAj"
   },
   "source": [
    "### Draw Box\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSwoXkDi9uVD"
   },
   "outputs": [],
   "source": [
    "# helper function that loads an image before adding it to the widget\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFGBhRQNC0-H"
   },
   "source": [
    "**NOTE:** Execute cell below and use your mouse to draw bounding box on the image 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zieb7wDZCoj2"
   },
   "outputs": [],
   "source": [
    "IS_COLAB = True\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "\n",
    "widget = BBoxWidget()\n",
    "widget.image = encode_image(IMAGE_PATH)\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSAhAXOULj0t"
   },
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsy-GikiuX5l"
   },
   "source": [
    "### Generate masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqxt0CdkFUf8"
   },
   "source": [
    "**NOTE:** `SamPredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format. Let's reorganise your data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYyhnP4xFO5_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# default_box is going to be used if you will not draw any box on image above\n",
    "default_box = {'x': 68, 'y': 247, 'width': 555, 'height': 678, 'label': ''}\n",
    "\n",
    "box = widget.bboxes[0] if widget.bboxes else default_box\n",
    "box = np.array([\n",
    "    box['x'], \n",
    "    box['y'], \n",
    "    box['x'] + box['width'], \n",
    "    box['y'] + box['height']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGxKHiK2uqtE"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_predictor.set_image(image_rgb)\n",
    "\n",
    "masks, scores, logits = mask_predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_JOjHBNnV5"
   },
   "source": [
    "### Results visualisation with Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2opSwP1Np7s"
   },
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n",
    "mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
    "    mask=masks\n",
    ")\n",
    "detections = detections[detections.area == np.max(detections.area)]\n",
    "\n",
    "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections, skip_label=True)\n",
    "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2RsmjSH5Kh"
   },
   "source": [
    "### Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0Ts3OZ3MuzP"
   },
   "outputs": [],
   "source": [
    "import supervision as v\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(1, 4),\n",
    "    size=(16, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S14Wh-hhr3wP"
   },
   "source": [
    "## Segment Anything in Roboflow Universe Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff80d5krPlkO"
   },
   "source": [
    "### Utils Supporting Dataset Processing\n",
    "\n",
    "A couple of helper functions that, unfortunately, we have to write ourselves to facilitate the processing of COCO annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZSU9BpHr2gc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from dataclasses_json import dataclass_json\n",
    "from supervision import Detections\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOCategory:\n",
    "    id: int\n",
    "    name: str\n",
    "    supercategory: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOImage:\n",
    "    id: int\n",
    "    width: int\n",
    "    height: int\n",
    "    file_name: str\n",
    "    license: int\n",
    "    date_captured: str\n",
    "    coco_url: Optional[str] = None\n",
    "    flickr_url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOAnnotation:\n",
    "    id: int\n",
    "    image_id: int\n",
    "    category_id: int\n",
    "    segmentation: List[List[float]]\n",
    "    area: float\n",
    "    bbox: Tuple[float, float, float, float]\n",
    "    iscrowd: int\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOLicense:\n",
    "    id: int\n",
    "    name: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOJson:\n",
    "    images: List[COCOImage]\n",
    "    annotations: List[COCOAnnotation]\n",
    "    categories: List[COCOCategory]\n",
    "    licenses: List[COCOLicense]\n",
    "\n",
    "\n",
    "def load_coco_json(json_file: str) -> COCOJson:\n",
    "    import json\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return COCOJson.from_dict(json_data)\n",
    "\n",
    "\n",
    "class COCOJsonUtility:\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
    "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
    "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
    "        if image:\n",
    "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
    "        for image in coco_data.images:\n",
    "            if image.file_name == image_path:\n",
    "                return image\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
    "        class_id, xyxy = [], []\n",
    "\n",
    "        for annotation in annotations:\n",
    "            x_min, y_min, width, height = annotation.bbox\n",
    "            class_id.append(annotation.category_id)\n",
    "            xyxy.append([\n",
    "                x_min,\n",
    "                y_min,\n",
    "                x_min + width,\n",
    "                y_min + height\n",
    "            ])\n",
    "\n",
    "        return Detections(\n",
    "            xyxy=np.array(xyxy, dtype=int),\n",
    "            class_id=np.array(class_id, dtype=int)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb",
     "timestamp": 1682518866424
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
