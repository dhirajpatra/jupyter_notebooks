{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand it with the help of an example:\n",
    "\n",
    "**The problem statement:**\n",
    "\n",
    "There are two machines which manufacture bulbs. Machine 1 produces 30 bulbs per hour and machine 2 produce 20 bulbs per hour. Out of all bulbs produced, 1 % turn out to be defective. Out of all the defective bulbs, the share of each machine is 50%.  What is the probability that a bulb produced by machine 2 is defective?\n",
    "\n",
    "We can write the information given above in mathematical terms as:\n",
    "\n",
    "The probability that a bulb was made by Machine 1, P(M1)=30/50=0.6\n",
    "\n",
    "The probability that a bulb was made by Machine 2, P(M2)=20/50=0.4\n",
    "\n",
    "The probability that a bulb is defective, P(Defective)=1%=0.01\n",
    "\n",
    "The probability that a defective bulb came out of Machine 1, P(M1 | Defective)=50%=0.5\n",
    "\n",
    "The probability that a defective bulb came out of Machine 2, P(M2 | Defective)=50%=0.5\n",
    "\n",
    "Now, we need to calculate the probability of a bulb produced by machine 2 is defective i.e.,\n",
    "P(Defective | M2).\n",
    "Using the Bayes Theorem above, it can be written as:\n",
    "\n",
    "$P(Defective | M2)=\\frac { P(M2 | Defective) * P(Defective)} { P(M2)}$\n",
    "\n",
    "Substituting the values, we get:$P(Defective | M2)=\\frac {0.5*0.01}{0.4}= 0.0125$\n",
    "\n",
    "Task for you is to calculate the probability that a bulb produced by machine 1 is defective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with importing necessary libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\") # Reading the Data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see there few data for columns Glucose, Insulin, skin thickness, BMI and Blood Pressure which have value as 0. That's not possible. You can do a quick search to see that one cannot have 0 values for these.\n",
    "Let's deal with that. we can either remove such data or simply replace it with their respective mean values.\n",
    "Let's do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing zero values with the mean of the column\n",
    "data['BMI'] = data['BMI'].replace(0,data['BMI'].mean())\n",
    "data['BloodPressure'] = data['BloodPressure'].replace(0,data['BloodPressure'].mean())\n",
    "data['Glucose'] = data['Glucose'].replace(0,data['Glucose'].mean())\n",
    "data['Insulin'] = data['Insulin'].replace(0,data['Insulin'].mean())\n",
    "data['SkinThickness'] = data['SkinThickness'].replace(0,data['SkinThickness'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Outliers\n",
    "\n",
    "q = data['Pregnancies'].quantile(0.98)\n",
    "\n",
    "# we are removing the top 2% data from the Pregnancies column\n",
    "data_cleaned = data[data['Pregnancies']<q]\n",
    "q = data_cleaned['BMI'].quantile(0.99)\n",
    "\n",
    "# we are removing the top 1% data from the BMI column\n",
    "data_cleaned  = data_cleaned[data_cleaned['BMI']<q]\n",
    "q = data_cleaned['SkinThickness'].quantile(0.99)\n",
    "# we are removing the top 1% data from the SkinThickness column\n",
    "data_cleaned  = data_cleaned[data_cleaned['SkinThickness']<q]\n",
    "q = data_cleaned['Insulin'].quantile(0.95)\n",
    "# we are removing the top 5% data from the Insulin column\n",
    "data_cleaned  = data_cleaned[data_cleaned['Insulin']<q]\n",
    "q = data_cleaned['DiabetesPedigreeFunction'].quantile(0.99)\n",
    "# we are removing the top 1% data from the DiabetesPedigreeFunction column\n",
    "data_cleaned  = data_cleaned[data_cleaned['DiabetesPedigreeFunction']<q]\n",
    "q = data_cleaned['Age'].quantile(0.99)\n",
    "# we are removing the top 1% data from the Age column\n",
    "data_cleaned  = data_cleaned[data_cleaned['Age']<q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data_cleaned:\n",
    "    if plotnumber<=9 :\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(data_cleaned[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        #plt.ylabel('Salary',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['Outcome'])\n",
    "y = data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale our data as well\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how our data looks now after scaling.\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check for multicollinearity using VIF(Variance Inflation factor)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\n",
    "vif[\"Features\"] = X.columns\n",
    "\n",
    "#let's check the values\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the VIF values are less than 5 and are very low. That means no multicollinearity. \n",
    "Now, we can go ahead with fitting our data to the model.\n",
    "Before that, let's split our data in test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X_scaled,y, test_size= 0.25, random_state = 355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Writing different model files to file\n",
    "with open( 'modelForPrediction.sav', 'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    \n",
    "with open('standardScalar.sav', 'wb') as f:\n",
    "    pickle.dump(scalar,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = conf_mat[0][0]\n",
    "false_positive = conf_mat[0][1]\n",
    "false_negative = conf_mat[1][0]\n",
    "true_negative = conf_mat[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down the formula for Accuracy\n",
    "Accuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precison\n",
    "Precision = true_positive/(true_positive+false_positive)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "Recall = true_positive/(true_positive+false_negative)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under Curve\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been doing  grid search to maximise the accuracy of our model.\n",
    " Here, we’ll follow a different approach. We’ll create two models, one with Logistic regression and other with Naïve Bayes and we’ll compare the AUC. The algorithm having a better AUC shall be considered for production deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Naive Bayes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logistic = accuracy_score(y_test,y_pred_logistic)\n",
    "accuracy_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test,y_pred_logistic)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "fpr_logistic, tpr_logistic, thresholds_logistic = roc_curve(y_test, y_pred_logistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_logistic, tpr_logistic, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics  import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_naive=roc_auc_score(y_test,y_pred)\n",
    "auc_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_logistic=roc_auc_score(y_test,y_pred_logistic)\n",
    "auc_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the AUC for Naïve Bayes is more. So, we’ll take that as our production-ready model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
