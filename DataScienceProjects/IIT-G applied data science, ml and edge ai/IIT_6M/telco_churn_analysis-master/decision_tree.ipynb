{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.churn\n",
    "X = df.drop('churn', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(mm.fit_transform(X), columns = X.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1) Vanilla Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier(random_state = 33)\n",
    "clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds1 = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print all four major metrics\n",
    "print(f\"Precision Score: {precision_score(y_test, test_preds1)}\")\n",
    "print(f\"Recall Score: {recall_score(y_test, test_preds1)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, test_preds1)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, test_preds1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, test_preds1)\n",
    "# Calculate AUC score from sklearn.metrics library\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Print auc score\n",
    "print(f'AUC Score: {roc_auc}')\n",
    "\n",
    "# Plot AUC curve\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(fpr, tpr, lw = 2, label = 'Baseline AUC ='+str(roc_auc))\n",
    "plt.plot([0,1],[0,1], linestyle = '--', lw = 2)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.ylabel('True Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.title('ROC Curve: Decision Tree Classifier (Default)', fontsize = 25, fontweight = 'bold')\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, test_preds1, rownames = ['True'], colnames = ['Predicted'], margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2) GridSearchCV Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = DecisionTreeClassifier(random_state = 33)\n",
    "param_grid = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_depth':[None, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],\n",
    "    'min_samples_split':[20, 25, 30, 35, 40],\n",
    "    'min_samples_leaf':[1,2,3,4,5,6]\n",
    "}\n",
    "gs_tree = GridSearchCV(clf3, param_grid, cv = 4, verbose = True)\n",
    "gs_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters: {gs_tree.best_params_}\")\n",
    "print(f\"Best Score: {gs_tree.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = DecisionTreeClassifier(\n",
    "    criterion = 'entropy',\n",
    "    max_depth = 6,\n",
    "    min_samples_leaf = 1,\n",
    "    min_samples_split = 30,\n",
    "    random_state = 33)\n",
    "final_clf.fit(X_train, y_train)\n",
    "test_preds2 = final_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 4 major metrics\n",
    "print(f\"Precision Score: {precision_score(y_test, test_preds2)}\")\n",
    "print(f\"Recall Score: {recall_score(y_test, test_preds2)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, test_preds2)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, test_preds2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, test_preds2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'AUC Score: {roc_auc}')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(fpr, tpr,lw = 2, label = 'KNN(k=5) AUC = '+str(roc_auc))\n",
    "plt.plot([0,1],[0,1], linestyle = '--', lw = 2)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.ylabel('True Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.title('ROC Curve: GridSearchCV', fontsize = 25, fontweight = 'bold')\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, test_preds2, rownames = ['True'], colnames = ['Predicted'], margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2) Vanilla Classifier: Manual Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1,32,32,endpoint = True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "    dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = max_depth, random_state = 33)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(max_depths, train_results, label = 'Train AUC', lw = 2)\n",
    "plt.plot(max_depths, test_results, label = 'Test AUC', lw = 2)\n",
    "plt.ylabel('AUC Score', fontsize = 20, fontweight = 'bold')\n",
    "plt.xticks(np.arange(0,33))\n",
    "plt.xlabel('Tree Depth', fontsize = 20, fontweight = 'bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Training error decreases with tree depth (a sign of overfitting as tree depth increases)\n",
    " - Test error increases after depth = 3 (some fluctuations, not stable)\n",
    " - Training and Test errors rise rapidly between depths of 3 & 4\n",
    " - **Optimal value roughly 2/3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Sample Split\n",
    "- Minimum number of samples required to split an internal node\n",
    "- As number increases, tree becomes more constrained as it has to consider more samples @ node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = np.linspace(0.1, 1.0, 10, endpoint = True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for min_split in min_samples_split:\n",
    "    dt = DecisionTreeClassifier('entropy', min_samples_split = min_split, random_state = 33)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    fpr, tpr, threshold = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    train_results.append(roc_auc)\n",
    "    test_pred = dt.predict(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, test_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.plot(min_samples_split, train_results, 'b', label = 'Train AUC')\n",
    "plt.plot(min_samples_split, test_results, 'r', label = 'Test AUC')\n",
    "plt.xlabel('Min. Sample Splits', fontweight = 'bold', fontsize = 20)\n",
    "plt.ylabel('AUC Score', fontweight = 'bold', fontsize = 20)\n",
    "plt.title('')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Samples Split both train and test both begin to stabilize at min_samples_split = 33\n",
    "# Further increase in min_sample_split will not increase learning ability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Samples Leaf\n",
    "- Identify min. number of samples that we want a leaf node to contain\n",
    "- When min. size is achieved at a node, it does not get split any further\n",
    "- Too many samples in leaf node indicate high level of uncertainty in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = np.linspace(0.1, 0.5, 10, endpoint = True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for min_leaf in min_samples_leaf:\n",
    "    dt = DecisionTreeClassifier('entropy', min_samples_leaf = min_leaf, random_state = 33)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_preds = dt.predict(X_train)\n",
    "    fpr, tpr, threshold = roc_curve(y_train, train_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    train_results.append(roc_auc)\n",
    "    test_preds = dt.predict(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, test_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(min_samples_leaf, train_results, 'b', label = 'Train AUC')\n",
    "plt.plot(min_samples_leaf, test_results, 'r', label = 'Test AUC')\n",
    "plt.xlabel('Min. Samples Leaf', fontweight = 'bold', fontsize = 20)\n",
    "plt.ylabel('AUC Score', fontweight = 'bold', fontsize = 20)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Accuracy decreases significantly after 0.23 for both test and train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = list(range(1, X_train.shape[1]))\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_feature in max_features:\n",
    "    dt = DecisionTreeClassifier('entropy', max_features = max_feature, random_state = 33)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_preds = dt.predict(X_train)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, train_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    train_results.append(roc_auc)\n",
    "    test_preds = dt.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, test_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(max_features, train_results, 'b', label = 'Train AUC')\n",
    "plt.plot(max_features, test_results, 'r', label = 'Test AUC')\n",
    "plt.xlabel('Max # Features', fontweight = 'bold', fontsize = 20)\n",
    "plt.xticks(np.arange(0, 45))\n",
    "plt.ylabel('AUC Score', fontweight = 'bold', fontsize = 20)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No clear effect on training (flat AUC)\n",
    "- Multiple fluctuations in test AUC but not definitive\n",
    "- Highest AUC value is 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3) Manual Pruning Classifier Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeClassifier(\n",
    "                            criterion = 'entropy',\n",
    "                            max_depth = 3,\n",
    "                            min_samples_split = 0.4,\n",
    "                            min_samples_leaf = 0.23,\n",
    "                            random_state = 33)\n",
    "dt2.fit(X_train, y_train)\n",
    "y_preds = dt2.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision Score: {precision_score(y_test, y_preds)}\")\n",
    "print(f\"Recall Score: {recall_score(y_test, y_preds)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_preds)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'AUC Score: {roc_auc}')\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(fpr, tpr, lw = 2, label = 'Baseline AUC= ' + str(roc_auc))\n",
    "plt.plot([0,1], [0,1], lw = 2)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontweight = 'bold', fontsize = 20)\n",
    "plt.ylabel('True Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.title('ROC Curve: Pruned Decision Tree', fontweight = 'bold', fontsize = 25)\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_preds, rownames = ['True'], colnames = ['Predicted'], margins = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
