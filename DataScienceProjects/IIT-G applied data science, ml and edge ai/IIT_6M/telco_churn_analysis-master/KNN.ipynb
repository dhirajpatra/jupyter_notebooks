{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "from regression_module import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "df = pd.read_csv('data/final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first two unnecessary columns from DF\n",
    "df = df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target variable\n",
    "y = df.churn\n",
    "# Drop target variable from independent features DF\n",
    "X = df.drop('churn', axis = 1)\n",
    "# Save columns as list of strings for reassign after scaling\n",
    "cols = X.columns\n",
    "scaled_df.to_csv('data/processed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scaling object from SKlearn\n",
    "mm = MinMaxScaler()\n",
    "# Fit_Transform the independent features DF to the min-max scaler\n",
    "scaled_X = mm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign scaled dataset to pandas dataframe\n",
    "scaled_df = pd.DataFrame(scaled_X)\n",
    "# Reassign columns names to new dataframe\n",
    "scaled_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train test split, maintaining test size sample and random state from logistic regression notebook\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, y, test_size = .25, random_state = 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline KNN Classifier (default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNN object\n",
    "knn1 = KNeighborsClassifier()\n",
    "\n",
    "# Fit training set to our classifying object\n",
    "knn1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign predictions to variable\n",
    "test_preds1 = knn1.predict(X_test)\n",
    "\n",
    "# Calculate (accuracy) score metric\n",
    "knn1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print all four major metrics\n",
    "print(f\"Precision Score: {precision_score(y_test, test_preds1)}\")\n",
    "print(f\"Recall Score: {recall_score(y_test, test_preds1)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, test_preds1)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, test_preds1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign false positive rate, true pos rate, and thresholds to variables using sklearn.metrics library\n",
    "fpr, tpr, threshold = roc_curve(y_test, test_preds1)\n",
    "# Calculate AUC score from sklearn.metrics library\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Print auc score\n",
    "print(f'AUC Score: {roc_auc}')\n",
    "\n",
    "# Plot AUC curve\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(fpr, tpr, lw = 2, label = 'Baseline AUC ='+str(roc_auc))\n",
    "plt.plot([0,1],[0,1], linestyle = '--', lw = 2)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.ylabel('True Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.title('ROC Curve: KNN Classifier (k = 5)', fontsize = 25, fontweight = 'bold')\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Val - KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNN classifier object again, assigning a different name\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# perform a cross validation score using sklearn.model_selection (iterate until maximum output score)\n",
    "knn_cv_score = cross_val_score(knn_clf, X_train, y_train, cv = 4)\n",
    "\n",
    "# Use numpy to obtain mean accuracy score from cross-validation folds and display\n",
    "mean_knn_cv_score = np.mean(knn_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_knn_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a second KNN classifier object\n",
    "knn2 = KNeighborsClassifier()\n",
    "# Establish parameters grid in dictionary form per KNN documentation\n",
    "param_grid = {'n_neighbors':np.arange(1,20)}\n",
    "# Pass instantiated KNN object, parameter grid, and optimal fold value as arguments\n",
    "knn2_gscv = GridSearchCV(knn2, param_grid, cv = 4)\n",
    "# Fit train data to our new object\n",
    "knn2_gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return best parameters and score\n",
    "print(f\"Best Parameters: {knn2_gscv.best_params_}\")\n",
    "print(f\"Best Score: {knn2_gscv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test values\n",
    "test_preds2 = knn2_gscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain second accuracy score of optimal model\n",
    "knn2_gscv.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 4 major metrics\n",
    "print(f\"Precision Score: {precision_score(y_test, test_preds2)}\")\n",
    "print(f\"Recall Score: {recall_score(y_test, test_preds2)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, test_preds2)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, test_preds2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain major metrics and plot second ROC curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, test_preds2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'AUC Score: {roc_auc}')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(fpr, tpr,lw = 2, label = 'KNN(k=5) AUC = '+str(roc_auc))\n",
    "plt.plot([0,1],[0,1], linestyle = '--', lw = 2)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.ylabel('True Positive Rate', fontsize = 20, fontweight = 'bold')\n",
    "plt.title('ROC Curve: KNN Classifier (k = 5)', fontsize = 25, fontweight = 'bold')\n",
    "plt.legend(loc = 4, fontsize = 15)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
