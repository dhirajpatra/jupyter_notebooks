{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import io \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GenAI Setup ---\n",
    "MODEL_ID = \"gemini-robotics-er-1.5-preview\"\n",
    "client = genai.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Camera Capture (Same as previous solution) ---\n",
    "CAMERA_INDEX = 0 \n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "if not cap.isOpened(): raise IOError(\"Cannot open webcam\")\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "if not ret: raise IOError(\"Failed to capture image from camera\")\n",
    "ret, buffer = cv2.imencode('.jpg', frame)\n",
    "if not ret: raise IOError(\"Failed to encode image to JPEG\")\n",
    "image_bytes = buffer.tobytes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Queries for Sorting by Size and Debris ---\n",
    "\n",
    "# The model will visually classify these categories. \n",
    "# You can use general terms for size or specific variety names.\n",
    "queries = [\n",
    "    \"large coffee bean\",\n",
    "    \"medium coffee bean\",\n",
    "    \"small coffee bean (peaberry or undersized)\",\n",
    "    \"stick\",\n",
    "    \"rock or stone\",\n",
    "    \"miscellaneous debris\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Construction for Size-Based Sorting and Debris Removal ---\n",
    "# We make the prompt more descriptive to guide the model's reasoning.\n",
    "prompt = f\"\"\"\n",
    "    You are a coffee bean quality control robot.\n",
    "    Identify and locate all the following objects in the image: {', '.join(queries)}.\n",
    "    For the coffee beans, try to categorize them by approximate size.\n",
    "    For sticks, rocks, and debris, identify them clearly so they can be removed.\n",
    "    \n",
    "    The answer should follow the json format:\n",
    "    [\\{\\{\"point\": <point>, \"label\": <label1>\\}\\}, ...]. The points are in\n",
    "    [y, x] format normalized to 0-1000.\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Execution ---\n",
    "print(\"Sending captured image to Gemini model for size and debris recognition...\")\n",
    "\n",
    "image_response = client.models.generate_content(\n",
    "  model=MODEL_ID,\n",
    "  contents=[\n",
    "    types.Part.from_bytes(\n",
    "      data=image_bytes,\n",
    "      mime_type='image/jpeg',\n",
    "    ),\n",
    "    prompt\n",
    "  ],\n",
    "  config = types.GenerateContentConfig(\n",
    "      temperature=0.5,\n",
    "      thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "  ))\n",
    "\n",
    "print(\"\\n--- Recognition Results (Visual Sorting/Debris) ---\")\n",
    "print(image_response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- POST-PROCESSING & ROBOT ACTION LOGIC (Conceptual) ---\n",
    "# After receiving the JSON, your robot control code (not shown here) would:\n",
    "# 1. Parse the JSON points and labels.\n",
    "# 2. **Move to all points labeled \"stick,\" \"rock,\" or \"debris\" and discard them.**\n",
    "# 3. **Move to points labeled \"large,\" \"medium,\" or \"small\" coffee bean and place them in the corresponding bin.**\n",
    "\n",
    "# --- ADVANCED SORTING (Requires Additional Hardware/Sensors) ---\n",
    "# For *precise* size and density sorting, the Gemini model provides the visual\n",
    "# location (the <point>). Your robot would then use its physical tools:\n",
    "# - **Precise Sizing:** The robot's gripper could attempt to measure the object\n",
    "#   (if equipped with force/tactile sensors) or use a calibrated vision system\n",
    "#   (stereo camera/depth sensor) integrated with the robot's control stack to get\n",
    "#   a precise 3D size *after* the initial coarse classification from Gemini.\n",
    "# - **Density Sorting:** Density cannot be determined visually. This requires a \n",
    "#   physical process, such as dropping the bean into an air classifier or on an \n",
    "#   inclined vibrating table (which is what industrial sorters use), or possibly\n",
    "#   using a calibrated force sensor on a gripper to measure weight (mass) after \n",
    "#   calculating the approximate volume."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
