{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cc1e036-160d-4ca3-aad3-86e29b8d2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2948012-f25c-4bc9-80aa-022754f2c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_RUN = False\n",
    "IMAGE_WIDTH=256\n",
    "IMAGE_HEIGHT=256\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3\n",
    "# Define a list of possible batch sizes\n",
    "BATCH_SIZES = [16, 32, 64, 128]\n",
    "# Generate a random index to select a batch size from the list\n",
    "random_index = np.random.randint(0, len(BATCH_SIZES))\n",
    "\n",
    "# Select a batch size using the random index\n",
    "BATCH_SIZE = BATCH_SIZES[random_index]\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440dccb1-6dbc-4cc1-bc8e-2d5a69525bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all image files in the directory\n",
    "def resize_images(path_to_directory, target_size):\n",
    "  for filename in os.listdir(path_to_directory):\n",
    "        root, ext = os.path.splitext(filename)\n",
    "        if ext.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            img_path = os.path.join(path_to_directory, filename)\n",
    "            if Image.open(img_path).size != target_size:\n",
    "              img = Image.open(img_path).resize(target_size)\n",
    "              img.save(img_path)\n",
    "  print(f\"All images in {path_to_directory} resized to {target_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d570e3-4d2f-4fb6-8ca2-d52b38c3eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images in data/landscape/train/forest resized to (256, 256)\n",
      "All images in data/landscape/train/buildings resized to (256, 256)\n",
      "All images in data/landscape/train/glacier resized to (256, 256)\n",
      "All images in data/landscape/train/street resized to (256, 256)\n",
      "All images in data/landscape/train/mountain resized to (256, 256)\n",
      "All images in data/landscape/train/sea resized to (256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_path = 'data/landscape/train'\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(dataset_path):\n",
    "    if label.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    label_path = os.path.join(dataset_path, label)\n",
    "    resize_images(label_path, IMAGE_SIZE)\n",
    "    for img_path in os.listdir(label_path):\n",
    "        if os.path.exists(os.path.join(label_path, img_path)):\n",
    "            if img_path.startswith('.'):\n",
    "                continue\n",
    "            img = Image.open(os.path.join(label_path, img_path)).resize(IMAGE_SIZE)\n",
    "            x = img_to_array(img)\n",
    "            x /= 255.0  # Normalize pixel values\n",
    "            img = np.expand_dims(x, axis=0)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21582f40-86a9-445f-8331-bd619ae25b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "class AlexNet():\n",
    "    def __init__(self, input_shape=(224,224,3), activation='sigmoid', class_size=1):\n",
    "        super().__init__()\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # 1st Convolutional Layer\n",
    "        self.model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape))\n",
    "        self.model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "        # 2nd Convolutional Layer\n",
    "        self.model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "        # 3rd Convolutional Layer\n",
    "        self.model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "\n",
    "        # 4th Convolutional Layer\n",
    "        self.model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "\n",
    "        # 5th Convolutional Layer\n",
    "        self.model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "        # Flatten the layer\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        # 1st Dense Layer\n",
    "        self.model.add(Dense(4096, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "\n",
    "        # 2nd Dense Layer\n",
    "        self.model.add(Dense(4096, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "\n",
    "        # 3rd Dense Layer\n",
    "        self.model.add(Dense(class_size, activation=activation))\n",
    "\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "        \n",
    "        \n",
    "    def save_weights(self, filepath):\n",
    "        self.model.save_weights(filepath)\n",
    "\n",
    "\n",
    "    def compile(self, loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']):\n",
    "        # Compile the model\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976e0773-f53c-4f62-bd61-a56f732fd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# import datasets\n",
    "\n",
    "# !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !unzip -q tiny-imagenet-200.zip -d tiny-imagenet\n",
    "\n",
    "# imagenet = datasets.load_dataset(\n",
    "#     'tiny_imagenet',\n",
    "#     split='validation',\n",
    "#     ignore_verifications=True  # set to True if seeing splits Error\n",
    "# )\n",
    "# imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ae3945-393a-4239-958e-f0e1f7ed0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 1 classes.\n",
      "{'images': 0}\n"
     ]
    }
   ],
   "source": [
    "# Set up the image data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Set up the directory paths for the dataset\n",
    "train_dir = 'data/tiny-imagenet-200/train'\n",
    "val_dir = 'data/tiny-imagenet-200/val'\n",
    "\n",
    "# Load the training set using the ImageDataGenerator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load the validation set using the ImageDataGenerator\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print the class labels\n",
    "print(val_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc0615-6f2c-4e93-8cbe-c5583d76e3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1248/1563 [======================>.......] - ETA: 8:05 - loss: 0.0329 - accuracy: 0.9945"
     ]
    }
   ],
   "source": [
    "# Create an instance of your AlexNet model\n",
    "alexnet = AlexNet(input_shape=(224, 224, 3), class_size=NUM_CLASSES)\n",
    "\n",
    "# Compile the model with appropriate optimizer and loss function\n",
    "alexnet.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training set\n",
    "history = alexnet.model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "\n",
    "# Save the model weights to a file\n",
    "# alexnet.save_weights('alexnet_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f217569a-a816-4e6f-baf7-a3c5fc6203d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for imagenet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m resnet50 \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      8\u001b[0m alexnet \u001b[38;5;241m=\u001b[39m AlexNet(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m alexnet \u001b[38;5;241m=\u001b[39m \u001b[43malexnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mAlexNet.load_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py38/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     27\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for imagenet"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\n",
    "\n",
    "\n",
    "# Load the models\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "inception_v3 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "alexnet = AlexNet(input_shape=(224, 224, 3))\n",
    "alexnet = alexnet.load_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac0ef8-9bc0-4315-8f4d-73389bed0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Freeze the base model layers to prevent them from being updated during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add your custom layers on top of the base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model and specify the loss function, optimizer, and evaluation metric(s)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5787ac-00e6-48e4-8146-5bbc3af4516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data generator with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create data generator without data augmentation\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create training data generator\n",
    "train_generator = train_datagen.flow(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create validation data generator\n",
    "val_generator = val_datagen.flow(\n",
    "    val_images,\n",
    "    val_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057a701-17d7-4da8-a2cd-8297c2f922a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "vgg16.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "inception_v3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "resnet50.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "alexnet.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2917-4dd7-4a58-b4bb-2962656bd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "vgg16.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "inception_v3.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "resnet50.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "alexnet.fit(train_generator, epochs=10, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880b935-26b3-4c28-b384-f090124330e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "vgg16.evaluate(val_generator)\n",
    "inception_v3.evaluate(val_generator)\n",
    "resnet50.evaluate(val_generator)\n",
    "alexnet.evaluate(val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf54638-ae41-4776-a608-e72e260639dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = cv2.imread('/path/to/image')\n",
    "\n",
    "# Preprocess the image\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = img.astype('float32') / 255.0\n",
    "\n",
    "# Make a prediction using the models\n",
    "vgg16_pred = vgg16.predict(np.array([img]))\n",
    "inception_v3_pred = inception_v3.predict(np.array([img]))\n",
    "resnet50_pred = resnet50.predict(np.array([img]))\n",
    "alexnet_pred = alexnet.predict(np.array([img]))\n",
    "\n",
    "# Print the predictions\n",
    "print('VGG16:', vgg16_pred)\n",
    "print('InceptionV3:', inception_v3_pred)\n",
    "print('Resnet50:', resnet50_pred)\n",
    "print('AlexNet:', alexnet_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bcaf5-5ab5-4e56-8292-9e875ee44d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
