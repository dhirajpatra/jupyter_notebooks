{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybMCVFh0_5R8"
   },
   "source": [
    "# Training Mistral-7b on a Single GPU using PEFT LORA with Google Colab (Free Version)\n",
    "In this notebook, I will show you how to finetune Mistral-7b using the  recent peft library and bitsandbytes for loading large models in 4-bit.\n",
    "\n",
    "The fine-tuning method will rely on a method called \"Low Rank Adapters\" (LoRA), instead of fine-tuning the entire model you just have to fine-tune these adapters and load them properly inside the model. After fine-tuning the model you can also share your adapters on the ðŸ¤— Hub and load them very easily. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoTMt4AXgYhr"
   },
   "source": [
    "## Step 0 -  Define some helper functions\n",
    "1. Enable text wrapping so we don't have to scroll horizontally\n",
    "2. Define a wrapper function which pass our query to the model for inference and return decoded model's completion(response).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0xjesvvgWrz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arrkM1d69rGK"
   },
   "source": [
    "Let's define a wrapper function which will get completion from the model from a user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "NVZbaVf69quq",
    "outputId": "45915828-3277-4ad3-97ba-8553e3169ca9"
   },
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  ### Question:\n",
    "  {query}\n",
    "\n",
    "  ### Answer:\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query) # this is just plain old string formatting; don't confuse this with PromptTemplate in LangChain\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "WE64sQvvRTl-",
    "outputId": "a5f5fca7-984a-4030-f41c-3184f158584a"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juj9DSmmh-Gw"
   },
   "source": [
    "## Step 1 - Install necessary packages\n",
    "First, install the dependencies below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "3nNWXXc7ol1n",
    "outputId": "bd5ce7c4-5514-4f3e-8781-efab08fc7d4c"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NVvNhohkvwF"
   },
   "source": [
    "## Step 2 - Model loading\n",
    "We'll load the model using QLoRA quantization to reduce the usage of memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "kvvLg99Opw5R",
    "outputId": "a07a895e-8cd5-4a39-d611-df4a6fa63dff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgqxSuxsBX3r"
   },
   "source": [
    "Now we specify the model ID and then we load it with our previously defined quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7e374d0bdf204fb380a8d90179090268",
      "e3d198f1dd3041e081ded6757460b040",
      "c8d5639f610d49cca169cb0cebd195bd",
      "2dbf9ef7797744bb89da4e769f13525a",
      "d5d4b6c001d6490eb69ee9ae970154d2",
      "0f1b9659c0c84ff8850203abc7e52eda",
      "8e78fff2d65b4fd4bb8c174d16916324",
      "c9d42aab3a6c4ef8abc56e6d560dd657",
      "1bc8634c739f4c07968f1e6b740184d7",
      "c9d4c8041f8844fcaefabcf030f899a2",
      "018a54f850814c1a9ef77ddb4c62af5d"
     ]
    },
    "id": "7St-hFLNmS2v",
    "outputId": "66c1f13a-eb22-4a05-cd3c-5ec4d89b2aa9"
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}) # best practice for Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omw10c2djdIw"
   },
   "source": [
    "Run a inference on the base model. The model does not seem to understand our instruction and gives us a list of questions related to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "TDkUkF2So2-7",
    "outputId": "45e923c3-5554-4ec4-9988-0662a2de8094"
   },
   "outputs": [],
   "source": [
    "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m06rH8cTrZof"
   },
   "source": [
    "## Step 3 - Load dataset for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy6QLtYfrxD5"
   },
   "source": [
    "Let's load a dataset on finance, to fine tune our model on basic finance knowledges. In this guide, we'll load 10% data from the original dataset for the sake of the demo just to showcase how to use this integration with existing tools on the HF ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "NAP-jYBjrwUc",
    "outputId": "8e937c86-c617-4d09-d7d0-8d33bda33d77"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"gbharti/finance-alpaca\", split='train')\n",
    "\n",
    "# Explore the data\n",
    "df = data.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2f5tr1-SJd6"
   },
   "source": [
    "Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
    "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
    "2. shuffle the dataset\n",
    "3. tokenizer the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Mjgn9ptNTrw8",
    "outputId": "a3ade738-267d-4be6-9b3c-37ce412d9f35"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_row):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_row: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    # Samples with additional context into.\n",
    "    if data_row['input']:\n",
    "        text = 'Below is an instruction that describes a task, paired with an input that provides' \\\n",
    "               ' further context. Write a response that appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_row[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Context:\\n{data_row[\"input\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_row[\"output\"]}'\n",
    "\n",
    "    # Without\n",
    "    else:\n",
    "        text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_row[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_row[\"output\"]}'\n",
    "    return text\n",
    "\n",
    "# add the \"prompt\" column in the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in data]\n",
    "data = data.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lIPlPAQ0S9Hk",
    "outputId": "e2cac52b-ac03-4813-ca5d-8f33f6433d2a"
   },
   "outputs": [],
   "source": [
    "str(data['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m6oFEdeTW_G"
   },
   "outputs": [],
   "source": [
    "generate_prompt(data['prompt'][10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmwdXOBGoZF7"
   },
   "source": [
    "We'll need to tokenize our data so the model can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "810o72N7SI9A"
   },
   "outputs": [],
   "source": [
    "data = data.shuffle(seed=42)  # Shuffle dataset here\n",
    "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True) # tokenize all rows in the data of the prompt column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H_wXxkhC8Yv"
   },
   "source": [
    "Split dataset into 90% for training and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TKCLTOVDR1x"
   },
   "outputs": [],
   "source": [
    "data = data.train_test_split(test_size=0.1)\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgAyy_xDamxg"
   },
   "outputs": [],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzNQf6lkqo-T"
   },
   "source": [
    "## Step 4 - Apply Lora  \n",
    "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMELsVV6q2my"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable() # tells AutoModel library that training is going to happen, and checkpoints need to be saved.\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkQmGWXvrNBp"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qdqo_GSiT-3p"
   },
   "source": [
    "Add adapter to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QETeqWrTNjR"
   },
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config, adapter_name=\"lora_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jGWwA25r-x0"
   },
   "source": [
    "## Step 5 - Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G6w-TvuU5lN"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlBiY1OzFZnN"
   },
   "source": [
    "Setting the training arguments:\n",
    "* for the reason of demo, we just ran it for few steps (5) just to showcase how to use this integration with existing tools on the HF ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZRzecfdjb1B"
   },
   "outputs": [],
   "source": [
    "!pip install -q trl==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTdPmdXupyj8"
   },
   "outputs": [],
   "source": [
    "# Here I reload the model AGAIN and specify it should be loaded in a single GPU to avoid errors \"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! when resuming training\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQyMqLg5izHF"
   },
   "outputs": [],
   "source": [
    "# code using SFTTrainer\n",
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# https://huggingface.co/docs/trl/en/sft_trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=1,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"step\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnXtaw9qFcz6"
   },
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4HNvrh5FYqM"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PksHLfiM_9V"
   },
   "source": [
    " Share adapters on the ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV1csnOBNEyK"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"mistral_7b_finance_finetuned_test\")\n",
    "tokenizer.push_to_hub(\"mistral_7b_finance_finetuned_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki-m4hzHFqTu"
   },
   "source": [
    "## Step 6 Evaluating the model qualitatively: run an inference!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edIRBmRBaPBG"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIqHF9Zlfrc"
   },
   "source": [
    "Load directly adapters from the Hub using the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW3oeIAMlerc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"sampurnr/mistral_7b_finance_finetuned_test\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz0l3qR7oC3h"
   },
   "source": [
    "You can then directly use the trained model that you have loaded from the ðŸ¤— Hub for inference as you would do it usually in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdx9wI0ZdVZr"
   },
   "outputs": [],
   "source": [
    "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
