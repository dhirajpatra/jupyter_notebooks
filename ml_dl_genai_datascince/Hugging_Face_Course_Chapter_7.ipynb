{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVCmOyuVxRK-"
   },
   "source": [
    "# DataCollator\n",
    "The `DataCollator` is a core component of the Hugging Face `transformers` library, specifically designed for managing the batching process during model training and inference. It ensures that inputs of varying lengths (like text sequences) are properly padded, tokenized, and collated into a batch that can be processed by a model. Understanding its details will give you more control over how data is processed and fed into transformers.\n",
    "\n",
    "\n",
    "### 1. **What is a Data Collator?**\n",
    "A `DataCollator` in `transformers` is a function or class responsible for combining several samples (like tokenized text) into a batch during training or inference. It typically handles tasks such as:\n",
    "- Padding sequences of varying lengths.\n",
    "- Creating attention masks.\n",
    "- Handling special tokens (e.g., `[CLS]`, `[SEP]`, etc.).\n",
    "- Formatting input for models.\n",
    "  \n",
    "It operates at the level between raw tokenized data and the input to the model. The transformers library comes with pre-built collators, but you can also create custom ones.\n",
    "\n",
    "### 2. **Pre-Built Data Collators in the Transformers Library**\n",
    "\n",
    "The library provides several types of `DataCollator` classes:\n",
    "\n",
    "#### 2.1. `DataCollatorWithPadding`\n",
    "This collator pads each sequence in a batch to the length of the longest sequence, making sure all the input tensors are the same size. It is particularly useful when working with models that expect fixed-length inputs (like BERT).\n",
    "\n",
    "- **Key Parameters**:\n",
    "  - `tokenizer`: The tokenizer used to convert text to tokens.\n",
    "  - `padding`: Defines the padding strategy, such as `True`, `'max_length'`, `'longest'`, etc.\n",
    "  - `max_length`: If specified, it will ensure that sequences longer than this are truncated.\n",
    "  - `pad_to_multiple_of`: If set, it will pad sequences to a multiple of this value, which is useful for optimized GPU performance.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  from transformers import DataCollatorWithPadding\n",
    "  data_collator = DataCollatorWithPadding(tokenizer)\n",
    "  ```\n",
    "\n",
    "- **Behind the Scenes**:\n",
    "  When a batch of tokenized sequences is passed, it uses the tokenizer’s `pad()` method to ensure that all sequences are padded to the correct length. It also generates attention masks for padded tokens.\n",
    "\n",
    "#### 2.2. `DataCollatorForLanguageModeling`\n",
    "This collator is designed for language modeling tasks (like BERT's masked language modeling). It randomly masks tokens in the input sequence and creates labels for those masked positions.\n",
    "\n",
    "- **Key Parameters**:\n",
    "  - `tokenizer`: Tokenizer used to encode the data.\n",
    "  - `mlm`: Whether to use masked language modeling (default is `True`).\n",
    "  - `mlm_probability`: Probability of masking a token in the sequence.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  from transformers import DataCollatorForLanguageModeling\n",
    "  data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15)\n",
    "  ```\n",
    "\n",
    "- **Behind the Scenes**:\n",
    "  It first tokenizes the input, then randomly selects tokens to mask based on `mlm_probability`. Masked tokens are replaced with the `[MASK]` token, random words, or left unchanged with a 10%, 10%, 80% probability split, respectively.\n",
    "\n",
    "#### 2.3. `DataCollatorForSeq2Seq`\n",
    "This collator is specifically designed for sequence-to-sequence models (like BART, T5). It ensures the input and output sequences are padded separately.\n",
    "\n",
    "- **Key Parameters**:\n",
    "  - `tokenizer`: Tokenizer used to encode the input and output data.\n",
    "  - `model`: The sequence-to-sequence model (important for special token handling).\n",
    "  - `padding`: Strategy to pad input and target sequences.\n",
    "  - `max_length`: Maximum length for the input sequences.\n",
    "  - `max_target_length`: Maximum length for the target sequences.\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  from transformers import DataCollatorForSeq2Seq\n",
    "  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "  ```\n",
    "\n",
    "- **Behind the Scenes**:\n",
    "  When you provide a batch of tokenized source and target sequences, it pads them independently. It ensures that the model has appropriate inputs and attention masks, which is important for models like T5 and BART that operate on both encoder and decoder input/output.\n",
    "\n",
    "#### 2.4. `DataCollatorForTokenClassification`\n",
    "This collator is used in token classification tasks (e.g., Named Entity Recognition). It ensures that both the input sequences and the labels (tags for each token) are padded correctly.\n",
    "\n",
    "- **Key Parameters**:\n",
    "  - `tokenizer`: The tokenizer to handle input sequences.\n",
    "  - `padding`: Strategy to pad inputs.\n",
    "  - `label_pad_token_id`: ID for padded label tokens (to ensure consistency across batch sizes).\n",
    "\n",
    "- **Example Usage**:\n",
    "  ```python\n",
    "  from transformers import DataCollatorForTokenClassification\n",
    "  data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, label_pad_token_id=-100)\n",
    "  ```\n",
    "\n",
    "- **Behind the Scenes**:\n",
    "  It ensures that token labels (tags) are padded along with the tokenized input sequences. This is important when dealing with models that predict labels for each token in the input (e.g., BERT for token classification).\n",
    "\n",
    "### 3. **Creating Custom Data Collators**\n",
    "While Hugging Face provides several built-in data collators, there are cases when you may need to create a custom one. Custom collators allow you to control how data is batched and processed before it reaches the model.\n",
    "\n",
    "- **Custom Collator Example**:\n",
    "  ```python\n",
    "  class MyCustomCollator:\n",
    "      def __init__(self, tokenizer, max_length=None):\n",
    "          self.tokenizer = tokenizer\n",
    "          self.max_length = max_length\n",
    "      \n",
    "      def __call__(self, batch):\n",
    "          # Tokenizing and padding input sequences in the batch\n",
    "          inputs = [item['input_ids'] for item in batch]\n",
    "          inputs_padded = self.tokenizer.pad(\n",
    "              {\"input_ids\": inputs},\n",
    "              padding=True,\n",
    "              max_length=self.max_length,\n",
    "              return_tensors=\"pt\"\n",
    "          )\n",
    "          \n",
    "          # Tokenizing and padding labels (if applicable)\n",
    "          labels = [item['labels'] for item in batch]\n",
    "          labels_padded = self.tokenizer.pad(\n",
    "              {\"input_ids\": labels},\n",
    "              padding=True,\n",
    "              max_length=self.max_length,\n",
    "              return_tensors=\"pt\"\n",
    "          )\n",
    "          \n",
    "          return {\n",
    "              \"input_ids\": inputs_padded['input_ids'],\n",
    "              \"labels\": labels_padded['input_ids']\n",
    "          }\n",
    "  ```\n",
    "\n",
    "  In this example, we define a collator that handles both input sequences and labels by tokenizing them and then padding them to the desired `max_length`. It also uses PyTorch tensors (`return_tensors=\"pt\"`), making it compatible with models that use PyTorch as a backend.\n",
    "\n",
    "### 4. **Collating with PyTorch's DataLoader**\n",
    "\n",
    "The `DataCollator` is typically used in conjunction with PyTorch's `DataLoader`. Here's how it fits into a training loop:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Define your dataset (usually a Dataset object)\n",
    "train_dataset = ...\n",
    "\n",
    "# Initialize the collator and data loader\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "# Training loop\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # Pass these to your model and compute loss, etc.\n",
    "```\n",
    "\n",
    "### 5. **Special Cases and Advanced Collation**\n",
    "\n",
    "- **Dynamic Padding**: Data collators like `DataCollatorWithPadding` pad sequences dynamically within a batch, which is useful when sequences vary in length. It reduces wasted computation by avoiding padding to a fixed maximum length.\n",
    "- **Handling Multiple Tokenizers**: Some models may use different tokenizers for input and output sequences (e.g., a source-target setup in translation). In such cases, the `DataCollatorForSeq2Seq` manages both encoder and decoder tokenization and padding separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpQ5zv38yUTF"
   },
   "source": [
    "# Model Evaluation Metrics: Perplexity, BLEU, and ROUGE\n",
    "\n",
    "When evaluating models, particularly in Natural Language Processing (NLP), we often use a variety of metrics to assess the quality of model outputs. Below, I'll explain **perplexity**, **BLEU**, and **ROUGE**, their use cases, and how to implement them using Python and NumPy. I will also show how to use the `evaluate` library to compute these metrics, and how you can create custom evaluation metrics.\n",
    "\n",
    "### 1. **Perplexity**\n",
    "\n",
    "#### Explanation\n",
    "**Perplexity** is commonly used to evaluate language models. It measures how well a probability distribution or probability model predicts a sample. A lower perplexity indicates the model is better at predicting the next word in a sequence. Mathematically, it is the exponentiated cross-entropy loss.\n",
    "\n",
    "For a language model predicting the next word \\( w_t \\) in a sequence, perplexity is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(P) = \\exp\\left( - \\frac{1}{N} \\sum_{t=1}^{N} \\log P(w_t | w_1, \\dots, w_{t-1}) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $ is the number of words in the sequence.\n",
    "- $ P(w_t | w_1, \\dots, w_{t-1}) $ is the probability assigned by the model to the true next word.\n",
    "\n",
    "#### Utilization with the `evaluate` library\n",
    "The `evaluate` library provides support for metrics like perplexity.\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "# Load perplexity metric\n",
    "perplexity_metric = evaluate.load(\"perplexity\")\n",
    "\n",
    "# Simulate predictions and references (for perplexity, it works on probabilities)\n",
    "predictions = [0.2, 0.3, 0.5, 0.7, 0.6]\n",
    "references = [1, 0, 1, 1, 1]\n",
    "\n",
    "# Compute perplexity\n",
    "results = perplexity_metric.compute(predictions=predictions, references=references)\n",
    "print(\"Perplexity:\", results)\n",
    "```\n",
    "\n",
    "#### Implementation of Perplexity using NumPy\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def perplexity(probs):\n",
    "    N = len(probs)\n",
    "    cross_entropy = -np.sum(np.log(probs)) / N\n",
    "    return np.exp(cross_entropy)\n",
    "\n",
    "# Example: probabilities assigned by a language model\n",
    "probs = np.array([0.2, 0.3, 0.5, 0.7, 0.6])\n",
    "\n",
    "# Calculate perplexity\n",
    "pp = perplexity(probs)\n",
    "print(f\"Perplexity: {pp}\")\n",
    "```\n",
    "\n",
    "### 2. **BLEU (Bilingual Evaluation Understudy Score)**\n",
    "\n",
    "#### Explanation\n",
    "**BLEU** is a precision-based metric for evaluating machine translation and text generation models. It compares the n-grams (sequences of words) in the predicted sentence to those in the reference sentence(s). BLEU ranges from 0 to 1, where higher values indicate better translations.\n",
    "\n",
    "The formula for BLEU is:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ BP $ is the brevity penalty to penalize short translations.\n",
    "- $ w_n $ are the weights (usually equal) for each n-gram.\n",
    "- $ p_n $ is the precision for n-grams of size $ n $.\n",
    "\n",
    "#### Utilization with the `evaluate` library\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# Example: predictions and references\n",
    "predictions = [[\"this\", \"is\", \"a\", \"test\"]]\n",
    "references = [[[\"this\", \"is\", \"a\", \"test\"]]]\n",
    "\n",
    "# Compute BLEU score\n",
    "results = bleu_metric.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", results)\n",
    "```\n",
    "\n",
    "#### Implementation of BLEU using NumPy\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def n_gram_precision(reference, candidate, n):\n",
    "    ref_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
    "    cand_ngrams = Counter([tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)])\n",
    "    \n",
    "    match_count = sum((cand_ngrams & ref_ngrams).values())\n",
    "    total_count = sum(cand_ngrams.values())\n",
    "    \n",
    "    return match_count / total_count if total_count > 0 else 0\n",
    "\n",
    "def brevity_penalty(candidate, reference):\n",
    "    c = len(candidate)\n",
    "    r = len(reference)\n",
    "    return np.exp(1 - r / c) if c < r else 1\n",
    "\n",
    "def bleu_score(reference, candidate, n_gram_weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(candidate, reference)\n",
    "    p_n = [n_gram_precision(reference, candidate, n) for n in range(1, 5)]\n",
    "    \n",
    "    score = bp * np.exp(np.sum([w * np.log(p) for w, p in zip(n_gram_weights, p_n) if p > 0]))\n",
    "    return score\n",
    "\n",
    "# Example usage\n",
    "reference = [\"this\", \"is\", \"a\", \"test\"]\n",
    "candidate = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "bleu = bleu_score(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu}\")\n",
    "```\n",
    "\n",
    "### 3. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "\n",
    "#### Explanation\n",
    "**ROUGE** measures the overlap between the n-grams in the generated text and the reference text. Unlike BLEU, ROUGE focuses on **recall** rather than precision. There are different versions of ROUGE, but the most common are:\n",
    "- **ROUGE-N**: Measures n-gram overlap.\n",
    "- **ROUGE-L**: Measures the longest common subsequence (LCS).\n",
    "- **ROUGE-S**: Measures skip-bigrams (word pairs that occur in the same order, but not necessarily consecutively).\n",
    "\n",
    "#### Utilization with the `evaluate` library\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Example: predictions and references\n",
    "predictions = [\"this is a test\"]\n",
    "references = [\"this is the test\"]\n",
    "\n",
    "# Compute ROUGE score\n",
    "results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE Score:\", results)\n",
    "```\n",
    "\n",
    "#### Implementation of ROUGE-N using NumPy\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "def n_gram_overlap(reference, candidate, n):\n",
    "    ref_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
    "    cand_ngrams = Counter([tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)])\n",
    "    \n",
    "    match_count = sum((cand_ngrams & ref_ngrams).values())\n",
    "    total_ref_count = sum(ref_ngrams.values())\n",
    "    \n",
    "    return match_count / total_ref_count if total_ref_count > 0 else 0\n",
    "\n",
    "# Example usage\n",
    "reference = [\"this\", \"is\", \"the\", \"test\"]\n",
    "candidate = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "rouge_1 = n_gram_overlap(reference, candidate, 1)\n",
    "print(f\"ROUGE-1 Score: {rouge_1}\")\n",
    "```\n",
    "\n",
    "### Creating Custom Evaluation Metrics with `evaluate`\n",
    "\n",
    "You can also create custom evaluation metrics with the `evaluate` library by defining a function that implements your metric, then passing it into `evaluate`.\n",
    "\n",
    "#### Custom Metric Example\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "# Define a custom metric function\n",
    "def custom_accuracy(predictions, references):\n",
    "    correct = sum(p == r for p, r in zip(predictions, references))\n",
    "    return {\"accuracy\": correct / len(predictions)}\n",
    "\n",
    "# Create custom evaluation metric\n",
    "custom_metric = evaluate.Metric.from_function(custom_accuracy)\n",
    "\n",
    "# Example: predictions and references\n",
    "predictions = [1, 0, 1, 1, 0]\n",
    "references = [1, 0, 0, 1, 0]\n",
    "\n",
    "# Compute custom accuracy\n",
    "results = custom_metric.compute(predictions=predictions, references=references)\n",
    "print(\"Custom Accuracy:\", results)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Perplexity** is used for evaluating language models and measures uncertainty.\n",
    "- **BLEU** is used for evaluating machine translation or text generation based on n-gram precision.\n",
    "- **ROUGE** is more recall-oriented and is useful for summarization tasks.\n",
    "\n",
    "The `evaluate` library makes it easy to compute these metrics and customize your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9opnT-dzN8-"
   },
   "source": [
    "# Fine-tuning a model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmNDdxyK4Wyx"
   },
   "source": [
    "### Step 1: Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXsK-7sn49Ra"
   },
   "source": [
    "First things first, we need a dataset suitable for token classification. In this section we will use the [CoNLL-2003](https://huggingface.co/datasets/conll2003) dataset, which contains news stories from Reuters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbClWhEcAPGP",
    "outputId": "79104499-4766-4146-c79a-15f49f427da6"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72DNuhxs5V2t"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVIbde6w5YUn"
   },
   "source": [
    "Inspecting this object shows us the columns present and the split between the training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2xgwtm75llj",
    "outputId": "0b536ff2-70d2-4d8b-8616-608b2a754eab"
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlUqLoqA5tJ3"
   },
   "source": [
    "In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called tokens, but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVsQWizF-LWB"
   },
   "source": [
    "Let’s have a look at the first element of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yj2YBkFG-N_R",
    "outputId": "9353947f-4fdb-4ad4-c13b-b44f83756cea"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCk-ewWY-RB3"
   },
   "source": [
    "Since we want to perform named entity recognition, we will look at the NER tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33AwrvcB-XNH",
    "outputId": "9b8753c6-57df-4613-ef27-a80fa5b8eebe"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHplZgaZ-iYM"
   },
   "source": [
    "Those are the labels as integers ready for training, but they’re not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the features attribute of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dKhdqGW-rBo",
    "outputId": "6f897b3e-4311-4dba-cf24-7195afc4c78a"
   },
   "outputs": [],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZfYvift-p4I"
   },
   "source": [
    "So this column contains elements that are sequences of ClassLabels. The type of the elements of the sequence is in the feature attribute of this ner_feature, and we can access the list of names by looking at the names attribute of that feature:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtfFxzAV-1s-",
    "outputId": "80ef0d8a-ef4c-4716-f46f-560aacb51800"
   },
   "outputs": [],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUiOmOvh-wdM"
   },
   "source": [
    "We already saw these labels when digging into the token-classification pipeline in Chapter 6, but for a quick refresher:\n",
    "\n",
    "- O means the word doesn’t correspond to any entity.\n",
    "- B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "- B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "- B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "- B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity.\n",
    "\n",
    "Now decoding the labels we saw earlier gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zw0CpIXp-81q",
    "outputId": "651e2035-c12e-44e6-8507-d4e6d316c56e"
   },
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLNyLabq_VAg"
   },
   "source": [
    "As usual, our texts need to be converted to token IDs before the model can make sense of them.\n",
    "\n",
    "To begin, let’s create our `tokenizer` object. As we said before, we will be using a BERT pretrained model, so we’ll start by downloading and caching the associated tokenizer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249,
     "referenced_widgets": [
      "c432a6397961411dbf16f83459e98bcc",
      "5a0088c5efc949ef83b0f2c20dc16fad",
      "5e9c83351bd14e30892470c583de5515",
      "5247406e8143450fa5999061d8dc898f",
      "8266993bff30407e808dd557b81ea220",
      "e42374173014452aab602592d8dc767f",
      "d56bebf1b2d54467baaaa69f1289b057",
      "59f80315eefe44f098a764bfcc8bb6f3",
      "257fac0f9a164f898ba447f628300f50",
      "2c7d2dc3b08e440cae75fea98321fa4c",
      "5c26fc8296b24f1f87af195983578d39",
      "1816627be85542519b52cad16a26986f",
      "ee319b8f68564b44ad69dd263ac9d494",
      "e052078320ef40ab8f291c0ef440aa24",
      "c440c861474749e8b73709b33aded2b3",
      "4bdef4cdcdae47dea6d11c7fdee54f64",
      "7277b28d590a4ff7b6cd2e2494e7bf69",
      "ea3023cb13de451dbca37be7bcc587e6",
      "329ef1816aa046e895e7fdab5046634e",
      "1d5125579ee0492cbaf5b1fc2eda0505",
      "094c270593df4b2ab03fd0c143ef06c9",
      "2c2c23a0aa6940ec8779e638e8e6fb13",
      "9263f443815d4537b998fd9e05e9d4ce",
      "7b6908e295d542738bc64e095f9976a1",
      "2517cceb4d744fadad6d97da814091ea",
      "cb678736987d4c68b1c9f80b2029f0b3",
      "9a3123b906e84a1bb34684ff7f74aa27",
      "f3f17d338b864d719634e76f61050d9c",
      "57e4e3734d154637b2ec0df8c47f6d51",
      "14caa2c016d3434abe834a53d97f6e9b",
      "aa99119441d04c85aba35c7f5444db31",
      "990119053cb142a592404a1d2554c8eb",
      "1d1ca51e9e054b4b8ad96a30882a3b85",
      "74c6d319cb874e2d9db4fe948d00eecb",
      "6791503566314aedb86c25ce7f1ddc81",
      "5b62b67ccb934e6aa05d01a33e02b301",
      "2552de15667e4c8183d23be6f7b57ff2",
      "21caf000ad9e41d2b778e4dc47a63ec6",
      "7a6aa612162c484b86ebf5f4f2898950",
      "44fb36359aff499983ece23b4cc0a2ae",
      "9e44a9e18bb54aa0aed1948bc72a6f05",
      "e72fd26ceb6c4fa38d91537a7c478d92",
      "645bc83c645e4e08bad64a9295f5feb3",
      "034fff2d2bca4a81a1eee6e04e86421f",
      "ef53dae6a77e44bca40ddf18a160b7d1",
      "b50137fbaf7a4b74a79a57c39ca64445",
      "9145e18278d14d66a3df5c9f4e4796d5",
      "9acd8b1845b7493f8233dc7ecae8e845",
      "9b86738e74fe4cc4b479d74461d0e0d5",
      "1994a4eace9d4f9ba38a9a1733ea9020",
      "2e6fbc88ed374df489a576d730835e1f",
      "ab7074792fe04ce3a92498ec5ddc77ff",
      "f0e3aa89cb304db6941390b61ee001a1",
      "75d8b85b860f41c68b452834b9abbf36",
      "f9f65eeb2b96450fb1eae5a7d214d389"
     ]
    },
    "id": "Zv6cfTMv_ZgS",
    "outputId": "4c9e4a09-81f8-4091-9cb6-db23b57f8618"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYJlr4fa_jFB"
   },
   "source": [
    "To tokenize a pre-tokenized input, we can use our `tokenizer` as usual and just add `is_split_into_words=True`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66bkUJpI_oJv",
    "outputId": "9980750d-ab32-4465-f43b-9995a4ca3560"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN1c74wr_ulg"
   },
   "source": [
    "As we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginning and `[SEP]` at the end) and left most of the words untouched. The word lamb, however, was tokenized into two subwords, la and ##mb. This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 12 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9YTmGTz_2IG",
    "outputId": "408af264-4a58-4a4a-8ddc-cc4c18df8f0a"
   },
   "outputs": [],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49rpLlEn_4Jh"
   },
   "source": [
    "With a tiny bit of work, we can then expand our label list to match the tokens. The first rule we’ll apply is that special tokens get a label of -100. This is because by default -100 is an index that is ignored in the loss function we will use (cross entropy). Then, each token gets the same label as the token that started the word it’s inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the B- with I- (since the token does not begin the entity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpDd9BBa_6ol"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FLDgsdo_9Hl"
   },
   "source": [
    "Let’s try it out on our first sentence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvHI1vva_-gk",
    "outputId": "762b1561-4675-487e-a21a-d341997ed178"
   },
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUkuX6WkAAcw"
   },
   "source": [
    "As we can see, our function added the -100 for the two special tokens at the beginning and the end, and a new 0 for our word that was split into two tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVXyimtaACxY"
   },
   "source": [
    "To preprocess our whole dataset, we need to tokenize all the inputs and apply align_labels_with_tokens() on all the labels. To take advantage of the speed of our fast tokenizer, it’s best to tokenize lots of texts at the same time, so we’ll write a function that processes a list of examples and use the Dataset.map() method with the option batched=True. The only thing that is different from our previous example is that the word_ids() function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeoK42rOAFas"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxFMdky9AIkg"
   },
   "source": [
    "Note that we haven’t padded our inputs yet; we’ll do that later, when creating the batches with a data collator.\n",
    "\n",
    "We can now apply all that preprocessing in one go on the other splits of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "010e62acee52467abdc36cc82ad25527",
      "b52c0f09af9e4b669fe45f5bf00a4b04",
      "b94dd85d439146a2a48abdad2f8a4070",
      "0598b346b7de4c98a3ceeb7b4292d710",
      "eabbe2d9a2ad48f38114d0966ae61257",
      "063c37ebf7cb4ec1ab17ce4bed03c44c",
      "1940cf6079184682b8444425ecc2e025",
      "5e0595ba334a459a97b6b500774d2d08",
      "bc27f76d0f2446cc8bc1684ab4245957",
      "7dbe88e76b88406081b2dc9b74c25848",
      "bd13300bd1ee40b49291aa6639e3a2be",
      "b6b78bd03fbf483eaa11c4fba2478008",
      "c2d12ad4fab848ec86ce070f66e86181",
      "46344f3ce42944789c710e7790ea5805",
      "144d62d2df5f4cb5a9abae83c8d7c807",
      "c7af02c084ff4d54a023a808bf770502",
      "f8ac8ab94d0f4e36a54d2dd0665a0604",
      "28c5827870dd4ac79dd9d0f8193fa9a9",
      "ac32b02bd2b24cbe86c344805919beec",
      "e25b028ddccb4834bad5eae14b393bdf",
      "6b75a3ce87604ed4b03b985d3cd050cc",
      "c84931b12f244d28b41955d043e4ad73",
      "40801ff5a42a496383ee5e2b28efd4ea",
      "1922b7766d124cc29c252bd680702b2e",
      "d03fada5c33c42209d5f6b7114165621",
      "1e21c54d9af44e228b7727f06bbe5e03",
      "f6232c59d809458fba5a3a7fc0310e2e",
      "075f01095d0e4d809896fb1839e3be0c",
      "887d304079e947e786c5e6f8c50b1133",
      "6c26f764eb1e40ed935df7b90d9963c5",
      "5b64ea407d91409daa893e8e4be914f5",
      "ef79b5acc82f4278a336d6561049375b",
      "f343b547a75f4451a849baa1db100335"
     ]
    },
    "id": "4-TGTuw5AKYN",
    "outputId": "b896149e-edec-4314-b2dc-c847a5849f20"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "245lGQU0n8zA",
    "outputId": "ad9641f5-2876-437a-bf8f-5beb4eb19159"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vITPn8mxobUN",
    "outputId": "329ae755-5c90-4cd2-c312-4aed995f060b"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3yNjtIfzUWj"
   },
   "source": [
    "We can’t just use a DataCollatorWithPadding like in Chapter 3 because that only pads the inputs (input IDs, attention mask, and token type IDs). Here our labels should be padded the exact same way as the inputs so that they stay the same size, using -100 as a value so that the corresponding predictions are ignored in the loss computation.\n",
    "\n",
    "This is all done by a DataCollatorForTokenClassification. Like the DataCollatorWithPadding, it takes the tokenizer used to preprocess the inputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNh3GEXexNhZ"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-5_hW32zXRS"
   },
   "source": [
    "To test this on a few samples, we can just call it on a list of examples from our tokenized training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDphJd8fzZBV",
    "outputId": "84fbf297-2604-4953-d10b-99708bc73513"
   },
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-5rZUvfzc-p"
   },
   "source": [
    "Let’s compare this to the labels for the first and second elements in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sC7se2xzddy",
    "outputId": "9b099ff7-6a43-4dba-cfa7-cfa044ceaff8"
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMyNmxq3zmvE"
   },
   "source": [
    "As we can see, the second set of labels has been padded to the length of the first one using -100s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjv8jue0zyg-"
   },
   "source": [
    "### Step 2: Defining Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yktegqLDzstS"
   },
   "source": [
    "To have the Trainer compute a metric every epoch, we will need to define a `compute_metrics()` function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZEaDbYAz8o8"
   },
   "source": [
    "The traditional framework used to evaluate token classification prediction is `seqeval`. To use this metric, we first need to install the `seqeval` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFJiaBNcze_d",
    "outputId": "b1e4c4db-e070-4b95-a530-1b5b41424b39"
   },
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA9ARwss0VyC",
    "outputId": "c548f1da-36bc-45d5-df6f-7c4a2b89be94"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOwDDlZ70KXC"
   },
   "source": [
    "We can then load it via the `evaluate.load()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b5093688ed75448489e85216a912541e",
      "091a698c803c4b4e8f696fd193b50ef2",
      "f91f248b21274352a9e20dc142c1c6a1",
      "919a824817134c94847ce3e44d1ed432",
      "6b3d4273af754f81aca3f253c844e961",
      "ee463cdfcd194f78ac7564c5bd213ed7",
      "3c2398b0f0f3496080deaa36ff932518",
      "40504d5b60ad44989e280882e3f09e17",
      "167940f712f449e9a04177a4e61bdc58",
      "abb96c14f5724d0c81e423c35e9d2da5",
      "127f767df09240a58375b418587f55ed"
     ]
    },
    "id": "0UK3Wuy3z401",
    "outputId": "caea4895-7fa1-4dfb-91e7-b8ae46cb557d"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu8r1Eut0bij"
   },
   "source": [
    "This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let’s see how it works. First, we’ll get the labels for our first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OBatNbh0cSK",
    "outputId": "ad7ce339-e33e-4fa9-82e1-a11e443d3e02"
   },
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG3_mFev0f_6"
   },
   "source": [
    "We can then create fake predictions for those by just changing the value at index 2:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTPKFpRZ0gPy",
    "outputId": "6ed561b9-83ff-4b92-938c-91636eda6f20"
   },
   "outputs": [],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaLi3fWJ0t0j"
   },
   "source": [
    "Note that the metric takes a list of predictions (not just one) and a list of labels.\n",
    "\n",
    "\n",
    "This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. For our metric computation we will only keep the overall score, but feel free to tweak the compute_metrics() function to return all the metrics you would like reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0rDUgsw04qn"
   },
   "source": [
    "This `compute_metrics()` function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don’t need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the `metric.compute()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qquu1Xw00ynW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJMK0Yvi09cC"
   },
   "source": [
    "Now that this is done, we are almost ready to define our `Trainer`. We just need a `model` to fine-tune!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2fas-o1CcV"
   },
   "source": [
    "### Step 3: Defining the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hrv3S6aR1HoX"
   },
   "source": [
    "Since we are working on a token classification problem, we will use the `AutoModelForTokenClassification` class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the `num_labels` argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it’s better to set the correct label correspondences instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1aMKrrh1Oau"
   },
   "source": [
    "They should be set by two dictionaries, `id2label` and `label2id`, which contain the mappings from ID to label and vice versa:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdC-afEb1A3C"
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExSGHGg41TAW"
   },
   "source": [
    "Now we can just pass them to the `AutoModelForTokenClassification.from_pretrained()` method, and they will be set in the model’s configuration and then properly saved and uploaded to the Hub:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "e76065a3dd924c40b3fa2b6ee8fb9d8d",
      "77059a286d4f4d3a86e767c527ed4d60",
      "bd276e7b01f34d65b906fd3c24b3b904",
      "4b7d76dcc9d840b2b50626ebd0b04937",
      "bebe465436a14895b71cc2e010ad0445",
      "9b7a805b08e04835abc38ba66705151b",
      "0726b17bbf3d492b8d65413e5ebfbe7f",
      "402c7949b3964ab58b31f10d13ca1ef0",
      "6967eb7da47e4ffbb127b86207cd21cb",
      "6eae153930ca45ab9fbc8dbef805e126",
      "327d08d871744461b50b6099e4ef2c53"
     ]
    },
    "id": "JM6wH_hl1XNI",
    "outputId": "69791d91-31cd-4c50-fb14-4a171aeaab99"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixb1VKRY1afY"
   },
   "source": [
    "Let’s double-check that our model has the right number of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9LdUA3Y1gUj",
    "outputId": "373d34e1-7e27-440b-ba55-a1eb68a2341c"
   },
   "outputs": [],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmN7Taq01iht"
   },
   "source": [
    "### Step 4: Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOCmTlP31tPw"
   },
   "source": [
    "We are now ready to train our model! We just need to do two last things before we define our `Trainer`: log in to Hugging Face and define our training arguments. If you’re working in a notebook, there’s a convenience function to help you with this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1CTpSp_2t0q"
   },
   "source": [
    "This will display a widget where you can enter your Hugging Face login credentials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "22a7532d3aa24f8c9cacd2dd88147cbb",
      "39c6f826e22e45ef83cdf41709c01490",
      "237476cf8fca46528c936369c2bfb0ae",
      "b3db69fe469e421eac21c6fa26e88d99",
      "de21f7bf1b8a4cbbaf19e4e615bafd66",
      "c4e396d02f174b8cb2fbcdc61142e87a",
      "b71c7fc0420f47619e6533e03818ae55",
      "619bcf383e9040ae9dd57959417e6ce0",
      "a988a8f7c06941a6898ed1222ef1f00c",
      "827268bdc4634bcfa587f3d835cee053",
      "569f0c1325bc4a4cbede1e70e7f177cc",
      "232c8bf91372462c9023c7fccd6fbd2f",
      "7d32ad1e210448fcbb0f36a29f0564e0",
      "ed9cb71c43704668848dbdeaae086eb4",
      "ce2ffb4f6b0d42c9ba315aae2afbc1b5",
      "7850f8a6552340208ac920aa2c04c663",
      "df97d4bcbb7841df8df7596ee41e878f",
      "f0d87e9ae5fe46beae8427f947bd9e97",
      "be5869ee6f524e1094bf859dcbf4237d",
      "29ab5918540049d79118fa2b1d15a997",
      "ea00bc8fef5e41cc841ad24e2538b1f0",
      "3000dac0a8724bc8982161f0dc1a01cd",
      "d60943d5fca74aa68d6ab04a7a45eb81",
      "d1b863c7d6a649a5abee685c23fc8cdb",
      "09e59a6a04ba480d9b30fefcfdb1430a",
      "41818e91d3154c88bba2327dcf3cd9e2",
      "115dc04652be463da06834c306ddd3b3",
      "89e2948439a8422eb57047d17d48fd53",
      "c559a782967e49ec80e35207e2341931",
      "ad9583058a9f4b3fb5777663986f2605",
      "89f31fd33be64006bab6054c610aa79a",
      "63a253a8d8be4d36b293e1db20d47350"
     ]
    },
    "id": "esuLIKYT2pQQ",
    "outputId": "08ce1a63-ac4b-443a-d9e3-bc3c32cd7810"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEp8H7LL2w3_"
   },
   "source": [
    "Once this is done, we can define our TrainingArguments:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP821RUu1oQ3",
    "outputId": "e1dc50d9-cdf0-4fb4-d9ba-e76b2a6eb52c"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"finetuned-geeks\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3B-fnIE25gA"
   },
   "source": [
    "You’ve seen most of those before: we set some hyperparameters (like the learning rate, the number of epochs to train for, and the weight decay), and we specify `push_to_hub=True` to indicate that we want to save the model and evaluate it at the end of every epoch, and that we want to upload our results to the Model Hub.\n",
    "\n",
    "Note that you can specify the name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). By default, the repository used will be in your namespace and named after the output directory you set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GTS1RGP2TcB"
   },
   "source": [
    "Finally, we just pass everything to the `Trainer` and launch the training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XnaHiaqp2Qq0",
    "outputId": "dc1486ec-19e5-4ba7-c794-087540b16fb1"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD7OCOLT3RDT"
   },
   "source": [
    "Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L90Exnif3s-A"
   },
   "source": [
    "Once the training is complete, we use the `push_to_hub()` method to make sure we upload the most recent version of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "fsg_EdGL3RZk",
    "outputId": "27d168ea-393f-4251-f2cf-f5d12f52c916"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioy3gyuy30W7"
   },
   "source": [
    "This command returns the URL of the commit it just did, if you want to inspect it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC-bB8pT34q9"
   },
   "source": [
    "The Trainer also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task — congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a4Iuzsi37v1"
   },
   "source": [
    "### Step 5: Using the fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611,
     "referenced_widgets": [
      "a540e652957e4bd19b67c3f3111fdff1",
      "2548ba134d8347698f84ff99e925defa",
      "a904ee5e434041fcaa36bd433c8725a3",
      "fc9e37df37eb432a81469dfc4672d91c",
      "11bd244bab8c4d3a8608d6b9323fda35",
      "f4a9503a0f9b492d8d1c80c00f827f53",
      "ed87d55e23f9428dba6dcfb15987f43a",
      "e9202653802e427d89903d9d1c002510",
      "86eab8a8196544e3b3fd412e515898d7",
      "979da8bcab4b4b049a7a0146a208deb5",
      "6cf66c540ce841bf8edcdf4ea35095ed",
      "f800b5cc88b548a3a2eceab2546e15fc",
      "125ebe1edf4445328c79758065ee8797",
      "d20a1171acc449388b55f2e2f9dc5a24",
      "f8fb946c9dd349f98f32f185672c07b4",
      "4c8cfafd338a4c4ca05dccaa9acbef99",
      "4f6942a4604d40f69b1e0849579294c9",
      "1771fa48f8fa4594a0a540f7f6cfc462",
      "8971d397fd8747b59a36b2dbe97c8526",
      "da2526251a1d4e5b87d14433c4e4be01",
      "fe0feaccdf28429e92d9dd5178e23019",
      "e305a3ed348d4604a31b8e85732ca61f",
      "5afc1242a0ab4874ac2b8de7fb9adce7",
      "e210782700484420b704e20e22c1d599",
      "6a08ceb3219b4214a2fae3fdb2112f1b",
      "3c3991b27ce3422fb1c3a4ec8567ffff",
      "529cfebddab449cfbeae5089c33751bb",
      "7c7cfc46810c4ebfb4c7f09a8c33f925",
      "dacd8496a03e45f09bef2ffec4d66dcb",
      "b533dd222aa048a296c4113cd36a06ad",
      "6b9df9268f8243a783b21ae66e491789",
      "bc87622e98c343edb10e0ff6f3e65827",
      "ed73aa93eb6e4ece886158cd32fd22b4",
      "4129b2707ade475fa3111ce5f2e9b189",
      "98462d3b688747a79ee847b34061a88c",
      "87cf9c4cad3f4901b7046df5674c2f76",
      "334d684f3c7e41af9e1625796f559be3",
      "12155561a6fe4890bf561b421abb4edf",
      "cfd65643b6404a228fe750cd9aebcc68",
      "b1d26ec7fe5d477483aad35e560370e3",
      "b6b9e2f351d2402890ee90a931cf0387",
      "f325bd694188448ba3fbaf6c2beee4bd",
      "fbfa2c457bde4bca9f34dbe22317945b",
      "aae19872608d4c63944a3bff7e03b3a4",
      "70829e418ace42ecb22bba71a1e28b14",
      "6d4e5693f58f4893a5b42c3e593592ce",
      "102ed7adf0e640c68e73870de0eec2cd",
      "53ff2b0f9f0f4190ac3b1c8918592f2f",
      "c40a4c48590c41e3b878b99ef4820099",
      "74b89d6ae095454597b44425f4a67dac",
      "8a584928796b4e61b8923dcea7bb859e",
      "5dfa80b66b8a4ca99929b1b56d98dc8f",
      "25bc13e7b1c6474cbd781f73ad8301e2",
      "94c6f30f7b174c24add8f819ad5c7e41",
      "beae3c1a05344f4cbd54a2bb87585f2f",
      "0a91a4a4596b46ceb4d37a01c63fa3d4",
      "87f8e17da15e4e9eb38cf5ef02d00eaa",
      "83a7802f72fa46deb1e53eb7a76cf80e",
      "b280453f8c7549bbb714a45b770b59ab",
      "8606f4dcd09f429c8d4cdad8994aad1a",
      "a5a11cc571fe45f4aaa03f4d7c781c97",
      "8fc580470f914e25b7ab4b4bc512140d",
      "6342c2fb3c02423ab3b72d4281b7d6a0",
      "17bc0038a2364679a2d9c1bd051338ba",
      "7c648c9cfc9742f1b1bf658d611b20ac",
      "8b5f8ee41e544a21a321f9ddd7c85fa1"
     ]
    },
    "id": "qJzvFjte31du",
    "outputId": "01dbbd07-4781-4226-8cd0-268446ad7526"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"sampurnr/finetuned-geeks\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sampurn and I work at GFG in India.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FpcIzg7utQd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
