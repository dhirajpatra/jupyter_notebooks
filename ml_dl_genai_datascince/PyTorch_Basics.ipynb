{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-_tnJd7pbBx"
   },
   "source": [
    "Let’s dive into PyTorch, a popular deep learning framework that’s widely used in data science. I’ll cover the core concepts and show you as much code as possible to ensure you can understand and implement these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMWjwaoYLTjK",
    "outputId": "a9441c62-6072-477d-9239-54d37d93149b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lista = [1, 2, 3]\n",
    "print(\"Python List:\", lista)\n",
    "\n",
    "# Create a NumPy array\n",
    "a = np.array(lista)\n",
    "print(\"NumPy Array:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVfp24x_pmaq"
   },
   "source": [
    "### What is PyTorch?\n",
    "\n",
    "**PyTorch** is an open-source machine learning library developed by Facebook’s AI Research lab. It provides two main features:\n",
    "\n",
    "1. **Tensor Computation (like NumPy) with strong GPU acceleration.**\n",
    "2. **Deep Neural Networks built on a tape-based autograd system.**\n",
    "\n",
    "PyTorch is designed to be intuitive and flexible, making it a favorite among researchers and practitioners for developing machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30jw5KvDpq4G"
   },
   "source": [
    "### 1. Tensors: The Core of PyTorch\n",
    "\n",
    "A **Tensor** is a multi-dimensional array, similar to NumPy’s `ndarray`, but with additional capabilities, such as being able to run on a GPU for accelerated computing.\n",
    "\n",
    "Here’s how you create and manipulate tensors in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7V5Y_wMgql_X",
    "outputId": "cd74e19d-420a-4244-e268-67dea032a858"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(\"1D Tensor:\", x)\n",
    "\n",
    "# Create a 2D tensor (matrix)\n",
    "y = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"2D Tensor:\", y)\n",
    "\n",
    "# Tensor operations\n",
    "z = x + 1\n",
    "print(\"Tensor after addition:\", z)\n",
    "\n",
    "# Tensors on GPU\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to('cuda')  # Move tensor to GPU\n",
    "    print(\"Tensor on GPU:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1akCg6yqsQH"
   },
   "source": [
    "**Interview Tip:** An interviewer might ask, \"What is a tensor, and how does it differ from a NumPy array?\" A good response is that a tensor is similar to a NumPy array but with additional support for GPU acceleration and automatic differentiation, which are essential for deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPXeQnijpq1L"
   },
   "source": [
    "### 2. Autograd: Automatic Differentiation\n",
    "\n",
    "PyTorch provides an **autograd** package that automatically calculates the gradients of tensors. This is crucial for backpropagation in training neural networks.\n",
    "\n",
    "Let’s look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fwxA3ZCqZxS",
    "outputId": "3ecaa0f7-0164-4bd9-c444-73b37b885688"
   },
   "outputs": [],
   "source": [
    "# Create a tensor with requires_grad=True to track operations\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "y = x ** 2 + 2 * x + 1\n",
    "\n",
    "# dy/dx = 2x + 2\n",
    "# grad(x=1) = 4\n",
    "# grad(x=2) = 6\n",
    "# grad(x=3) = 8\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "y.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IL2rZTKbqfbi"
   },
   "source": [
    "In this example, `x.grad` will contain the derivatives of `y` with respect to `x`. This is essential for updating the weights during training.\n",
    "\n",
    "**Interview Tip:** An interviewer might ask, \"How does autograd work in PyTorch?\" You could explain that PyTorch uses a dynamic computational graph, meaning the graph is built as operations are performed. When `backward()` is called, gradients are computed by traversing this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9oBm4pVpqxz"
   },
   "source": [
    "### 3. Building Neural Networks\n",
    "\n",
    "In PyTorch, neural networks are built using the `torch.nn` module, which provides all the building blocks to define and train models. Here’s how you can create a simple feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XudNfG5cq5ke",
    "outputId": "ba91e0b7-633d-4de4-b1ef-5eadb46e30b9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 5)  # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(5, 1)  # Fully connected layer 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean squared error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Sample data\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "actuals = torch.tensor([[10.0], [20.0]])\n",
    "\n",
    "# Forward pass: Compute predicted output by passing inputs to the model\n",
    "predictions = model(inputs)\n",
    "print(predictions)\n",
    "\n",
    "# Compute and print loss\n",
    "loss = criterion(predictions, actuals)\n",
    "print('Loss:', loss.item())\n",
    "\n",
    "# Backward pass and optimize\n",
    "optimizer.zero_grad()  # Zero the gradients\n",
    "loss.backward()  # Backpropagation\n",
    "optimizer.step()  # Update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41yNtAtkq-H8"
   },
   "source": [
    "**Interview Tip:** You might be asked, \"What is the purpose of `optimizer.zero_grad()`?\" Explain that PyTorch accumulates gradients by default, so `zero_grad()` is used to reset the gradients before computing them in the backward pass. This prevents gradients from being incorrectly accumulated over multiple backward passes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pSOqn_jpqur"
   },
   "source": [
    "### 4. Training a Model\n",
    "\n",
    "Here’s how you typically train a model in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIIcVSXOrBUy",
    "outputId": "899e1509-7493-4d41-e6cf-0536c7255d2e"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "    loss = criterion(predictions, actuals)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz9d6fEmrFW2"
   },
   "source": [
    "**Interview Tip:** Be prepared to discuss the training process, including the roles of forward and backward passes, loss calculation, and optimization. You might also be asked about overfitting and techniques to prevent it, such as using regularization or dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9U7dUtzpqrz"
   },
   "source": [
    "### 5. Data Handling with `torch.utils.data`\n",
    "\n",
    "In data science, efficiently loading and processing data is crucial. PyTorch provides the `torch.utils.data` module, which includes `Dataset` and `DataLoader` classes for handling data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_oZNLnzrJwm",
    "outputId": "ef91182d-ec00-482d-e8a7-0097443e9ccd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Initialize the data here\n",
    "        self.data = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        self.targets = torch.tensor([[10.0], [20.0]])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a sample and its corresponding target\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate over the data\n",
    "for data, targets in dataloader:\n",
    "    print(\"Data:\", data)\n",
    "    print(\"Targets:\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpMCRK8MrNfr"
   },
   "source": [
    "**Interview Tip:** If asked about data loading, you should know how `Dataset` and `DataLoader` work. The `DataLoader` is particularly useful for batching and shuffling the data, which are important for training models efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNWoVPAnpqo8"
   },
   "source": [
    "### 6. Transfer Learning\n",
    "\n",
    "Transfer learning involves taking a pre-trained model and fine-tuning it on a new dataset. This is common in data science when you have limited data but still want to leverage powerful models.\n",
    "\n",
    "Here’s a simple example using a pre-trained model from `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeGrmjR0rQzY",
    "outputId": "452b7e51-f31d-40d4-891d-57230b82f155"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer (classifier) to match the number of classes in your dataset\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # Assume 10 output classes\n",
    "\n",
    "# Now, only the new layer's parameters will be optimized\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Forward pass, loss computation, backward pass, and optimization proceed as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSFxAYZKrUFh"
   },
   "source": [
    "**Interview Tip:** You might be asked, \"What is transfer learning, and why is it useful?\" Transfer learning allows you to use models trained on large datasets (like ImageNet) as a starting point, which can significantly speed up training and improve performance on your own task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayX73HqBpqib"
   },
   "source": [
    "### 7. Model Evaluation and Metrics\n",
    "\n",
    "After training a model, evaluating its performance on unseen data (i.e., the test set) is crucial. In PyTorch, this involves switching the model to evaluation mode, calculating predictions, and comparing them with the actual labels using various metrics.\n",
    "\n",
    "#### Switching to Evaluation Mode\n",
    "\n",
    "When evaluating the model, you should disable dropout and batch normalization, which behave differently during training and testing. This is done using `model.eval()`.\n",
    "\n",
    "```python\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example with no_grad (more on this below)\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    # Here you would compute the metrics (like accuracy, precision, etc.)\n",
    "```\n",
    "\n",
    "The `torch.no_grad()` context manager is used to disable gradient calculation, which saves memory and computations during evaluation.\n",
    "\n",
    "#### Calculating Accuracy\n",
    "\n",
    "Here’s a simple example of how to calculate accuracy:\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "```\n",
    "\n",
    "**Interview Tip:** You might be asked, \"What’s the purpose of `model.eval()`?\" The answer is that it tells the model you are in inference mode, so layers like dropout or batch normalization behave accordingly, which is different from their behavior during training.\n",
    "\n",
    "### 8. Saving and Loading Models\n",
    "\n",
    "Saving and loading models are crucial for reusing trained models or resuming training. PyTorch provides an easy way to do this using `torch.save` and `torch.load`.\n",
    "\n",
    "#### Saving the Model\n",
    "\n",
    "```python\n",
    "# Save the entire model\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# Or save just the state dict (recommended)\n",
    "torch.save(model.state_dict(), 'model_state.pth')\n",
    "```\n",
    "\n",
    "#### Loading the Model\n",
    "\n",
    "```python\n",
    "# Load the entire model\n",
    "model = torch.load('model.pth')\n",
    "\n",
    "# Or load the state dict\n",
    "model = SimpleNN()\n",
    "model.load_state_dict(torch.load('model_state.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "```\n",
    "\n",
    "**Interview Tip:** An interviewer might ask, \"Why is saving the state dict of a model preferred over saving the entire model?\" The state dict only contains the parameters and buffers, which makes it more flexible. For example, you can load the state dict into a model with the same architecture but different code.\n",
    "\n",
    "### 9. Using GPUs Effectively\n",
    "\n",
    "To speed up training, you should leverage GPUs, especially for large models and datasets. PyTorch makes it easy to move data and models to the GPU.\n",
    "\n",
    "#### Moving Models and Tensors to GPU\n",
    "\n",
    "```python\n",
    "# Move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Move inputs and targets to GPU\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "```\n",
    "\n",
    "#### Mixed Precision Training\n",
    "\n",
    "Mixed precision training involves using both 16-bit and 32-bit floating-point numbers, which can accelerate training and reduce memory usage. PyTorch supports this with the `torch.cuda.amp` module.\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "**Interview Tip:** You could be asked, \"What are the benefits of using mixed precision training?\" It allows faster computations and reduced memory usage while maintaining model accuracy, which is especially beneficial when training large models on GPUs.\n",
    "\n",
    "### 10. Common Neural Network Architectures\n",
    "\n",
    "PyTorch makes it easy to implement various neural network architectures. Let’s look at some common ones:\n",
    "\n",
    "#### 10.1 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are widely used for image data. Here’s a simple example of a CNN in PyTorch:\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # 1 input channel, 32 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)  # Assuming input size is 28x28\n",
    "        self.fc2 = nn.Linear(128, 10)  # Assuming 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Interview Tip:** You might be asked, \"How does a convolutional layer work?\" Explain that it applies a filter to the input image to detect features like edges, textures, etc., and then these features are combined to recognize higher-level patterns.\n",
    "\n",
    "#### 10.2 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are used for sequential data, such as time series or text. Here’s a simple RNN example:\n",
    "\n",
    "```python\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Interview Tip:** A common question is, \"What are the limitations of vanilla RNNs?\" They struggle with long-term dependencies due to the vanishing gradient problem, making it hard to capture patterns in longer sequences. This is why architectures like LSTMs or GRUs are often preferred.\n",
    "\n",
    "#### 10.3 Transformers\n",
    "\n",
    "Transformers are widely used in NLP tasks and have become the architecture of choice for many tasks.\n",
    "\n",
    "Here’s a simplified transformer implementation for sequence data:\n",
    "\n",
    "```python\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, nhead, num_encoder_layers, dim_model, dim_feedforward):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model, nhead=nhead, num_encoder_layers=num_encoder_layers, dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_model, 10)  # Assuming 10 output classes\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.fc(out[-1])\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Interview Tip:** You could be asked, \"Why have transformers become so popular in NLP?\" Transformers can capture long-range dependencies in sequences without the sequential processing bottlenecks of RNNs, thanks to the self-attention mechanism.\n",
    "\n",
    "### 11. Regularization Techniques\n",
    "\n",
    "Overfitting is a common problem in deep learning, where the model performs well on the training data but poorly on unseen data. Regularization techniques help mitigate this.\n",
    "\n",
    "#### 11.1 Dropout\n",
    "\n",
    "Dropout is a technique where, during training, a certain percentage of neurons are randomly set to zero in each forward pass.\n",
    "\n",
    "```python\n",
    "class SimpleNNWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 5)\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% dropout\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Interview Tip:** A potential question could be, \"How does dropout help prevent overfitting?\" By randomly dropping neurons, dropout prevents the model from becoming too reliant on any one neuron, which encourages the model to learn more robust features.\n",
    "\n",
    "#### 11.2 Weight Decay (L2 Regularization)\n",
    "\n",
    "Weight decay adds a penalty to the loss function based on the magnitude of the weights, discouraging large weights that might indicate overfitting.\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "```\n",
    "\n",
    "**Interview Tip:** You might be asked, \"What’s the difference between L1 and L2 regularization?\" L1 regularization encourages sparsity by adding a penalty proportional to the absolute value of the weights, while L2 regularization (weight decay) penalizes the square of the weights, encouraging smaller but non-zero weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmjGrwqPpXba"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
