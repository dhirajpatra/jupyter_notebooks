{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uFaJB-iw7su"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABghSZbRvVI-"
   },
   "source": [
    "For any modeling exercise in sklearn, the following steps are required:\n",
    "\n",
    "0. **Dataset Preparation:** This initial step involves collecting, cleaning, and organizing the data to be used for modeling. It includes tasks such as data collection, data cleaning (handling missing values, removing duplicates, etc.), data transformation (feature engineering, scaling, encoding categorical variables, etc.), and splitting the data into training and testing sets.\n",
    "1. **Model Instantiation:** In this step, you select the appropriate model class or algorithm class to use for your specific problem. This involves choosing from a variety of machine learning or statistical models such as linear regression, decision trees, neural networks, etc. Additionally, you set up the parameters and configuration of the model.\n",
    "\n",
    "2. **Model Fine-tuning or Fitting or Training:** Once the model is instantiated, it needs to be trained on the prepared dataset. During training, the model learns the patterns and relationships within the data. This involves feeding the training data into the model and adjusting the model's parameters iteratively to minimize the difference between the actual values and the predicted values. The goal is to optimize the model's performance on the training data.\n",
    "\n",
    "3. **Model Predictions:** After the model has been trained, it can be used to make predictions on new, unseen data. In this step, you provide new input data to the trained model, and it outputs predictions based on the patterns it learned during training. This is typically done using the predict function/method provided by most machine learning libraries.\n",
    "\n",
    "4. **Model Evaluation:** Once predictions have been generated, it's essential to evaluate the performance of the model to assess how well it generalizes to new, unseen data. Various evaluation metrics can be used depending on the type of problem (classification, regression, etc.) and the specific requirements of the application. Common evaluation metrics include accuracy, precision, recall, F1-score for classification tasks, and metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared for regression tasks. This step helps in understanding how well the model is performing and whether any further adjustments or improvements are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttECh8GCw-Z6"
   },
   "source": [
    "# SKLEARN = CRAZY LEVELS OF ABSTRACTION\n",
    "`scikit-learn`, often abbreviated as `sklearn`, is a popular machine learning library in Python. It provides simple and efficient tools for data analysis and data modeling, built on NumPy, SciPy, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpMPF9W1vAsI",
    "outputId": "9a07c95d-1c4b-4c07-d189-35340f6e3377"
   },
   "outputs": [],
   "source": [
    "# Step 0: Dataset Preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset (example using Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 1: Model Instantiation\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate Support Vector Classifier\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "# Step 2: Model Fitting or Training\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train) # features, target_value\n",
    "\n",
    "\n",
    "# Step 3: Model Predictions\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred) # actual values, predicted values\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ixu7SsBfUKA"
   },
   "outputs": [],
   "source": [
    "help(SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eLco9EKyG_w"
   },
   "source": [
    "# The Fit Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwBnYBKiyJNc"
   },
   "source": [
    "Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "AyxvO-_QxPXS",
    "outputId": "c100a3d1-7182-40e7-c309-f32ec899697d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]  # classes of each sample\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ-zruT7yfAb"
   },
   "source": [
    "The fit method generally accepts 2 inputs:\n",
    "\n",
    "* The **samples matrix** (or feature matrix) `X`. The size of X is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "* The **target** values `y` which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervised learning tasks, y does not need to be specified. y is usually a 1d array where the i th entry corresponds to the target of the i th sample (row) of X.\n",
    "\n",
    "Both X and y are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkNSLMzDzWCy"
   },
   "source": [
    "Once the estimator is fitted, it can be used for predicting target values of new data. You donâ€™t need to re-train the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "357zE0n8yONj",
    "outputId": "7332926e-b603-403f-d261-7416353b1a4b"
   },
   "outputs": [],
   "source": [
    "clf.predict(X)  # predict classes of the training data\n",
    "clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL4J3oe5y-Eb"
   },
   "source": [
    "Each estimator class has its own fit function: https://scikit-learn.org/stable/search.html?q=fit\n",
    "\n",
    "Each estimator class has its own predict function: https://scikit-learn.org/stable/search.html?q=predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERGTA43Nzsin"
   },
   "source": [
    "# Let's learn by example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gEWcbUMVh-8"
   },
   "source": [
    "## sklearn Preprocesssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tMblIeYWdnl"
   },
   "source": [
    "### Scaling Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bMrN2tpVtCN",
    "outputId": "885eb37f-8671-4f08-8fe7-6ec2020845fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Sample data\n",
    "train_data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "test_data = np.array([[10, 20], [30, 40], [50, 60]])\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform the data\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(train_data)\n",
    "print(\"\\nScaled data (standardized):\")\n",
    "print(scaled_train_data, \"\\n\")\n",
    "print(scaled_test_data)\n",
    "\n",
    "\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled data (Min-Max scaled):\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV55idHzWifP"
   },
   "source": [
    "### Handling categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xecy9ClIV-UL",
    "outputId": "8eb5b501-56ba-4321-dd72-27978d9de77d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "data = np.array([['red'], ['green'], ['blue'], ['red']])\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data).toarray()\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nOne-hot encoded data:\")\n",
    "print(encoded_data)\n",
    "\n",
    "\n",
    "# Sample categorical data\n",
    "data = np.array(['red', 'green', 'blue', 'red'])\n",
    "                # 2,      1,      0,      2\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_labels = encoder.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nEncoded labels:\")\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXoruimBGW8Z"
   },
   "outputs": [],
   "source": [
    "# https://ishanjainoffical.medium.com/understanding-weight-of-evidence-woe-with-python-code-cd0df0e4001e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4beiCvNWoC_"
   },
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XAy3yWxWcJS",
    "outputId": "b58bea90-5840-415c-fd7b-3eac920571cb"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = np.array([[1, 2], [np.nan, 4], [5, np.nan], [8, 9]])\n",
    "\n",
    "# Initialize the SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')  # Other strategies: mean, most_frequent\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nImputed data:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT4LKSGHW1QO",
    "outputId": "9bf958a8-7f3a-46b9-ac78-b1710a5d799b"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = np.array([[1, 2], [np.nan, 4], [5, np.nan], [8, 9]])\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nImputed data using KNNImputer:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnfxDSqmVX3e"
   },
   "source": [
    "## Classification Example using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDeBvHkJyrWP",
    "outputId": "e70d70b1-4a21-447c-c2ac-b2de2ebb8d2b"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXvHcLWdXFky"
   },
   "source": [
    "## Regression Example using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DZbV3lJXIXL",
    "outputId": "3a3ba072-b1b5-4c07-ee94-5a9e607ccc7a"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Random Forest MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4fHjzAkXzKs"
   },
   "source": [
    "## Clustering Example using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "eLQ4nzeuXfmb",
    "outputId": "85459d65-d80a-4075-e511-a0d6b2889364"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Create KMeans clustering model\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igOoJNluYI8K"
   },
   "source": [
    "## Dimensionality Reduction Example using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "DWEffL34X1SY",
    "outputId": "e43dd50d-d66b-478c-e476-c4fb9aedb482"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Visualize reduced data\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# APPLY K-MEANS AFTER THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPKjK2q6YQV2"
   },
   "source": [
    "## Text Classification Example using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrsVwMImYLGy",
    "outputId": "c1b174c9-5f4e-4225-e3d6-a0ee26ff6e24"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, newsgroups_train.target)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNFkWB68bz53"
   },
   "source": [
    "## Model Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEalgjCmb-Km",
    "outputId": "afd56431-24c2-4f5e-bc28-9d928a29d546"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umB6rjxYcOV8"
   },
   "source": [
    "To get an idea of the importance of the features, we are going to use the RidgeCV estimator. The features with the highest absolute coef_ value are considered the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "XEyiWiU9cCMJ",
    "outputId": "aed14668-6615-42e9-f8bd-1aa3f5b9955a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(diabetes.feature_names)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTbF54zBcB-H",
    "outputId": "7410735d-6072-4646-adff-158acd4d63cc"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "threshold = np.sort(importance)[-5] + 0.01\n",
    "print(threshold)\n",
    "\n",
    "tic = time()\n",
    "sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "toc = time()\n",
    "print(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\n",
    "print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGLmgbrCbHqI"
   },
   "source": [
    "## Using different SVM kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HV1HA48kYTJa",
    "outputId": "af46e49d-60ba-4856-98b4-b8fd15bdeb3c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(float)\n",
    "\n",
    "X_train = X[: int(0.9 * n_sample)]\n",
    "y_train = y[: int(0.9 * n_sample)]\n",
    "X_test = X[int(0.9 * n_sample) :]\n",
    "y_test = y[int(0.9 * n_sample) :]\n",
    "\n",
    "# fit the model\n",
    "for kernel in (\"linear\", \"rbf\", \"poly\"):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor=\"k\", s=20\n",
    "    )\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], s=80, facecolors=\"none\", zorder=10, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.contour(\n",
    "        XX,\n",
    "        YY,\n",
    "        Z,\n",
    "        colors=[\"k\", \"k\", \"k\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "        levels=[-0.5, 0, 0.5],\n",
    "    )\n",
    "\n",
    "    plt.title(kernel)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
